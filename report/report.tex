%File: report.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai25.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

\usepackage{amsmath}
\usepackage{hyperref}
\hypersetup{
    colorlinks = true,
    allcolors = blue
}

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using, and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{CS 598 Deep Learning for Healthcare (Spring, 2025)\\Project Report}
\author {Eric Schrock}
\affiliations{ejs9@illinois.edu}

\begin{document}

\maketitle

\begin{abstract}
This report details the results of my final project for CS 598: Deep Learning for Healthcare. My final project had four components.

First, I attempted to reproduce a portion of the findings of the paper "Multi-Label Generalized Zero Shot Learning for the Classification of Disease in Chest Radiographs" \cite{hayat2021multilabel}, specifically the AUROC scores of the proposed model on each of the fourteen diseases labeled in the the dataset, using the code provided by the paper. Running the model with the provided pre-trained weights reproduced the AUROC scores reported by the paper, but retraining the model did not.

Second, I attempted to rapidly rewrite the proposed model from scratch using an LLM. This was part of a research project on whether LLMs accelerate and enhance healthcare-related data science research. It took me just over one day to get to code that looked correct to me, but the resulting AUROC scores were much worse than those from the original model (pre-trained and re-trained). I saved my LLM chat log\footnote{\url{https://github.com/EricSchrock/cxr-ml-gzsl/blob/main/report/report-llm-chat-log-for-research.txt}} for review by the researchers.

Third, performed an extension study. I replaced the visual encoder used in the proposed model, DenseNet-121 \cite{huang2018denselyconnectedconvolutionalnetworks}, with the lighter weight EfficientNet-B0 \cite{tan2020efficientnetrethinkingmodelscaling} to see if I could achieve faster training times with comparable AUROC scores. Training was significantly faster, but the AUROC scores were significantly worse.

Fourth, I submitted a pull request (PR) to PyHealth \cite{pyhealth2023yang}, an open source project dedicated to supporting healthcare-related AI research and applications. My PR added support for the ChestX-ray14 dataset \cite{Wang_2017} and for binary classification tasks on the fourteen diseases labeled in the dataset.

\begin{itemize}
    \item \href{https://drive.google.com/file/d/1fTt2B8VNEQrtBT_Iooby2_viOujf59bX}{Video presentation} \footnote{\url{https://drive.google.com/file/d/1fTt2B8VNEQrtBT_Iooby2_viOujf59bX}}
    \item \href{https://github.com/EricSchrock/cxr-ml-gzsl}{Project GitHub repository} \footnote{\url{https://github.com/EricSchrock/cxr-ml-gzsl}}
    \item \href{https://github.com/sunlabuiuc/PyHealth/pull/392}{PyHealth pull request} \footnote{\url{https://github.com/sunlabuiuc/PyHealth/pull/392}}
\end{itemize}
\end{abstract}

\section{Introduction}

\subsection{Summary of the Paper in Question}

Deep learning models for classifying diseases from chest X-ray (CXR) images have had great success, achieving results comparable to those of human experts. However, collecting and labeling training data for these models is expensive and time consuming. For rarer diseases, it is often not economical and sometimes even impossible to gather the amount of data needed for supervised learning models. When a new disease emerges, the need for massive data collection slows the response. Human radiologists leverage other sources of knowledge to identify diseases they have previously never seen in X-ray form. Could a deep learning model do the same?

Multi-label generalized zero shot learning (ML-GZSL), which uses semantic information to identify classes not present in the set of labeled images used to train the model, has worked well in similar circumstances \cite{10.1109/TPAMI.2012.256, 10.1109/TMM.2019.2924511, 9157745}. However, these prior works have at least two limitations. First, they "extract a fixed visual representation of the image from a pre-trained visual encoder or a detection network" \cite{hayat2021multilabel}, which means they cannot be trained end-to-end. Second, "projecting these extracted visual features to the semantic space shrinks the diversity of the visual information, which gives rise to inherent limitations" \cite{hayat2021multilabel}, one of those being the hubness problem \cite{dinu2015improvingzeroshotlearningmitigating}. Can these limitations be overcome?

"Multi-Label Generalized Zero Shot Learning for the Classification of Disease in Chest Radiographs" \cite{hayat2021multilabel} proposes the CXR-ML-GZSL model, which addresses both limitations. The result is better performance when classifying both seen and unseen diseases in chest X-ray images, compared to two state-of-the-art ML-GZSL models: LESA \cite{9157745} and MLZSL \cite{lee2018multilabelzeroshotlearningstructured} (see Figure~\ref{fig:results}).

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\columnwidth]{results.png}
\caption{Performance of LESA, MLZSL, and CXR-ML-ZSL (``$OUR_{e2e}$") on the ChestX-ray14 dataset. Metrics are precision, recall, f1, and AUROC (split by seen, unseen, and the harmonic mean of the two) \cite{hayat2021multilabel}.}
\label{fig:results}
\end{figure}

\subsection{Scope of Reproducibility}

I was able to access the dataset used by the paper in question (details in Section~\ref{sec:data}) and reproduce the reported AUROC scores for the fourteen diseases against which the CXR-ML-GZSL model was tested using the provided code and pre-trained weights (details in Section~\ref{sec:model}). However, when I re-trained the model using the code provided, I was not able to reproduce the reported AUROC scores. Furthermore, when I rewrote the data processing, model, training, and evaluation code from scratch with the help of an LLM, I was unable to reproduce the reported AUROC scores.

\section{Methodology}

\subsection{Environment}

Table~\ref{tab:environment} shows the Python version and libraries used to implement the CXR-ML-GZSL model.

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{} & \textbf{Original\footnote{\url{https://github.com/nyuad-cai/CXR-ML-GZSL/blob/master/environment.yml}}} & \textbf{Reproduction} \\
\hline
Python & 3.6.12 & 3.11.12 \\
matplotlib & 3.3.3 & 3.10.0 \\
numpy & 1.19.2 & 2.0.2 \\
pandas & 1.1.3 & 2.2.2 \\
pillow & 8.0.0 & 11.1.0 \\
sklearn & 0.23.2 & 1.6.1 \\
torch & 1.4.0 & 2.6.0+cu124 \\
torchvision & 0.5.0 & 0.21.0+cu124 \\
tqdm & 4.55.1 & 4.67.1 \\
\hline
\end{tabular}
\caption{Environment}
\label{tab:environment}
\end{table}

\subsection{Data}
\label{sec:data}

The ChestX-ray14 dataset \cite{Wang_2017} contains 112,120 chest X-ray images labeled for the presence or absence of fourteen different diseases. The average number of diseases present in each X-ray is 0.72. Table~\ref{tab:dataset-statistics} shows how frequently each disease appears in the dataset. Figure~\ref{fig:cxr} shows an example chest X-ray from the dataset.

\begin{table}[h!]
\centering
\begin{tabular}{|l|r|r|}
\hline
\textbf{Disease} & \textbf{Count} & \textbf{Percentage} \\
\hline
None & 60,361 & 53.8\% \\
Atelectasis & 11,559 & 10.3\% \\
Cardiomegaly & 2,776 & 2.5\% \\
Consolidation & 4,667 & 4.2\% \\
Edema & 2,303 & 2.1\% \\
Effusion & 13,317 & 11.9\% \\
Emphysema & 2,516 & 2.2\% \\
Fibrosis & 1,686 & 1.5\% \\
Hernia & 227 & 0.2\% \\
Infiltration & 19,894 & 17.7\% \\
Mass & 5,782 & 5.2\% \\
Nodule & 6,331 & 5.6\% \\
Pleural Thickening & 3,385 & 3.0\% \\
Pneumonia & 1,431 & 1.3\% \\
Pneumothorax & 5,302 & 4.7\% \\
\hline
\end{tabular}
\caption{Dataset Disease Frequency}
\label{tab:dataset-statistics}
\end{table}

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\columnwidth]{cxr.png}
\caption{Chest X-ray Displaying Cardiomegaly}
\label{fig:cxr}
\end{figure}

The dataset is hosted by the National Institutes of Health (NIH)\footnote{\url{https://www.nih.gov/}} on box.com\footnote{\url{https://nihcc.app.box.com/v/ChestXray-NIHCC/folder/36938765345}}. It includes a Python script\footnote{\url{https://nihcc.app.box.com/v/ChestXray-NIHCC/file/371647823217}} for automatically downloading the images.

\subsection{Model}
\label{sec:model}

The paper provides code\footnote{\url{https://github.com/nyuad-cai/CXR-ML-GZSL/}} implementing the CXR-ML-GZSL model and training, as well as pre-trained weights\footnote{\url{https://drive.google.com/file/d/17ioJMW3qNx1Ktmr-hXn-eqp431cm49Rm/view}} that reproduce the reported results. 

CXR-ML-GZSL, shown in Figure~\ref{fig:model}, is comprised of a pre-trained text encoder to convert class labels to a semantic embedding space, a trainable visual encoder to convert X-ray images to a visual embedding space, and mapping models into a shared latent embedding space. The output is a score for each possible class, representing how relevant it is to the input image.

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\columnwidth]{model.png}
\caption{CXR-ML-GZSL \cite{hayat2021multilabel}}
\label{fig:model}
\end{figure}

Since the visual encoder is trainable, the whole pipeline between the X-ray image and the semantic embedding of the class label is trainable from end-to-end, better tuning the overall model to the task at hand. Additionally, the added latent embedding space is meant to address the limitations of mapping the visual embedding space directly onto the semantic embedding space.

For the pre-trained text encoder, the model uses BioBERT \cite{10.1093/bioinformatics/btz682}, which was trained specifically on a biomedical corpora. For the trainable visual encoder, the model uses Densenet-121 \cite{rajpurkar2017chexnetradiologistlevelpneumoniadetection}, as at that time it was the best chest X-ray classifier. For both BioBERT and Densenet-121, the model skips the final classification step in order to get semantic and visual embeddings, respectively, as output instead. The two mapping models are both three layer feed-forward neural networks.

CXR-ML-GZSL is the first use of ML-GZSL to classify diseases from chest X-ray images. It is also unique from other ML-GZSL models in the following three ways.

\begin{itemize}
    \item It can be trained end-to-end, thanks to the trainable visual encoder.
    \item It maps both the semantic and visual embedding spaces into a shared latent embedding space, instead of mapping the visual space onto the semantic space, losing less visual information.
    \item It uses BioBERT, which was trained on a biomedical corpora, resulting semantic embeddings that are tuned for healthcare use cases.
\end{itemize}

\subsection{Training}

\subsubsection{Hyperparameters}

According to the paper, the CXR-ML-GZSL model was trained on a range of learning rate (LR) and gamma values, but it does not state which values produced the best results. I picked a value from each range for my reproduction attempts.

The paper does not state the batch sizes used for the training, validation, and test datasets, so I used the hardcoded values when retraining the provided code and a set batch size of 64 when training the LLM generated model. Where the hyperparameter values in the paper do not match the provided code ($\delta$ and the learning rate decay patience and factor), I used the values from the paper. Table~\ref{tab:hyperparameters} lists the hyperparameter values used for training.

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{} & \textbf{Original} & \textbf{Repro} & \textbf{LLM} \\
\hline
LR & [1e-4, 5e-5, 1e-5] & 1e-4 & 1e-4 \\
Patience & 10 epochs & 10 epochs & 10 epochs \\
Decay factor & 0.01 & 0.01 & 0.01 \\
Batch sizes & 16/160/48 & 16/160/48 & 64/64/64 \\
$\gamma_1$ & [0.1, 0.05, 0.01] & 0.1 & 0.1 \\
$\gamma_2$ & [0.1, 0.05, 0.01] & 0.1 & 0.1 \\
$\delta$ & 0.5 & 0.5 & 0.5 \\
Optimizer & Adam & Adam & Adam \\
\hline
\end{tabular}
\caption{Hyperparameters Used (Original, Reproduction Using Original Code, and Reproduction Using LLM Generated Code}
\label{tab:hyperparameters}
\end{table}

\subsubsection{Computational Requirements}

Table~\ref{tab:compute} shows the computational requirements for training.

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{} & \textbf{Original} & \textbf{Repro} & \textbf{LLM} \\
\hline
GPU & Quadro RTX 6000 & A100 & A100 \\
GPU RAM & ? & 38.8 & 19.1 \\
System RAM & ? & 24.6 & 22.5 \\
Epochs & 100 & 100 & 100 \\
Training (hrs) & $\sim$8 & $\sim$7.5 & $\sim$8.5 \\
\hline
\end{tabular}
\caption{Computational Requirements (Original, Reproduction Using Original Code, and Reproduction Using LLM Generated Code}
\label{tab:compute}
\end{table}

\subsubsection{Loss Function}

The increased complexity of CXR-ML-GZSL, compared to other ML-GZSL models, requires a multifaceted training objective, captured in a three-part loss function, where $\gamma_1$ and $\gamma_2$ are the regularization parameters for the second and third components of the loss function.

\begin{equation}
    \min_{\boldsymbol{\phi} ,\boldsymbol{\rho} ,\boldsymbol{\psi}} \mathcal{L} = \mathcal{L}_{{rank}} +\gamma_{1} \mathcal{L}_{align} +\gamma_{2} \mathcal{L}_{con},
    \label{eqn:full_loss}
\end{equation}

\texorpdfstring{$\mathcal{L}_{rank}$}: adds penalties for any positive ground-truth relevance scores not larger than all negative ground-truth relevance scores by at least a margin of $\delta$. \texorpdfstring{$\mathcal{L}_{align}$}: adds penalties if input images and their labels do not map near each other in the latent embedding space. \texorpdfstring{$\mathcal{L}_{con}$}: add penalties if labels do not have similar relationships in both the semantic and latent embedding spaces, as those semantic relationships are key to zero shot learning.

\subsection{Evaluation}

For my reproductions, I generated AUROC scores for each of the fourteen diseases present in the dataset. Additionally, I generated the mean AUROC scores for the ten diseases seen by the model during training and the four diseases withheld from training, along with the harmonic mean between the "seen" and "unseen" means.

\section{Results}

\subsection{Reproduction Results}

Figure~\ref{fig:means} shows the "seen" and "unseen" disease AUROC score means, as well as the harmonic mean of the two. Figure~\ref{fig:seen} shows the AUROC score for each disease seen during training, while Figure~\ref{fig:unseen} shows the AUROC score for each disease not seen during training.

TBD (hyperparams, wrong batch sizes, bugs, etc.)

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\columnwidth]{means.png}
\caption{Overall Results}
\label{fig:means}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\columnwidth]{seen.png}
\caption{Results (diseases seen during training)}
\label{fig:seen}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\columnwidth]{unseen.png}
\caption{Results (diseases not seen during training)}
\label{fig:unseen}
\end{figure}

\subsection{Extension Study Results}

TBD (mention different epochs and training size)

\section{Discussion}

TBD

\subsection{Recommendations for Reproducibility}

\begin{itemize}
    \item Use version control from the start to provide a history of changes.
    \item Check-in the exact code used to generate the results you report. If different results are generated with different commits, tag those commits to make it clear exactly how each result was produced.
    \item Set default parameter values to match the values used to generate the results you report.
    \item Train your models on a single epoch to check for off-by-one errors in epoch indexing.
    \item If you experiment with different hyperparameter values, make it clear which values produced the results you report.
\end{itemize}

\section{Contributions}

I completed this project as a team of one. All work, unless otherwise stated, is my own.

\bibliography{report}

\end{document}
