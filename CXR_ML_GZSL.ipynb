{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CXR-ML-GZSL"
      ],
      "metadata": {
        "id": "5-a-wVNbsbVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "The goal of this notebook is to reproduce the findings of the paper, \"Multi-Label Generalized Zero Shot Learning for the Classification of Disease in Chest Radiographs\" using the provided code.\n",
        "\n",
        "* Paper: https://arxiv.org/abs/2107.06563\n",
        "* Code: https://github.com/nyuad-cai/CXR-ML-GZSL/\n",
        "\n",
        "The provided code is four years old, so some changes were needed to resolve deprecation warnings and errors. Additionally, I cleaned up some imports, whitespace, etc. and adapted the code for a Jupyter notebook. However, my goal was to use the code as is in most cases.\n",
        "\n",
        "**Note**: I used [Google Colab](https://colab.research.google.com/) to run this notebook. I had to use the paid version, as the free version does not provide enough RAM or a powerful enough GPU."
      ],
      "metadata": {
        "id": "POJObkEsliMM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JCSEZ3d_rbYa"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "from datetime import datetime, timedelta\n",
        "import glob\n",
        "import hashlib\n",
        "import os\n",
        "import tarfile\n",
        "import time\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import kl_div, softmax, log_softmax\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "The paper used a dataset developed by another paper, initially known as `ChestX-ray8`, but then renamed to `ChestX-ray14` when the dataset was expanded from eight to fourteen distinct disease labels.\n",
        "\n",
        "* Paper: https://arxiv.org/abs/1705.02315\n",
        "* Dataset: https://nihcc.app.box.com/v/ChestXray-NIHCC/folder/36938765345\n",
        "\n",
        "This notebook provides two different ways of uploading the dataset to session storage.\n",
        "\n",
        "1. The default is to download the image dataset from the official source. The dataset is over 42 GB, which takes 20+ minutes to download to session storage. Additionally, there are several metadata files from the `ChestX-ray14` dataset and `CXR-ML-GZSL` model repository that you will be prompted to manually upload to session storage. Be aware that session storage is wiped for each new session, including runtime changes like switching to a GPU.\n",
        "2. The other option is to download the [this](https://drive.google.com/file/d/11mk4sq5KgMpujdPxxQc8lnkqV18mQvxN/view) mirror of the dataset and metadata, save it to your Google Drive in `MyDrive`, and mount your Google Drive to the Google Colab runtime. Once the dataset is in Google Drive, it only takes 7+ minutes to copy to session storage and unzip."
      ],
      "metadata": {
        "id": "SD2XmBZYt43o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_PATH = 'CXR-ML-GZSL'\n",
        "\n",
        "if not os.path.exists(ROOT_PATH):\n",
        "    drive_path = 'drive/MyDrive/CXR-ML-GZSL.zip'\n",
        "    if os.path.isfile(drive_path):\n",
        "        with zipfile.ZipFile(drive_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(ROOT_PATH)\n",
        "        print(\"Data downloaded from Google Drive\")\n",
        "    else:\n",
        "        os.mkdir(ROOT_PATH)"
      ],
      "metadata": {
        "id": "BuKlfmq-6ZJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e057d9c-791c-4332-b29c-ccc405f5e827"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data downloaded from Google Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_PATH = f'{ROOT_PATH}/checkpoints'\n",
        "WEIGHTS_PATH = f'{SAVE_PATH}/best_auroc_checkpoint.pth.tar'\n",
        "\n",
        "if os.path.isfile(WEIGHTS_PATH):\n",
        "    print(\"Using existing pretrained weights\")\n",
        "else:\n",
        "    if not os.path.exists(SAVE_PATH): os.mkdir(SAVE_PATH)\n",
        "    link = 'https://drive.google.com/file/d/17ioJMW3qNx1Ktmr-hXn-eqp431cm49Rm/view'\n",
        "    assert False, f\"Please download the pretrained weights from {link} and place them in {SAVE_PATH}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfU2dFaLrHxn",
        "outputId": "68261767-f81c-40a7-d0cc-d7bfbe74652b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using existing pretrained weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = f'{ROOT_PATH}/data/nih_chest_xrays'\n",
        "IMAGE_PATH = f'{DATA_PATH}/images'\n",
        "\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    os.makedirs(DATA_PATH)\n",
        "\n",
        "if os.path.isfile(f'{DATA_PATH}/Data_Entry_2017_v2020.csv'):\n",
        "    print(\"Using existing data entry file\")\n",
        "else:\n",
        "    assert False, f\"Please download Data_Entry_2017_v2020.csv from the ChestX-ray14 dataset link and place it in {DATA_PATH}\"\n",
        "\n",
        "if os.path.exists(IMAGE_PATH):\n",
        "    print(\"Using existing image data\")\n",
        "else:\n",
        "    # Credit: https://nihcc.app.box.com/v/ChestXray-NIHCC/file/371647823217\n",
        "    links = [\n",
        "        'https://nihcc.box.com/shared/static/vfk49d74nhbxq3nqjg0900w5nvkorp5c.gz',\n",
        "        'https://nihcc.box.com/shared/static/i28rlmbvmfjbl8p2n3ril0pptcmcu9d1.gz',\n",
        "        'https://nihcc.box.com/shared/static/f1t00wrtdk94satdfb9olcolqx20z2jp.gz',\n",
        "        'https://nihcc.box.com/shared/static/0aowwzs5lhjrceb3qp67ahp0rd1l1etg.gz',\n",
        "        'https://nihcc.box.com/shared/static/v5e3goj22zr6h8tzualxfsqlqaygfbsn.gz',\n",
        "        'https://nihcc.box.com/shared/static/asi7ikud9jwnkrnkj99jnpfkjdes7l6l.gz',\n",
        "        'https://nihcc.box.com/shared/static/jn1b4mw4n6lnh74ovmcjb8y48h8xj07n.gz',\n",
        "        'https://nihcc.box.com/shared/static/tvpxmn7qyrgl0w8wfh9kqfjskv6nmm1j.gz',\n",
        "        'https://nihcc.box.com/shared/static/upyy3ml7qdumlgk2rfcvlb9k6gvqq2pj.gz',\n",
        "        'https://nihcc.box.com/shared/static/l6nilvfa9cg3s28tqv1qc1olm3gnz54p.gz',\n",
        "        'https://nihcc.box.com/shared/static/hhq8fkdgvcari67vfhs7ppg2w6ni4jze.gz',\n",
        "        'https://nihcc.box.com/shared/static/ioqwiy20ihqwyr8pf4c24eazhh281pbu.gz'\n",
        "    ]\n",
        "\n",
        "    # Credit: https://nihcc.app.box.com/v/ChestXray-NIHCC/file/249502714403\n",
        "    md5_checksums = [\n",
        "        'fe8ed0a6961412fddcbb3603c11b3698',\n",
        "        'ab07a2d7cbe6f65ddd97b4ed7bde10bf',\n",
        "        '2301d03bde4c246388bad3876965d574',\n",
        "        '9f1b7f5aae01b13f4bc8e2c44a4b8ef6',\n",
        "        '1861f3cd0ef7734df8104f2b0309023b',\n",
        "        '456b53a8b351afd92a35bc41444c58c8',\n",
        "        '1075121ea20a137b87f290d6a4a5965e',\n",
        "        'b61f34cec3aa69f295fbb593cbd9d443',\n",
        "        '442a3caa61ae9b64e61c561294d1e183',\n",
        "        '09ec81c4c31e32858ad8cf965c494b74',\n",
        "        '499aefc67207a5a97692424cf5dbeed5',\n",
        "        'dc9fda1757c2de0032b63347a7d2895c'\n",
        "    ]\n",
        "\n",
        "    for idx, link in enumerate(links):\n",
        "        fn = os.path.join(DATA_PATH, 'images_%02d.tar.gz' % (idx + 1))\n",
        "\n",
        "        print(f'Downloading {fn}...')\n",
        "        urllib.request.urlretrieve(link, fn)\n",
        "\n",
        "        print(f\"Checking MD5 checksum for {fn}...\")\n",
        "        with open(fn, 'rb') as f:\n",
        "            file_md5 = hashlib.md5(f.read()).hexdigest()\n",
        "\n",
        "        assert file_md5 == md5_checksums[idx], \"Invalid MD5 checksum\"\n",
        "\n",
        "        print(f\"Extracting {fn}...\")\n",
        "        with tarfile.open(fn, 'r:gz') as tar:\n",
        "            tar.extractall(path=DATA_PATH)\n",
        "\n",
        "        print(f\"Deleting {fn}...\")\n",
        "        os.remove(fn)\n",
        "\n",
        "    assert len([f for f in os.listdir(IMAGE_PATH) if os.path.isfile(os.path.join(IMAGE_PATH, f))]) == 112120, \"Dataset is not the expected size!\"\n",
        "    print(\"Image data download complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZEngFTmsa-3",
        "outputId": "d6f19a68-7544-4af8-8d30-6ec1a8a043e4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using existing data entry file\n",
            "Using existing image data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SPLITS_PATH = f'{ROOT_PATH}/dataset_splits'\n",
        "\n",
        "files = ['train.txt', 'test.txt', 'val.txt']\n",
        "if all(os.path.exists(f'{SPLITS_PATH}/{f}') for f in files):\n",
        "    print(\"Using existing dataset splits\")\n",
        "else:\n",
        "    if not os.path.exists(SPLITS_PATH): os.mkdir(SPLITS_PATH)\n",
        "    link = 'https://github.com/nyuad-cai/CXR-ML-GZSL/tree/master/dataset_splits'\n",
        "    assert False, f\"Please download the dataset splits from {link} and place them in {SPLITS_PATH}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgnZHGyyx4So",
        "outputId": "de5d589d-4944-454b-d510-12bf2a68aa37"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using existing dataset splits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDINGS_PATH = f'{ROOT_PATH}/embeddings'\n",
        "BIOBERT_PATH = f'{EMBEDDINGS_PATH}/nih_chest_xray_biobert.npy'\n",
        "\n",
        "if os.path.isfile(BIOBERT_PATH):\n",
        "    print(\"Using existing text embeddings\")\n",
        "else:\n",
        "    if not os.path.exists(EMBEDDINGS_PATH): os.mkdir(EMBEDDINGS_PATH)\n",
        "    link = 'https://github.com/nyuad-cai/CXR-ML-GZSL/blob/master/embeddings/nih_chest_xray_biobert.npy'\n",
        "    assert False, f\"Please download the text embeddings from {link} and place them in {EMBEDDINGS_PATH}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0EaBbm3zjyT",
        "outputId": "7e7cde2c-5068-486d-c080-c7ea5160d9ff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using existing text embeddings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "RwIGiX-JrmdI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KLDivLoss"
      ],
      "metadata": {
        "id": "aDcFrrhTCo6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Credit: https://github.com/nyuad-cai/CXR-ML-GZSL/blob/master/loss.py\n",
        "\n",
        "class KLDivLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.2):\n",
        "        super(KLDivLoss, self).__init__()\n",
        "\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, emb1, emb2):\n",
        "        emb1 = softmax(emb1/self.temperature, dim=1).detach()\n",
        "        emb2 = log_softmax(emb2/self.temperature, dim=1)\n",
        "        loss_kldiv = kl_div(emb2, emb1, reduction='none')\n",
        "        loss_kldiv = torch.sum(loss_kldiv, dim=1)\n",
        "        loss_kldiv = torch.mean(loss_kldiv)\n",
        "\n",
        "        return loss_kldiv"
      ],
      "metadata": {
        "id": "mamyKMa4CoTj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RankingLoss"
      ],
      "metadata": {
        "id": "uY2t76EAiFBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Credit: https://github.com/nyuad-cai/CXR-ML-GZSL/blob/master/loss.py\n",
        "\n",
        "class RankingLoss(nn.Module):\n",
        "    def __init__(self, neg_penalty=0.03):\n",
        "        super(RankingLoss, self).__init__()\n",
        "\n",
        "        self.neg_penalty = neg_penalty\n",
        "\n",
        "    def forward(self, ranks, labels, class_ids_loaded, device):\n",
        "        '''\n",
        "        for each correct it should be higher then the absence\n",
        "        '''\n",
        "        labels = labels[:, class_ids_loaded]\n",
        "        ranks_loaded = ranks[:, class_ids_loaded]\n",
        "        neg_labels = 1+(labels*-1)\n",
        "        loss_rank = torch.zeros(1).to(device)\n",
        "        for i in range(len(labels)):\n",
        "            correct = ranks_loaded[i, labels[i]==1]\n",
        "            wrong = ranks_loaded[i, neg_labels[i]==1]\n",
        "            correct = correct.reshape((-1, 1)).repeat((1, len(wrong)))\n",
        "            wrong = wrong.repeat(len(correct)).reshape(len(correct), -1)\n",
        "            image_level_penalty = ((self.neg_penalty+wrong) - correct)\n",
        "            image_level_penalty[image_level_penalty<0]=0\n",
        "            loss_rank += image_level_penalty.sum()\n",
        "        loss_rank /=len(labels)\n",
        "\n",
        "        return loss_rank"
      ],
      "metadata": {
        "id": "HGFT0qLAiFIx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CosineLoss"
      ],
      "metadata": {
        "id": "RFX6rw5NiFc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Credit: https://github.com/nyuad-cai/CXR-ML-GZSL/blob/master/loss.py\n",
        "\n",
        "class CosineLoss(nn.Module):\n",
        "    def forward(self, t_emb, v_emb ):\n",
        "        a_norm = v_emb / v_emb.norm(dim=1)[:, None]\n",
        "        b_norm = t_emb / t_emb.norm(dim=1)[:, None]\n",
        "        loss = 1 - torch.mean(torch.diagonal(torch.mm(a_norm, b_norm.t()), 0))\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "pwi6_lTOiFkI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ZSLNet"
      ],
      "metadata": {
        "id": "ZsODvFq8iicH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Credit: https://github.com/nyuad-cai/CXR-ML-GZSL/blob/master/zsl_models.py\n",
        "\n",
        "class ZSLNet(nn.Module):\n",
        "    def __init__(self, args, textual_embeddings=None, device='cpu'):\n",
        "        super(ZSLNet, self).__init__()\n",
        "        self.args = args\n",
        "        self.device = device\n",
        "        self.vision_backbone = getattr(torchvision.models, self.args.vision_backbone)(pretrained=self.args.pretrained)\n",
        "        # remove classification layer from visual encoder\n",
        "        classifiers = [ 'classifier', 'fc']\n",
        "        for classifier in classifiers:\n",
        "            cls_layer = getattr(self.vision_backbone, classifier, None)\n",
        "            if cls_layer is None:\n",
        "                continue\n",
        "            d_visual = cls_layer.in_features\n",
        "            setattr(self.vision_backbone, classifier, nn.Identity(d_visual))\n",
        "            break\n",
        "\n",
        "        pretrained_encoder = False\n",
        "        if pretrained_encoder:\n",
        "            self.vision_backbone.classifier = nn.Identity(d_visual)\n",
        "\n",
        "            path = 'checkpoints/bce_only_imagenet/last_epoch_checkpoint.pth.tar'\n",
        "\n",
        "            self.classifier = nn.Sequential(nn.Linear(d_visual, self.args.num_classes), nn.Sigmoid())\n",
        "            checkpoint = torch.load(path)\n",
        "            self.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "            for p in self.vision_backbone.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        if self.args.bce_only:\n",
        "            self.bce_loss = torch.nn.BCELoss(size_average=True)\n",
        "            self.classifier = nn.Sequential(nn.Linear(d_visual, self.args.num_classes), nn.Sigmoid())\n",
        "        else:\n",
        "            self.emb_loss = CosineLoss()\n",
        "            self.ranking_loss = RankingLoss(neg_penalty=self.args.neg_penalty)\n",
        "            self.textual_embeddings = textual_embeddings\n",
        "            d_textual = self.textual_embeddings.shape[-1]\n",
        "\n",
        "            self.textual_embeddings = torch.from_numpy(self.textual_embeddings).to(self.device)\n",
        "\n",
        "            self.fc_v = nn.Sequential(\n",
        "                nn.Linear(d_visual, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(512, 256),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(256, 128),\n",
        "            )\n",
        "\n",
        "            self.fc_t = nn.Sequential(\n",
        "                nn.Linear(d_textual, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(512, 256),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(256, 128)\n",
        "            )\n",
        "\n",
        "    def forward(self, x, labels=None, epoch=0, n_crops=0, bs=16):\n",
        "        if self.args.bce_only:\n",
        "            return self.forward_bce_only(x, labels=labels, n_crops=n_crops, bs=bs)\n",
        "        else:\n",
        "            return self.forward_ranking(x, labels=labels, epoch=epoch, n_crops=n_crops, bs=bs)\n",
        "\n",
        "    def forward_bce_only(self, x, labels=None, n_crops=0, bs=16):\n",
        "        lossvalue_bce = torch.zeros(1).to(self.device)\n",
        "\n",
        "        visual_feats = self.vision_backbone(x)\n",
        "        preds = self.classifier(visual_feats)\n",
        "\n",
        "        if labels is not None:\n",
        "            lossvalue_bce = self.bce_loss(preds, labels)\n",
        "\n",
        "        return preds, lossvalue_bce, f'bce:\\t {lossvalue_bce.item():0.4f}'\n",
        "\n",
        "    def forward_ranking(self, x, labels=None, epoch=0, n_crops=0, bs=16):\n",
        "        loss_rank = torch.zeros(1).to(self.device)\n",
        "        loss_allignment_cos = torch.zeros(1).to(self.device)\n",
        "        loss_mapping_consistency = torch.zeros(1).to(self.device)\n",
        "\n",
        "        visual_feats = self.vision_backbone(x)\n",
        "        visual_feats = self.fc_v(visual_feats)\n",
        "        text_feats = self.fc_t(self.textual_embeddings)\n",
        "\n",
        "        if not self.args.wo_con and epoch >= 0:\n",
        "            text_mapped_sim = self.sim_score(text_feats, text_feats.detach())\n",
        "            text_orig_sim = self.sim_score(self.textual_embeddings, self.textual_embeddings)\n",
        "            loss_mapping_consistency = torch.abs(text_orig_sim - text_mapped_sim).mean()\n",
        "\n",
        "        if labels is not None:\n",
        "            mapped_visual, mapped_text = self.map_visual_text(visual_feats, labels, text_feats)\n",
        "            if mapped_visual is not None and not self.args.wo_map and epoch >= 0:\n",
        "                loss_allignment_cos = self.emb_loss(mapped_text, mapped_visual)\n",
        "\n",
        "        ranks = self.sim_score(visual_feats, text_feats)\n",
        "        if n_crops > 0:\n",
        "            ranks = ranks.view(bs, n_crops, -1).mean(1)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_rank = self.ranking_loss(ranks, labels, self.class_ids_loaded, self.device)\n",
        "        loss_allignment_cos = (self.args.beta_map * loss_allignment_cos)\n",
        "        loss_rank = (self.args.beta_rank * loss_rank)\n",
        "        loss_mapping_consistency = (self.args.beta_con * loss_mapping_consistency)\n",
        "        losses = loss_rank + loss_mapping_consistency + 0.0*loss_allignment_cos\n",
        "\n",
        "        return ranks, losses\n",
        "\n",
        "    def sim_score(self, a, b):\n",
        "        a_norm = a / a.norm(dim=1)[:, None]\n",
        "        b_norm = b / (1e-6+b.norm(dim=1))[:, None]\n",
        "        score = (torch.mm(a_norm, b_norm.t()))\n",
        "\n",
        "        return score\n",
        "\n",
        "    def map_visual_text(self, visual_feats, labels, labels_embd):\n",
        "        mapped_labels_embd = []\n",
        "        labels == 1\n",
        "        for i in range(0, labels.shape[0]):\n",
        "            class_embd = labels_embd[labels[i]==1].mean(dim=0)[None,:]\n",
        "            mapped_labels_embd.append(class_embd)\n",
        "        mapped_labels_embd = torch.cat(mapped_labels_embd)\n",
        "\n",
        "        return visual_feats.detach(), mapped_labels_embd.detach()"
      ],
      "metadata": {
        "id": "nwlvZTX4iiks"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NIHChestXray"
      ],
      "metadata": {
        "id": "81EQt4_QjOWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Credit: https://github.com/nyuad-cai/CXR-ML-GZSL/blob/master/dataset.py\n",
        "\n",
        "class NIHChestXray(Dataset):\n",
        "    def __init__ (self, args, pathDatasetFile, transform, classes_to_load='seen', exclude_all=True):\n",
        "        self.listImagePaths = []\n",
        "        self.listImageLabels = []\n",
        "        self.transform = transform\n",
        "        self.num_classes = args.num_classes\n",
        "\n",
        "        self._data_path = args.data_root\n",
        "        self.args = args\n",
        "\n",
        "        self.split_path = pathDatasetFile\n",
        "        self.CLASSES = [ 'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia',\n",
        "                'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia']\n",
        "\n",
        "        self.unseen_classes = ['Edema', 'Pneumonia', 'Emphysema', 'Fibrosis']\n",
        "\n",
        "        self.seen_classes = [ 'Atelectasis', 'Effusion', 'Infiltration', 'Mass', 'Nodule',\n",
        "                'Pneumothorax', 'Consolidation', 'Cardiomegaly', 'Pleural_Thickening', 'Hernia']\n",
        "\n",
        "        self._class_ids = {v: i for i, v in enumerate(self.CLASSES) if v != 'No Finding'}\n",
        "\n",
        "        self.seen_class_ids = [self._class_ids[label] for label in self.seen_classes]\n",
        "        self.unseen_class_ids = [self._class_ids[label] for label in self.unseen_classes]\n",
        "\n",
        "        self.classes_to_load = classes_to_load\n",
        "        self.exclude_all = exclude_all\n",
        "        self._construct_index()\n",
        "\n",
        "    def _construct_index(self):\n",
        "        # Compile the split data path\n",
        "        max_labels = 0\n",
        "        paths = glob.glob(f'{self._data_path}/images/*.png')\n",
        "        self.names_to_path = {path.split('/')[-1]: path for path in paths}\n",
        "        data_entry_file = 'Data_Entry_2017_v2020.csv'\n",
        "\n",
        "        print(f'data partition path: {self.split_path}')\n",
        "        with open(self.split_path, 'r') as f: file_names = f.readlines()\n",
        "\n",
        "        split_file_names = np.array([file_name.strip().split(' ')[0].split('/')[-1] for file_name in file_names])\n",
        "        df = pd.read_csv(f'{self._data_path}/{data_entry_file}')\n",
        "        image_index = df.iloc[:, 0].values\n",
        "\n",
        "        _, split_index, _ = np.intersect1d(image_index, split_file_names, return_indices=True)\n",
        "\n",
        "        labels = df.iloc[:, 1].values\n",
        "        labels = np.array(labels)[split_index]\n",
        "\n",
        "        labels = [label.split('|') for label in labels]\n",
        "\n",
        "        image_index = image_index[split_index]\n",
        "\n",
        "        # Construct the image db\n",
        "        self._imdb = []\n",
        "        self.class_ids_loaded = []\n",
        "        for index in range(len(split_index)):\n",
        "            if len(labels[index]) == 1 and labels[index][0] == 'No Finding':\n",
        "                continue\n",
        "            if self._should_load_image(labels[index]) is False:\n",
        "                continue\n",
        "            class_ids = [self._class_ids[label] for label in labels[index]]\n",
        "            self.class_ids_loaded +=class_ids\n",
        "            self._imdb.append({\n",
        "                'im_path': self.names_to_path[image_index[index]],\n",
        "                'labels': class_ids,\n",
        "            })\n",
        "            max_labels = max(max_labels, len(class_ids))\n",
        "\n",
        "        self.class_ids_loaded = np.unique(np.array(self.class_ids_loaded))\n",
        "        print(f'Number of images: {len(self._imdb)}')\n",
        "        print(f'Number of max labels per image: {max_labels}')\n",
        "        print(f'Number of classes: {len(self.class_ids_loaded)}')\n",
        "\n",
        "    def _should_load_image(self, labels):\n",
        "        selected_class_labels = self.CLASSES\n",
        "        if self.classes_to_load == 'seen':\n",
        "            selected_class_labels = self.seen_classes\n",
        "        elif self.classes_to_load == 'unseen':\n",
        "            selected_class_labels = self.unseen_classes\n",
        "        elif self.classes_to_load == 'all':\n",
        "            return True\n",
        "\n",
        "        count = 0\n",
        "        for label in labels:\n",
        "            if label in selected_class_labels:\n",
        "                count+=1\n",
        "\n",
        "        if count == len(labels):\n",
        "            # all labels from selected sub set\n",
        "            return True\n",
        "        elif count == 0:\n",
        "            # none label in selected sub set\n",
        "            return False\n",
        "        else:\n",
        "            # some labels in selected sub set\n",
        "            if self.exclude_all is True:\n",
        "                return False\n",
        "            else:\n",
        "                return True\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        imagePath = self._imdb[index]['im_path']\n",
        "        imageData = Image.open(imagePath).convert('RGB')\n",
        "        labels = torch.tensor(self._imdb[index]['labels'])\n",
        "        labels = labels.unsqueeze(0)\n",
        "        imageLabel = torch.zeros(labels.size(0), self.num_classes).scatter_(1, labels, 1.).squeeze()\n",
        "        img = self.transform(imageData)\n",
        "\n",
        "        return img, imageLabel\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._imdb)"
      ],
      "metadata": {
        "id": "p4Nive-pjOel"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### plot_array"
      ],
      "metadata": {
        "id": "YdyCvcywoH06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Credit: https://github.com/nyuad-cai/CXR-ML-GZSL/blob/master/plots.py\n",
        "\n",
        "def plot_array(array, disc='loss'):\n",
        "    plt.plot(array)\n",
        "    plt.ylabel(disc)\n",
        "    plt.savefig(f'{disc}.pdf')\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "9XHV1q1xoH-9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ChexnetTrainer"
      ],
      "metadata": {
        "id": "5RvfqMkilbad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Credit: https://github.com/nyuad-cai/CXR-ML-GZSL/blob/master/ChexnetTrainer.py\n",
        "\n",
        "class ChexnetTrainer(object):\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.textual_embeddings = np.load(args.textual_embeddings)\n",
        "\n",
        "        self.model = ZSLNet(self.args, self.textual_embeddings, self.device).to(self.device)\n",
        "        self.optimizer = optim.Adam (self.model.parameters(), lr=self.args.lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n",
        "        self.scheduler = ReduceLROnPlateau(self.optimizer, factor=0.1, patience=5, mode='min')\n",
        "\n",
        "        self.loss = torch.nn.BCELoss(size_average=True)\n",
        "        self.auroc_min_loss = 0.0\n",
        "\n",
        "        self.start_epoch = 1\n",
        "        self.lossMIN = float('inf')\n",
        "        self.max_auroc_mean = float('-inf')\n",
        "        self.best_epoch = 1\n",
        "\n",
        "        self.val_losses = []\n",
        "\n",
        "        self.resume_from()\n",
        "        self.load_from()\n",
        "        self.init_dataset()\n",
        "\n",
        "        self.steps = [int(step) for step in self.args.steps.split(',')]\n",
        "        self.time_start = time.time()\n",
        "        self.time_end = time.time()\n",
        "        self.should_test = False\n",
        "        self.model.class_ids_loaded = self.train_dl.dataset.class_ids_loaded\n",
        "\n",
        "    def __call__(self):\n",
        "        self.train()\n",
        "\n",
        "    def load_from(self):\n",
        "        if self.args.load_from is not None:\n",
        "            checkpoint = torch.load(self.args.load_from, weights_only=False)\n",
        "            self.model.load_state_dict(checkpoint['state_dict'])\n",
        "            print(f'loaded checkpoint from {self.args.load_from}')\n",
        "\n",
        "    def resume_from(self):\n",
        "        if self.args.resume_from is not None:\n",
        "            checkpoint = torch.load(self.args.resume_from)\n",
        "            self.model.load_state_dict(checkpoint['state_dict'])\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            self.start_epoch = checkpoint['epoch'] + 1\n",
        "            self.lossMIN = checkpoint['lossMIN']\n",
        "            self.max_auroc_mean = checkpoint['max_auroc_mean']\n",
        "            print(f'resuming training from epoch {self.start_epoch}')\n",
        "\n",
        "    def save_checkpoint(self, prefix='best'):\n",
        "        path = f'{self.args.save_dir}/{prefix}_checkpoint.pth.tar'\n",
        "        torch.save(\n",
        "            {\n",
        "            'epoch': self.epoch,\n",
        "            'state_dict': self.model.state_dict(),\n",
        "            'max_auroc_mean': self.max_auroc_mean,\n",
        "            'optimizer' : self.optimizer.state_dict(),\n",
        "            'lossMIN' : self.lossMIN\n",
        "            }, path)\n",
        "        print(f\"saving {prefix} checkpoint\")\n",
        "\n",
        "    def init_dataset(self):\n",
        "        normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "\n",
        "        train_transforms = []\n",
        "        train_transforms.append(transforms.RandomResizedCrop(self.args.crop))\n",
        "        train_transforms.append(transforms.RandomHorizontalFlip())\n",
        "        train_transforms.append(transforms.ToTensor())\n",
        "        train_transforms.append(normalize)\n",
        "\n",
        "        datasetTrain = NIHChestXray(self.args, self.args.train_file, transform=transforms.Compose(train_transforms))\n",
        "\n",
        "        self.train_dl = DataLoader(dataset=datasetTrain, batch_size=self.args.batch_size, shuffle=True,  num_workers=4, pin_memory=True)\n",
        "\n",
        "        test_transforms = []\n",
        "        test_transforms.append(transforms.Resize(self.args.resize))\n",
        "        test_transforms.append(transforms.TenCrop(self.args.crop))\n",
        "        test_transforms.append(transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])))\n",
        "        test_transforms.append(transforms.Lambda(lambda crops: torch.stack([normalize(crop) for crop in crops])))\n",
        "\n",
        "        datasetVal = NIHChestXray(self.args, self.args.val_file, transform=transforms.Compose(test_transforms))\n",
        "        self.val_dl = DataLoader(dataset=datasetVal, batch_size=self.args.batch_size*10, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "        datasetTest = NIHChestXray(self.args, self.args.test_file, transform=transforms.Compose(test_transforms), classes_to_load='all')\n",
        "        self.test_dl = DataLoader(dataset=datasetTest, batch_size=self.args.batch_size*3, num_workers=8, shuffle=False, pin_memory=True)\n",
        "        print(datasetTest.CLASSES)\n",
        "\n",
        "    def train(self):\n",
        "        for self.epoch in range(self.start_epoch, self.args.epochs + 1):\n",
        "            self.epochTrain()\n",
        "            lossVal, val_ind_auroc = self.epochVal()\n",
        "            val_ind_auroc = np.array(val_ind_auroc)\n",
        "\n",
        "            aurocMean = val_ind_auroc.mean()\n",
        "            self.save_checkpoint(prefix=f'last_epoch')\n",
        "            self.should_test = False\n",
        "\n",
        "            if aurocMean > self.max_auroc_mean:\n",
        "                self.max_auroc_mean = aurocMean\n",
        "                self.save_checkpoint(prefix='best_auroc')\n",
        "                self.best_epoch = self.epoch\n",
        "                self.should_test = True\n",
        "\n",
        "            if lossVal < self.lossMIN:\n",
        "                self.lossMIN = lossVal\n",
        "                self.auroc_min_loss = aurocMean\n",
        "                self.save_checkpoint(prefix='min_loss')\n",
        "                self.should_test = True\n",
        "\n",
        "            self.print_auroc(val_ind_auroc, self.val_dl.dataset.class_ids_loaded, prefix='val')\n",
        "            if self.should_test is True:\n",
        "                test_ind_auroc = self.test()\n",
        "                test_ind_auroc = np.array(test_ind_auroc)\n",
        "\n",
        "                self.write_results(val_ind_auroc, self.val_dl.dataset.class_ids_loaded, prefix=f'\\n\\nepoch {self.epoch}\\nval', mode='a')\n",
        "\n",
        "                self.write_results(test_ind_auroc[self.test_dl.dataset.seen_class_ids], self.test_dl.dataset.seen_class_ids, prefix='\\ntest_seen', mode='a')\n",
        "                self.write_results(test_ind_auroc[self.test_dl.dataset.unseen_class_ids], self.test_dl.dataset.unseen_class_ids, prefix='\\ntest_unseen', mode='a')\n",
        "\n",
        "                self.print_auroc(test_ind_auroc[self.test_dl.dataset.seen_class_ids], self.test_dl.dataset.seen_class_ids, prefix='\\ntest_seen')\n",
        "                self.print_auroc(test_ind_auroc[self.test_dl.dataset.unseen_class_ids], self.test_dl.dataset.unseen_class_ids, prefix='\\ntest_unseen')\n",
        "\n",
        "            plot_array(self.val_losses, f'{self.args.save_dir}/val_loss')\n",
        "            print(f'best epoch {self.best_epoch} best auroc {self.max_auroc_mean} loss {lossVal:.6f} auroc at min loss {self.auroc_min_loss:0.4f}')\n",
        "\n",
        "            self.scheduler.step(lossVal)\n",
        "\n",
        "    def get_eta(self, epoch, iter):\n",
        "        self.time_end = time.time()\n",
        "        delta = self.time_end - self.time_start\n",
        "        delta = delta * (len(self.train_dl) * ((self.args.epochs + 1) - epoch) - iter)\n",
        "        sec = timedelta(seconds=int(delta))\n",
        "        d = (datetime(1,1,1) + sec)\n",
        "        eta = f\"{d.day-1} Days {d.hour}:{d.minute}:{d.second}\"\n",
        "        self.time_start = time.time()\n",
        "\n",
        "        return eta\n",
        "\n",
        "    def epochTrain(self):\n",
        "        self.model.train()\n",
        "        epoch_loss = 0\n",
        "        for batchID, (inputs, target) in enumerate (self.train_dl):\n",
        "\n",
        "            target = target.to(self.device)\n",
        "            inputs = inputs.to(self.device)\n",
        "            output, loss = self.model(inputs, target, self.epoch)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            eta = self.get_eta(self.epoch, batchID)\n",
        "            epoch_loss +=loss.item()\n",
        "            if batchID % 10 == 9:\n",
        "                print(f\" epoch [{self.epoch:04d} / {self.args.epochs:04d}] eta: {eta:<20} [{batchID:04}/{len(self.train_dl)}] lr: \\t{self.optimizer.param_groups[0]['lr']:0.4E} loss: \\t{epoch_loss/batchID:0.5f}\")\n",
        "\n",
        "    def epochVal(self):\n",
        "        self.model.eval()\n",
        "\n",
        "        lossVal = 0\n",
        "\n",
        "        outGT = torch.FloatTensor().to(self.device)\n",
        "        outPRED = torch.FloatTensor().to(self.device)\n",
        "        for i, (inputs, target) in enumerate (tqdm(self.val_dl)):\n",
        "            with torch.no_grad():\n",
        "                target = target.to(self.device)\n",
        "                inputs = inputs.to(self.device)\n",
        "                varTarget = torch.autograd.Variable(target)\n",
        "                bs, n_crops, c, h, w = inputs.size()\n",
        "\n",
        "                varInput = torch.autograd.Variable(inputs.view(-1, c, h, w).to(self.device))\n",
        "\n",
        "                varOutput, losstensor = self.model(varInput, varTarget, n_crops=n_crops, bs=bs)\n",
        "\n",
        "                outPRED = torch.cat((outPRED, varOutput), 0)\n",
        "                outGT = torch.cat((outGT, target), 0)\n",
        "\n",
        "                lossVal+=losstensor.item()\n",
        "                del varOutput, varTarget, varInput, target, inputs\n",
        "        lossVal = lossVal / len(self.val_dl)\n",
        "\n",
        "        aurocIndividual = self.computeAUROC(outGT, outPRED, self.val_dl.dataset.class_ids_loaded)\n",
        "        self.val_losses.append(lossVal)\n",
        "\n",
        "        return lossVal, aurocIndividual\n",
        "\n",
        "    def test(self):\n",
        "        cudnn.benchmark = True\n",
        "        outGT = torch.FloatTensor().cuda()\n",
        "        outPRED = torch.FloatTensor().cuda()\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        for i, (inputs, target) in enumerate(tqdm(self.test_dl)):\n",
        "            with torch.no_grad():\n",
        "                target = target.to(self.device)\n",
        "                outGT = torch.cat((outGT, target), 0)\n",
        "\n",
        "                bs, n_crops, c, h, w = inputs.size()\n",
        "\n",
        "                varInput = torch.autograd.Variable(inputs.view(-1, c, h, w).to(self.device))\n",
        "\n",
        "                out, _ = self.model(varInput, n_crops=n_crops, bs=bs)\n",
        "\n",
        "                outPRED = torch.cat((outPRED, out.data), 0)\n",
        "\n",
        "        aurocIndividual = self.computeAUROC(outGT, outPRED, self.test_dl.dataset.class_ids_loaded)\n",
        "\n",
        "        return aurocIndividual\n",
        "\n",
        "    def computeAUROC(self, dataGT, dataPRED, class_ids):\n",
        "        outAUROC = []\n",
        "        datanpGT = dataGT.cpu().numpy()\n",
        "        datanpPRED = dataPRED.cpu().numpy()\n",
        "\n",
        "        for i in class_ids:\n",
        "            outAUROC.append(roc_auc_score(datanpGT[:, i], datanpPRED[:, i]))\n",
        "        return outAUROC\n",
        "\n",
        "    def write_results(self, aurocIndividual, class_ids, prefix='val', mode='a'):\n",
        "        with open(f\"{self.args.save_dir}/results.txt\", mode) as results_file:\n",
        "            aurocMean = aurocIndividual.mean()\n",
        "\n",
        "            results_file.write(f'{prefix} AUROC mean {aurocMean:0.4f}\\n')\n",
        "            for i, class_id in enumerate(class_ids):\n",
        "                results_file.write(f'{self.val_dl.dataset.CLASSES[class_id]} {aurocIndividual[i]:0.4f}\\n')\n",
        "\n",
        "    def print_auroc(self, aurocIndividual, class_ids, prefix='val'):\n",
        "        aurocMean = aurocIndividual.mean()\n",
        "\n",
        "        print (f'{prefix} AUROC mean {aurocMean:0.4f}')\n",
        "\n",
        "        for i, class_id in enumerate(class_ids):\n",
        "            print (f'{self.val_dl.dataset.CLASSES[class_id]} {aurocIndividual[i]:0.4f}')"
      ],
      "metadata": {
        "id": "W5pimYcnlbgy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### argParser"
      ],
      "metadata": {
        "id": "8IbBrmujsoC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Credit: https://github.com/nyuad-cai/CXR-ML-GZSL/blob/master/arguments.py\n",
        "\n",
        "argParser = argparse.ArgumentParser(description='arguments')\n",
        "\n",
        "argParser.add_argument('--data-root', default=DATA_PATH, type=str, help='the path to dataset')\n",
        "argParser.add_argument('--save-dir', default=SAVE_PATH, type=str, help='the path to save the checkpoints')\n",
        "argParser.add_argument('--train-file', default=f'{SPLITS_PATH}/train.txt', type=str, help='the path to train list ')\n",
        "argParser.add_argument('--val-file', default=f'{SPLITS_PATH}/val.txt', type=str, help='the path to val list ')\n",
        "argParser.add_argument('--test-file', default=f'{SPLITS_PATH}/test.txt', type=str, help='the path to test list')\n",
        "\n",
        "argParser.add_argument('--pretrained', dest='pretrained', action='store_true',  help='load imagenet pretrained model')\n",
        "argParser.add_argument('--bce-only', dest='bce_only', help='train with only binary cross entropy loss', action='store_true')\n",
        "\n",
        "argParser.add_argument('--num-classes', default=14, type=int, help='number of classes')\n",
        "argParser.add_argument('--batch-size', default=16, type=int, help='training batch size')\n",
        "argParser.add_argument('--epochs', default=40, type=int, help='number of epochs to train')\n",
        "argParser.add_argument('--vision-backbone', default='densenet121', type=str, help='[densenet121, densenet169, densenet201]')\n",
        "argParser.add_argument('--resume-from', default=None, type=str, help='path to checkpoint to resume the training from')\n",
        "argParser.add_argument('--load-from', default=None, type=str, help='path to checkpoint to load the weights from')\n",
        "\n",
        "argParser.add_argument('--resize', default=256, type=int, help='number of epochs to train')\n",
        "argParser.add_argument('--crop', default=224, type=int, help='number of epochs to train')\n",
        "argParser.add_argument('--lr', default=0.0001, type=float, help='learning rate')\n",
        "argParser.add_argument('--steps', default='20, 40, 60, 80', type=str, help='learning rate decay steps comma separated')\n",
        "\n",
        "argParser.add_argument('--beta-map', default=0.1, type=float, help='learning rate')\n",
        "argParser.add_argument('--beta-con', default=0.1, type=float, help='learning rate')\n",
        "argParser.add_argument('--beta-rank', default=1, type=float, help='learning rate')\n",
        "argParser.add_argument('--neg-penalty', default=0.03, type=float, help='learning rate')\n",
        "\n",
        "argParser.add_argument('--wo-con', dest='wo_con', help='train with out semantic consistency regularizer loss', action='store_true')\n",
        "argParser.add_argument('--wo-map', dest='wo_map', help='train with out alignement loss', action='store_true')\n",
        "\n",
        "argParser.add_argument('--textual-embeddings', default=BIOBERT_PATH, type=str, help='the path to labels embeddings')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHONAIP5soMa",
        "outputId": "c03c900a-5c4c-402f-c8c3-d7dc21d460d3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--textual-embeddings'], dest='textual_embeddings', nargs=None, const=None, default='CXR-ML-GZSL/embeddings/nih_chest_xray_biobert.npy', type=<class 'str'>, choices=None, required=False, help='the path to labels embeddings', metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Testing\n",
        "\n"
      ],
      "metadata": {
        "id": "XrdwFdHCo4qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_training = True"
      ],
      "metadata": {
        "id": "uWwjCH-F55Q2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test with Pretrained Weights"
      ],
      "metadata": {
        "id": "gYsubhrzt-Sq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Credit: https://github.com/nyuad-cai/CXR-ML-GZSL/blob/master/scripts/test_densenet121.sh\n",
        "# Credit: https://github.com/nyuad-cai/CXR-ML-GZSL/blob/master/test.py\n",
        "\n",
        "if run_training:\n",
        "    print(\"Skipping to training\")\n",
        "else:\n",
        "    args = argParser.parse_args(['--load-from', WEIGHTS_PATH])\n",
        "\n",
        "    trainer = ChexnetTrainer(args)\n",
        "\n",
        "    test_ind_auroc = trainer.test()\n",
        "    test_ind_auroc = np.array(test_ind_auroc)\n",
        "\n",
        "    trainer.print_auroc(test_ind_auroc[trainer.test_dl.dataset.seen_class_ids], trainer.test_dl.dataset.seen_class_ids, prefix='\\ntest_seen')\n",
        "    trainer.print_auroc(test_ind_auroc[trainer.test_dl.dataset.unseen_class_ids], trainer.test_dl.dataset.unseen_class_ids, prefix='\\ntest_unseen')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWw5DgZco40u",
        "outputId": "f4ed802c-77fc-4166-970c-df4857933353"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping to training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "yjkIuWMV5dXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Credit: https://github.com/nyuad-cai/CXR-ML-GZSL/blob/master/scripts/train_densenet121.sh\n",
        "# Credit: https://github.com/nyuad-cai/CXR-ML-GZSL/blob/master/train.py\n",
        "\n",
        "if not run_training:\n",
        "    print(\"Skipping training\")\n",
        "else:\n",
        "    args = argParser.parse_args(['--epochs', '1'])\n",
        "\n",
        "    seed = 1002\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    trainer = ChexnetTrainer(args)\n",
        "    trainer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqUqQfFW5dg-",
        "outputId": "0cf0f3dd-1793-4eb8-ebaf-e4baa2652afc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data partition path: CXR-ML-GZSL/dataset_splits/train.txt\n",
            "Number of images: 30758\n",
            "Number of max labels per image: 7\n",
            "Number of classes: 10\n",
            "data partition path: CXR-ML-GZSL/dataset_splits/val.txt\n",
            "Number of images: 4474\n",
            "Number of max labels per image: 6\n",
            "Number of classes: 10\n",
            "data partition path: CXR-ML-GZSL/dataset_splits/test.txt\n",
            "Number of images: 10510\n",
            "Number of max labels per image: 7\n",
            "Number of classes: 14\n",
            "['Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia', 'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia']\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:54        [0009/1923] lr: \t1.0000E-04 loss: \t0.37229\n",
            " epoch [0001 / 0001] eta: 0 Days 0:3:19        [0019/1923] lr: \t1.0000E-04 loss: \t0.29935\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:50        [0029/1923] lr: \t1.0000E-04 loss: \t0.27061\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:53        [0039/1923] lr: \t1.0000E-04 loss: \t0.26288\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:50        [0049/1923] lr: \t1.0000E-04 loss: \t0.25798\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:54        [0059/1923] lr: \t1.0000E-04 loss: \t0.25464\n",
            " epoch [0001 / 0001] eta: 0 Days 0:3:16        [0069/1923] lr: \t1.0000E-04 loss: \t0.24971\n",
            " epoch [0001 / 0001] eta: 0 Days 0:3:48        [0079/1923] lr: \t1.0000E-04 loss: \t0.24631\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:47        [0089/1923] lr: \t1.0000E-04 loss: \t0.24438\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:51        [0099/1923] lr: \t1.0000E-04 loss: \t0.24513\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:49        [0109/1923] lr: \t1.0000E-04 loss: \t0.24389\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:52        [0119/1923] lr: \t1.0000E-04 loss: \t0.24093\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:49        [0129/1923] lr: \t1.0000E-04 loss: \t0.24154\n",
            " epoch [0001 / 0001] eta: 0 Days 0:3:17        [0139/1923] lr: \t1.0000E-04 loss: \t0.23959\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:47        [0149/1923] lr: \t1.0000E-04 loss: \t0.23807\n",
            " epoch [0001 / 0001] eta: 0 Days 0:4:8         [0159/1923] lr: \t1.0000E-04 loss: \t0.23652\n",
            " epoch [0001 / 0001] eta: 0 Days 0:3:37        [0169/1923] lr: \t1.0000E-04 loss: \t0.23598\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:46        [0179/1923] lr: \t1.0000E-04 loss: \t0.23502\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:44        [0189/1923] lr: \t1.0000E-04 loss: \t0.23385\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:41        [0199/1923] lr: \t1.0000E-04 loss: \t0.23382\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:39        [0209/1923] lr: \t1.0000E-04 loss: \t0.23436\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:36        [0219/1923] lr: \t1.0000E-04 loss: \t0.23454\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:38        [0229/1923] lr: \t1.0000E-04 loss: \t0.23320\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:36        [0239/1923] lr: \t1.0000E-04 loss: \t0.23291\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:42        [0249/1923] lr: \t1.0000E-04 loss: \t0.23188\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:40        [0259/1923] lr: \t1.0000E-04 loss: \t0.23149\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:36        [0269/1923] lr: \t1.0000E-04 loss: \t0.23361\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:32        [0279/1923] lr: \t1.0000E-04 loss: \t0.23359\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:31        [0289/1923] lr: \t1.0000E-04 loss: \t0.23370\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:27        [0299/1923] lr: \t1.0000E-04 loss: \t0.23326\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:31        [0309/1923] lr: \t1.0000E-04 loss: \t0.23307\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:31        [0319/1923] lr: \t1.0000E-04 loss: \t0.23233\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:29        [0329/1923] lr: \t1.0000E-04 loss: \t0.23190\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:28        [0339/1923] lr: \t1.0000E-04 loss: \t0.23159\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:29        [0349/1923] lr: \t1.0000E-04 loss: \t0.23122\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:28        [0359/1923] lr: \t1.0000E-04 loss: \t0.23005\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:26        [0369/1923] lr: \t1.0000E-04 loss: \t0.22977\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:30        [0379/1923] lr: \t1.0000E-04 loss: \t0.22984\n",
            " epoch [0001 / 0001] eta: 0 Days 0:11:12       [0389/1923] lr: \t1.0000E-04 loss: \t0.22914\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:23        [0399/1923] lr: \t1.0000E-04 loss: \t0.22839\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:16        [0409/1923] lr: \t1.0000E-04 loss: \t0.22795\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:21        [0419/1923] lr: \t1.0000E-04 loss: \t0.22820\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:14        [0429/1923] lr: \t1.0000E-04 loss: \t0.22762\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:19        [0439/1923] lr: \t1.0000E-04 loss: \t0.22757\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:26        [0449/1923] lr: \t1.0000E-04 loss: \t0.22715\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:14        [0459/1923] lr: \t1.0000E-04 loss: \t0.22679\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:16        [0469/1923] lr: \t1.0000E-04 loss: \t0.22658\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:15        [0479/1923] lr: \t1.0000E-04 loss: \t0.22615\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:17        [0489/1923] lr: \t1.0000E-04 loss: \t0.22574\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:20        [0499/1923] lr: \t1.0000E-04 loss: \t0.22544\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:14        [0509/1923] lr: \t1.0000E-04 loss: \t0.22521\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:9         [0519/1923] lr: \t1.0000E-04 loss: \t0.22557\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:6         [0529/1923] lr: \t1.0000E-04 loss: \t0.22552\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:8         [0539/1923] lr: \t1.0000E-04 loss: \t0.22551\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:17        [0549/1923] lr: \t1.0000E-04 loss: \t0.22531\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:17        [0559/1923] lr: \t1.0000E-04 loss: \t0.22496\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:58        [0569/1923] lr: \t1.0000E-04 loss: \t0.22481\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:32        [0579/1923] lr: \t1.0000E-04 loss: \t0.22484\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:23        [0589/1923] lr: \t1.0000E-04 loss: \t0.22487\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:20        [0599/1923] lr: \t1.0000E-04 loss: \t0.22496\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:11        [0609/1923] lr: \t1.0000E-04 loss: \t0.22500\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:0         [0619/1923] lr: \t1.0000E-04 loss: \t0.22518\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:59        [0629/1923] lr: \t1.0000E-04 loss: \t0.22533\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:58        [0639/1923] lr: \t1.0000E-04 loss: \t0.22532\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:59        [0649/1923] lr: \t1.0000E-04 loss: \t0.22560\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:2         [0659/1923] lr: \t1.0000E-04 loss: \t0.22535\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:58        [0669/1923] lr: \t1.0000E-04 loss: \t0.22501\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:11        [0679/1923] lr: \t1.0000E-04 loss: \t0.22503\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:13        [0689/1923] lr: \t1.0000E-04 loss: \t0.22512\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:54        [0699/1923] lr: \t1.0000E-04 loss: \t0.22492\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:58        [0709/1923] lr: \t1.0000E-04 loss: \t0.22457\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:53        [0719/1923] lr: \t1.0000E-04 loss: \t0.22456\n",
            " epoch [0001 / 0001] eta: 0 Days 0:2:5         [0729/1923] lr: \t1.0000E-04 loss: \t0.22464\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:50        [0739/1923] lr: \t1.0000E-04 loss: \t0.22438\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:47        [0749/1923] lr: \t1.0000E-04 loss: \t0.22423\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:48        [0759/1923] lr: \t1.0000E-04 loss: \t0.22454\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:45        [0769/1923] lr: \t1.0000E-04 loss: \t0.22446\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:45        [0779/1923] lr: \t1.0000E-04 loss: \t0.22451\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:43        [0789/1923] lr: \t1.0000E-04 loss: \t0.22447\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:40        [0799/1923] lr: \t1.0000E-04 loss: \t0.22428\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:43        [0809/1923] lr: \t1.0000E-04 loss: \t0.22417\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:48        [0819/1923] lr: \t1.0000E-04 loss: \t0.22378\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:44        [0829/1923] lr: \t1.0000E-04 loss: \t0.22383\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:48        [0839/1923] lr: \t1.0000E-04 loss: \t0.22360\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:39        [0849/1923] lr: \t1.0000E-04 loss: \t0.22349\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:39        [0859/1923] lr: \t1.0000E-04 loss: \t0.22361\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:37        [0869/1923] lr: \t1.0000E-04 loss: \t0.22358\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:37        [0879/1923] lr: \t1.0000E-04 loss: \t0.22367\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:35        [0889/1923] lr: \t1.0000E-04 loss: \t0.22358\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:34        [0899/1923] lr: \t1.0000E-04 loss: \t0.22351\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:35        [0909/1923] lr: \t1.0000E-04 loss: \t0.22334\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:34        [0919/1923] lr: \t1.0000E-04 loss: \t0.22322\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:40        [0929/1923] lr: \t1.0000E-04 loss: \t0.22291\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:42        [0939/1923] lr: \t1.0000E-04 loss: \t0.22266\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:35        [0949/1923] lr: \t1.0000E-04 loss: \t0.22223\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:49        [0959/1923] lr: \t1.0000E-04 loss: \t0.22195\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:29        [0969/1923] lr: \t1.0000E-04 loss: \t0.22184\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:31        [0979/1923] lr: \t1.0000E-04 loss: \t0.22166\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:27        [0989/1923] lr: \t1.0000E-04 loss: \t0.22172\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:26        [0999/1923] lr: \t1.0000E-04 loss: \t0.22186\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:25        [1009/1923] lr: \t1.0000E-04 loss: \t0.22193\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:25        [1019/1923] lr: \t1.0000E-04 loss: \t0.22204\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:45        [1029/1923] lr: \t1.0000E-04 loss: \t0.22209\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:22        [1039/1923] lr: \t1.0000E-04 loss: \t0.22203\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:20        [1049/1923] lr: \t1.0000E-04 loss: \t0.22222\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:20        [1059/1923] lr: \t1.0000E-04 loss: \t0.22230\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:21        [1069/1923] lr: \t1.0000E-04 loss: \t0.22210\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:31        [1079/1923] lr: \t1.0000E-04 loss: \t0.22196\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:14        [1089/1923] lr: \t1.0000E-04 loss: \t0.22201\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:17        [1099/1923] lr: \t1.0000E-04 loss: \t0.22195\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:16        [1109/1923] lr: \t1.0000E-04 loss: \t0.22183\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:14        [1119/1923] lr: \t1.0000E-04 loss: \t0.22186\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:15        [1129/1923] lr: \t1.0000E-04 loss: \t0.22175\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:11        [1139/1923] lr: \t1.0000E-04 loss: \t0.22155\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:9         [1149/1923] lr: \t1.0000E-04 loss: \t0.22140\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:8         [1159/1923] lr: \t1.0000E-04 loss: \t0.22148\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:8         [1169/1923] lr: \t1.0000E-04 loss: \t0.22162\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:7         [1179/1923] lr: \t1.0000E-04 loss: \t0.22151\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:5         [1189/1923] lr: \t1.0000E-04 loss: \t0.22159\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:11        [1199/1923] lr: \t1.0000E-04 loss: \t0.22180\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:6         [1209/1923] lr: \t1.0000E-04 loss: \t0.22155\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:9         [1219/1923] lr: \t1.0000E-04 loss: \t0.22130\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:4         [1229/1923] lr: \t1.0000E-04 loss: \t0.22134\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:3         [1239/1923] lr: \t1.0000E-04 loss: \t0.22123\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:2         [1249/1923] lr: \t1.0000E-04 loss: \t0.22102\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:10        [1259/1923] lr: \t1.0000E-04 loss: \t0.22106\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:6         [1269/1923] lr: \t1.0000E-04 loss: \t0.22115\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:11        [1279/1923] lr: \t1.0000E-04 loss: \t0.22102\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:58        [1289/1923] lr: \t1.0000E-04 loss: \t0.22082\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:11        [1299/1923] lr: \t1.0000E-04 loss: \t0.22057\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:8         [1309/1923] lr: \t1.0000E-04 loss: \t0.22037\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:10        [1319/1923] lr: \t1.0000E-04 loss: \t0.22028\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:55        [1329/1923] lr: \t1.0000E-04 loss: \t0.22028\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:0         [1339/1923] lr: \t1.0000E-04 loss: \t0.22020\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:0         [1349/1923] lr: \t1.0000E-04 loss: \t0.22017\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:52        [1359/1923] lr: \t1.0000E-04 loss: \t0.22000\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:1         [1369/1923] lr: \t1.0000E-04 loss: \t0.21992\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:52        [1379/1923] lr: \t1.0000E-04 loss: \t0.21976\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:52        [1389/1923] lr: \t1.0000E-04 loss: \t0.21974\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:50        [1399/1923] lr: \t1.0000E-04 loss: \t0.21959\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:48        [1409/1923] lr: \t1.0000E-04 loss: \t0.21960\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:48        [1419/1923] lr: \t1.0000E-04 loss: \t0.21951\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:46        [1429/1923] lr: \t1.0000E-04 loss: \t0.21968\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:46        [1439/1923] lr: \t1.0000E-04 loss: \t0.21983\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:48        [1449/1923] lr: \t1.0000E-04 loss: \t0.21979\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:55        [1459/1923] lr: \t1.0000E-04 loss: \t0.21961\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:42        [1469/1923] lr: \t1.0000E-04 loss: \t0.21964\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:45        [1479/1923] lr: \t1.0000E-04 loss: \t0.21961\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:40        [1489/1923] lr: \t1.0000E-04 loss: \t0.21963\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:43        [1499/1923] lr: \t1.0000E-04 loss: \t0.21963\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:39        [1509/1923] lr: \t1.0000E-04 loss: \t0.21968\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:38        [1519/1923] lr: \t1.0000E-04 loss: \t0.21969\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:36        [1529/1923] lr: \t1.0000E-04 loss: \t0.21984\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:35        [1539/1923] lr: \t1.0000E-04 loss: \t0.21990\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:35        [1549/1923] lr: \t1.0000E-04 loss: \t0.21985\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:33        [1559/1923] lr: \t1.0000E-04 loss: \t0.21985\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:35        [1569/1923] lr: \t1.0000E-04 loss: \t0.21995\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:32        [1579/1923] lr: \t1.0000E-04 loss: \t0.21995\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:31        [1589/1923] lr: \t1.0000E-04 loss: \t0.21990\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:30        [1599/1923] lr: \t1.0000E-04 loss: \t0.21992\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:28        [1609/1923] lr: \t1.0000E-04 loss: \t0.21977\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:31        [1619/1923] lr: \t1.0000E-04 loss: \t0.21985\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:28        [1629/1923] lr: \t1.0000E-04 loss: \t0.21990\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:26        [1639/1923] lr: \t1.0000E-04 loss: \t0.21984\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:25        [1649/1923] lr: \t1.0000E-04 loss: \t0.21980\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:24        [1659/1923] lr: \t1.0000E-04 loss: \t0.21970\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:23        [1669/1923] lr: \t1.0000E-04 loss: \t0.21961\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:22        [1679/1923] lr: \t1.0000E-04 loss: \t0.21955\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:22        [1689/1923] lr: \t1.0000E-04 loss: \t0.21963\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:25        [1699/1923] lr: \t1.0000E-04 loss: \t0.21962\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:22        [1709/1923] lr: \t1.0000E-04 loss: \t0.21958\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:18        [1719/1923] lr: \t1.0000E-04 loss: \t0.21954\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:17        [1729/1923] lr: \t1.0000E-04 loss: \t0.21954\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:16        [1739/1923] lr: \t1.0000E-04 loss: \t0.21956\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:15        [1749/1923] lr: \t1.0000E-04 loss: \t0.21951\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:15        [1759/1923] lr: \t1.0000E-04 loss: \t0.21948\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:14        [1769/1923] lr: \t1.0000E-04 loss: \t0.21946\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:13        [1779/1923] lr: \t1.0000E-04 loss: \t0.21940\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:12        [1789/1923] lr: \t1.0000E-04 loss: \t0.21926\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:14        [1799/1923] lr: \t1.0000E-04 loss: \t0.21927\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:11        [1809/1923] lr: \t1.0000E-04 loss: \t0.21944\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:10        [1819/1923] lr: \t1.0000E-04 loss: \t0.21931\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:10        [1829/1923] lr: \t1.0000E-04 loss: \t0.21923\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:7         [1839/1923] lr: \t1.0000E-04 loss: \t0.21906\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:6         [1849/1923] lr: \t1.0000E-04 loss: \t0.21894\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:5         [1859/1923] lr: \t1.0000E-04 loss: \t0.21891\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:4         [1869/1923] lr: \t1.0000E-04 loss: \t0.21878\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:4         [1879/1923] lr: \t1.0000E-04 loss: \t0.21876\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:3         [1889/1923] lr: \t1.0000E-04 loss: \t0.21866\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:2         [1899/1923] lr: \t1.0000E-04 loss: \t0.21858\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:1         [1909/1923] lr: \t1.0000E-04 loss: \t0.21855\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:0         [1919/1923] lr: \t1.0000E-04 loss: \t0.21849\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28/28 [00:45<00:00,  1.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving last_epoch checkpoint\n",
            "saving best_auroc checkpoint\n",
            "saving min_loss checkpoint\n",
            "val AUROC mean 0.4950\n",
            "Atelectasis 0.4816\n",
            "Cardiomegaly 0.4898\n",
            "Effusion 0.4865\n",
            "Infiltration 0.5510\n",
            "Mass 0.5029\n",
            "Nodule 0.5072\n",
            "Pneumothorax 0.4904\n",
            "Consolidation 0.6092\n",
            "Pleural_Thickening 0.4485\n",
            "Hernia 0.3833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 219/219 [01:12<00:00,  3.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "test_seen AUROC mean 0.4802\n",
            "Atelectasis 0.4867\n",
            "Effusion 0.4725\n",
            "Infiltration 0.5669\n",
            "Mass 0.4641\n",
            "Nodule 0.4890\n",
            "Pneumothorax 0.4818\n",
            "Consolidation 0.6127\n",
            "Cardiomegaly 0.4674\n",
            "Pleural_Thickening 0.4412\n",
            "Hernia 0.3199\n",
            "\n",
            "test_unseen AUROC mean 0.5410\n",
            "Edema 0.7173\n",
            "Pneumonia 0.5759\n",
            "Emphysema 0.4811\n",
            "Fibrosis 0.3897\n",
            "best epoch 1 best auroc 0.4950447860261833 loss 0.212920 auroc at min loss 0.4950\n"
          ]
        }
      ]
    }
  ]
}