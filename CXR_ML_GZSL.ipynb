{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CXR-ML-GZSL"
      ],
      "metadata": {
        "id": "5-a-wVNbsbVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "The goal of this notebook is to reproduce the findings of the paper, \"Multi-Label Generalized Zero Shot Learning for the Classification of Disease in Chest Radiographs\" using the provided code.\n",
        "\n",
        "* Paper: https://arxiv.org/abs/2107.06563\n",
        "* Code: https://github.com/nyuad-cai/CXR-ML-GZSL/\n",
        "\n",
        "The provided code is four years old, so some changes were needed to resolve deprecation warnings and errors. Additionally, I cleaned up some imports, whitespace, etc. and adapted the code for a Jupyter notebook. However, my goal was to use the code as is in most cases.\n",
        "\n",
        "**Note**: I used [Google Colab](https://colab.research.google.com/) to run this notebook. I had to use the paid version, as the free version does not provide enough RAM or a powerful enough GPU."
      ],
      "metadata": {
        "id": "POJObkEsliMM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JCSEZ3d_rbYa"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "from datetime import datetime, timedelta\n",
        "import glob\n",
        "import hashlib\n",
        "import os\n",
        "import tarfile\n",
        "import time\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import kl_div, softmax, log_softmax\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "The paper used a dataset developed by another paper, initially known as `ChestX-ray8`, but then renamed to `ChestX-ray14` when the dataset was expanded from eight to fourteen distinct disease labels.\n",
        "\n",
        "* Paper: https://arxiv.org/abs/1705.02315\n",
        "* Dataset: https://nihcc.app.box.com/v/ChestXray-NIHCC/folder/36938765345\n",
        "\n",
        "This notebook provides two different ways of uploading the dataset to session storage.\n",
        "\n",
        "1. The default is to download the image dataset from the official source. The dataset is over 42 GB, which takes 20+ minutes to download to session storage. Additionally, there are several metadata files from the `ChestX-ray14` dataset and `CXR-ML-GZSL` model repository that you will be prompted to manually upload to session storage. Be aware that session storage is wiped for each new session, including runtime changes like switching to a GPU.\n",
        "2. The other option is to download the [this](https://drive.google.com/file/d/11mk4sq5KgMpujdPxxQc8lnkqV18mQvxN/view) mirror of the dataset and metadata, save it to your Google Drive in `MyDrive`, and mount your Google Drive to the Google Colab runtime. Once the dataset is in Google Drive, it only takes 7+ minutes to copy to session storage and unzip."
      ],
      "metadata": {
        "id": "SD2XmBZYt43o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_PATH = 'CXR-ML-GZSL'\n",
        "\n",
        "if not os.path.exists(ROOT_PATH):\n",
        "    drive_path = 'drive/MyDrive/CXR-ML-GZSL.zip'\n",
        "    if os.path.isfile(drive_path):\n",
        "        with zipfile.ZipFile(drive_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(ROOT_PATH)\n",
        "        print(\"Data downloaded from Google Drive\")\n",
        "    else:\n",
        "        os.mkdir(ROOT_PATH)"
      ],
      "metadata": {
        "id": "BuKlfmq-6ZJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9226fff-eabe-40e2-d7c9-8f391cc26689"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data downloaded from Google Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_PATH = f'{ROOT_PATH}/checkpoints'\n",
        "WEIGHTS_PATH = f'{SAVE_PATH}/best_auroc_checkpoint.pth.tar'\n",
        "\n",
        "if os.path.isfile(WEIGHTS_PATH):\n",
        "    print(\"Using existing pretrained weights\")\n",
        "else:\n",
        "    if not os.path.exists(SAVE_PATH): os.mkdir(SAVE_PATH)\n",
        "    link = 'https://drive.google.com/file/d/17ioJMW3qNx1Ktmr-hXn-eqp431cm49Rm/view'\n",
        "    assert False, f\"Please download the pretrained weights from {link} and place them in {SAVE_PATH}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfU2dFaLrHxn",
        "outputId": "9888f7ad-3427-4145-e129-e26320256aff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using existing pretrained weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = f'{ROOT_PATH}/data/nih_chest_xrays'\n",
        "IMAGE_PATH = f'{DATA_PATH}/images'\n",
        "\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    os.makedirs(DATA_PATH)\n",
        "\n",
        "if os.path.isfile(f'{DATA_PATH}/Data_Entry_2017_v2020.csv'):\n",
        "    print(\"Using existing data entry file\")\n",
        "else:\n",
        "    assert False, f\"Please download Data_Entry_2017_v2020.csv from the ChestX-ray14 dataset link and place it in {DATA_PATH}\"\n",
        "\n",
        "if os.path.exists(IMAGE_PATH):\n",
        "    print(\"Using existing image data\")\n",
        "else:\n",
        "    # Credit: https://nihcc.app.box.com/v/ChestXray-NIHCC/file/371647823217\n",
        "    links = [\n",
        "        'https://nihcc.box.com/shared/static/vfk49d74nhbxq3nqjg0900w5nvkorp5c.gz',\n",
        "        'https://nihcc.box.com/shared/static/i28rlmbvmfjbl8p2n3ril0pptcmcu9d1.gz',\n",
        "        'https://nihcc.box.com/shared/static/f1t00wrtdk94satdfb9olcolqx20z2jp.gz',\n",
        "        'https://nihcc.box.com/shared/static/0aowwzs5lhjrceb3qp67ahp0rd1l1etg.gz',\n",
        "        'https://nihcc.box.com/shared/static/v5e3goj22zr6h8tzualxfsqlqaygfbsn.gz',\n",
        "        'https://nihcc.box.com/shared/static/asi7ikud9jwnkrnkj99jnpfkjdes7l6l.gz',\n",
        "        'https://nihcc.box.com/shared/static/jn1b4mw4n6lnh74ovmcjb8y48h8xj07n.gz',\n",
        "        'https://nihcc.box.com/shared/static/tvpxmn7qyrgl0w8wfh9kqfjskv6nmm1j.gz',\n",
        "        'https://nihcc.box.com/shared/static/upyy3ml7qdumlgk2rfcvlb9k6gvqq2pj.gz',\n",
        "        'https://nihcc.box.com/shared/static/l6nilvfa9cg3s28tqv1qc1olm3gnz54p.gz',\n",
        "        'https://nihcc.box.com/shared/static/hhq8fkdgvcari67vfhs7ppg2w6ni4jze.gz',\n",
        "        'https://nihcc.box.com/shared/static/ioqwiy20ihqwyr8pf4c24eazhh281pbu.gz'\n",
        "    ]\n",
        "\n",
        "    # Credit: https://nihcc.app.box.com/v/ChestXray-NIHCC/file/249502714403\n",
        "    md5_checksums = [\n",
        "        'fe8ed0a6961412fddcbb3603c11b3698',\n",
        "        'ab07a2d7cbe6f65ddd97b4ed7bde10bf',\n",
        "        '2301d03bde4c246388bad3876965d574',\n",
        "        '9f1b7f5aae01b13f4bc8e2c44a4b8ef6',\n",
        "        '1861f3cd0ef7734df8104f2b0309023b',\n",
        "        '456b53a8b351afd92a35bc41444c58c8',\n",
        "        '1075121ea20a137b87f290d6a4a5965e',\n",
        "        'b61f34cec3aa69f295fbb593cbd9d443',\n",
        "        '442a3caa61ae9b64e61c561294d1e183',\n",
        "        '09ec81c4c31e32858ad8cf965c494b74',\n",
        "        '499aefc67207a5a97692424cf5dbeed5',\n",
        "        'dc9fda1757c2de0032b63347a7d2895c'\n",
        "    ]\n",
        "\n",
        "    for idx, link in enumerate(links):\n",
        "        fn = os.path.join(DATA_PATH, 'images_%02d.tar.gz' % (idx + 1))\n",
        "\n",
        "        print(f'Downloading {fn}...')\n",
        "        urllib.request.urlretrieve(link, fn)\n",
        "\n",
        "        print(f\"Checking MD5 checksum for {fn}...\")\n",
        "        with open(fn, 'rb') as f:\n",
        "            file_md5 = hashlib.md5(f.read()).hexdigest()\n",
        "\n",
        "        assert file_md5 == md5_checksums[idx], \"Invalid MD5 checksum\"\n",
        "\n",
        "        print(f\"Extracting {fn}...\")\n",
        "        with tarfile.open(fn, 'r:gz') as tar:\n",
        "            tar.extractall(path=DATA_PATH)\n",
        "\n",
        "        print(f\"Deleting {fn}...\")\n",
        "        os.remove(fn)\n",
        "\n",
        "    assert len([f for f in os.listdir(IMAGE_PATH) if os.path.isfile(os.path.join(IMAGE_PATH, f))]) == 112120, \"Dataset is not the expected size!\"\n",
        "    print(\"Image data download complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZEngFTmsa-3",
        "outputId": "4dd1cc54-b7d3-41c8-ac10-87deb4d7fea2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using existing data entry file\n",
            "Using existing image data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SPLITS_PATH = f'{ROOT_PATH}/dataset_splits'\n",
        "\n",
        "files = ['train.txt', 'test.txt', 'val.txt']\n",
        "if all(os.path.exists(f'{SPLITS_PATH}/{f}') for f in files):\n",
        "    print(\"Using existing dataset splits\")\n",
        "else:\n",
        "    if not os.path.exists(SPLITS_PATH): os.mkdir(SPLITS_PATH)\n",
        "    link = 'https://github.com/nyuad-cai/CXR-ML-GZSL/tree/master/dataset_splits'\n",
        "    assert False, f\"Please download the dataset splits from {link} and place them in {SPLITS_PATH}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgnZHGyyx4So",
        "outputId": "08d8e5fc-22e3-4d03-bad6-39e0ba9aafec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using existing dataset splits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDINGS_PATH = f'{ROOT_PATH}/embeddings'\n",
        "BIOBERT_PATH = f'{EMBEDDINGS_PATH}/nih_chest_xray_biobert.npy'\n",
        "\n",
        "if os.path.isfile(BIOBERT_PATH):\n",
        "    print(\"Using existing text embeddings\")\n",
        "else:\n",
        "    if not os.path.exists(EMBEDDINGS_PATH): os.mkdir(EMBEDDINGS_PATH)\n",
        "    link = 'https://github.com/nyuad-cai/CXR-ML-GZSL/blob/master/embeddings/nih_chest_xray_biobert.npy'\n",
        "    assert False, f\"Please download the text embeddings from {link} and place them in {EMBEDDINGS_PATH}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0EaBbm3zjyT",
        "outputId": "630d43b9-5d8b-4b9b-902c-3beb22af879b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using existing text embeddings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "RwIGiX-JrmdI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KLDivLoss"
      ],
      "metadata": {
        "id": "aDcFrrhTCo6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Credit: https://github.com/nyuad-cai/CXR-ML-GZSL/blob/master/loss.py\n",
        "\n",
        "class KLDivLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.2):\n",
        "        super(KLDivLoss, self).__init__()\n",
        "\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, emb1, emb2):\n",
        "        emb1 = softmax(emb1/self.temperature, dim=1).detach()\n",
        "        emb2 = log_softmax(emb2/self.temperature, dim=1)\n",
        "        loss_kldiv = kl_div(emb2, emb1, reduction='none')\n",
        "        loss_kldiv = torch.sum(loss_kldiv, dim=1)\n",
        "        loss_kldiv = torch.mean(loss_kldiv)\n",
        "\n",
        "        return loss_kldiv"
      ],
      "metadata": {
        "id": "mamyKMa4CoTj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RankingLoss"
      ],
      "metadata": {
        "id": "uY2t76EAiFBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Credit: https://github.com/nyuad-cai/CXR-ML-GZSL/blob/master/loss.py\n",
        "\n",
        "class RankingLoss(nn.Module):\n",
        "    def __init__(self, neg_penalty=0.03):\n",
        "        super(RankingLoss, self).__init__()\n",
        "\n",
        "        self.neg_penalty = neg_penalty\n",
        "\n",
        "    def forward(self, ranks, labels, class_ids_loaded, device):\n",
        "        '''\n",
        "        for each correct it should be higher then the absence\n",
        "        '''\n",
        "        labels = labels[:, class_ids_loaded]\n",
        "        ranks_loaded = ranks[:, class_ids_loaded]\n",
        "        neg_labels = 1+(labels*-1)\n",
        "        loss_rank = torch.zeros(1).to(device)\n",
        "        for i in range(len(labels)):\n",
        "            correct = ranks_loaded[i, labels[i]==1]\n",
        "            wrong = ranks_loaded[i, neg_labels[i]==1]\n",
        "            correct = correct.reshape((-1, 1)).repeat((1, len(wrong)))\n",
        "            wrong = wrong.repeat(len(correct)).reshape(len(correct), -1)\n",
        "            image_level_penalty = ((self.neg_penalty+wrong) - correct)\n",
        "            image_level_penalty[image_level_penalty<0]=0\n",
        "            loss_rank += image_level_penalty.sum()\n",
        "        loss_rank /=len(labels)\n",
        "\n",
        "        return loss_rank"
      ],
      "metadata": {
        "id": "HGFT0qLAiFIx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CosineLoss"
      ],
      "metadata": {
        "id": "RFX6rw5NiFc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Credit: https://github.com/nyuad-cai/CXR-ML-GZSL/blob/master/loss.py\n",
        "\n",
        "class CosineLoss(nn.Module):\n",
        "    def forward(self, t_emb, v_emb ):\n",
        "        a_norm = v_emb / v_emb.norm(dim=1)[:, None]\n",
        "        b_norm = t_emb / t_emb.norm(dim=1)[:, None]\n",
        "        loss = 1 - torch.mean(torch.diagonal(torch.mm(a_norm, b_norm.t()), 0))\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "pwi6_lTOiFkI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ZSLNet"
      ],
      "metadata": {
        "id": "ZsODvFq8iicH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Credit: https://github.com/nyuad-cai/CXR-ML-GZSL/blob/master/zsl_models.py\n",
        "\n",
        "class ZSLNet(nn.Module):\n",
        "    def __init__(self, args, textual_embeddings=None, device='cpu'):\n",
        "        super(ZSLNet, self).__init__()\n",
        "        self.args = args\n",
        "        self.device = device\n",
        "        self.vision_backbone = getattr(torchvision.models, self.args.vision_backbone)(pretrained=self.args.pretrained)\n",
        "        # remove classification layer from visual encoder\n",
        "        classifiers = [ 'classifier', 'fc']\n",
        "        for classifier in classifiers:\n",
        "            cls_layer = getattr(self.vision_backbone, classifier, None)\n",
        "            if cls_layer is None:\n",
        "                continue\n",
        "            d_visual = cls_layer.in_features\n",
        "            setattr(self.vision_backbone, classifier, nn.Identity(d_visual))\n",
        "            break\n",
        "\n",
        "        pretrained_encoder = False\n",
        "        if pretrained_encoder:\n",
        "            self.vision_backbone.classifier = nn.Identity(d_visual)\n",
        "\n",
        "            path = 'checkpoints/bce_only_imagenet/last_epoch_checkpoint.pth.tar'\n",
        "\n",
        "            self.classifier = nn.Sequential(nn.Linear(d_visual, self.args.num_classes), nn.Sigmoid())\n",
        "            checkpoint = torch.load(path)\n",
        "            self.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "            for p in self.vision_backbone.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        if self.args.bce_only:\n",
        "            self.bce_loss = torch.nn.BCELoss(size_average=True)\n",
        "            self.classifier = nn.Sequential(nn.Linear(d_visual, self.args.num_classes), nn.Sigmoid())\n",
        "        else:\n",
        "            self.emb_loss = CosineLoss()\n",
        "            self.ranking_loss = RankingLoss(neg_penalty=self.args.neg_penalty)\n",
        "            self.textual_embeddings = textual_embeddings\n",
        "            d_textual = self.textual_embeddings.shape[-1]\n",
        "\n",
        "            self.textual_embeddings = torch.from_numpy(self.textual_embeddings).to(self.device)\n",
        "\n",
        "            self.fc_v = nn.Sequential(\n",
        "                nn.Linear(d_visual, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(512, 256),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(256, 128),\n",
        "            )\n",
        "\n",
        "            self.fc_t = nn.Sequential(\n",
        "                nn.Linear(d_textual, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(512, 256),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(256, 128)\n",
        "            )\n",
        "\n",
        "    def forward(self, x, labels=None, epoch=0, n_crops=0, bs=16):\n",
        "        if self.args.bce_only:\n",
        "            return self.forward_bce_only(x, labels=labels, n_crops=n_crops, bs=bs)\n",
        "        else:\n",
        "            return self.forward_ranking(x, labels=labels, epoch=epoch, n_crops=n_crops, bs=bs)\n",
        "\n",
        "    def forward_bce_only(self, x, labels=None, n_crops=0, bs=16):\n",
        "        lossvalue_bce = torch.zeros(1).to(self.device)\n",
        "\n",
        "        visual_feats = self.vision_backbone(x)\n",
        "        preds = self.classifier(visual_feats)\n",
        "\n",
        "        if labels is not None:\n",
        "            lossvalue_bce = self.bce_loss(preds, labels)\n",
        "\n",
        "        return preds, lossvalue_bce, f'bce:\\t {lossvalue_bce.item():0.4f}'\n",
        "\n",
        "    def forward_ranking(self, x, labels=None, epoch=0, n_crops=0, bs=16):\n",
        "        loss_rank = torch.zeros(1).to(self.device)\n",
        "        loss_allignment_cos = torch.zeros(1).to(self.device)\n",
        "        loss_mapping_consistency = torch.zeros(1).to(self.device)\n",
        "\n",
        "        visual_feats = self.vision_backbone(x)\n",
        "        visual_feats = self.fc_v(visual_feats)\n",
        "        text_feats = self.fc_t(self.textual_embeddings)\n",
        "\n",
        "        if not self.args.wo_con and epoch >= 0:\n",
        "            text_mapped_sim = self.sim_score(text_feats, text_feats.detach())\n",
        "            text_orig_sim = self.sim_score(self.textual_embeddings, self.textual_embeddings)\n",
        "            loss_mapping_consistency = torch.abs(text_orig_sim - text_mapped_sim).mean()\n",
        "\n",
        "        if labels is not None:\n",
        "            mapped_visual, mapped_text = self.map_visual_text(visual_feats, labels, text_feats)\n",
        "            if mapped_visual is not None and not self.args.wo_map and epoch >= 0:\n",
        "                loss_allignment_cos = self.emb_loss(mapped_text, mapped_visual)\n",
        "\n",
        "        ranks = self.sim_score(visual_feats, text_feats)\n",
        "        if n_crops > 0:\n",
        "            ranks = ranks.view(bs, n_crops, -1).mean(1)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_rank = self.ranking_loss(ranks, labels, self.class_ids_loaded, self.device)\n",
        "        loss_allignment_cos = (self.args.beta_map * loss_allignment_cos)\n",
        "        loss_rank = (self.args.beta_rank * loss_rank)\n",
        "        loss_mapping_consistency = (self.args.beta_con * loss_mapping_consistency)\n",
        "        losses = loss_rank + loss_mapping_consistency + 0.0*loss_allignment_cos\n",
        "\n",
        "        return ranks, losses\n",
        "\n",
        "    def sim_score(self, a, b):\n",
        "        a_norm = a / a.norm(dim=1)[:, None]\n",
        "        b_norm = b / (1e-6+b.norm(dim=1))[:, None]\n",
        "        score = (torch.mm(a_norm, b_norm.t()))\n",
        "\n",
        "        return score\n",
        "\n",
        "    def map_visual_text(self, visual_feats, labels, labels_embd):\n",
        "        mapped_labels_embd = []\n",
        "        labels == 1\n",
        "        for i in range(0, labels.shape[0]):\n",
        "            class_embd = labels_embd[labels[i]==1].mean(dim=0)[None,:]\n",
        "            mapped_labels_embd.append(class_embd)\n",
        "        mapped_labels_embd = torch.cat(mapped_labels_embd)\n",
        "\n",
        "        return visual_feats.detach(), mapped_labels_embd.detach()"
      ],
      "metadata": {
        "id": "nwlvZTX4iiks"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NIHChestXray"
      ],
      "metadata": {
        "id": "81EQt4_QjOWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Credit: https://github.com/nyuad-cai/CXR-ML-GZSL/blob/master/dataset.py\n",
        "\n",
        "class NIHChestXray(Dataset):\n",
        "    def __init__ (self, args, pathDatasetFile, transform, classes_to_load='seen', exclude_all=True):\n",
        "        self.listImagePaths = []\n",
        "        self.listImageLabels = []\n",
        "        self.transform = transform\n",
        "        self.num_classes = args.num_classes\n",
        "\n",
        "        self._data_path = args.data_root\n",
        "        self.args = args\n",
        "\n",
        "        self.split_path = pathDatasetFile\n",
        "        self.CLASSES = [ 'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia',\n",
        "                'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia']\n",
        "\n",
        "        self.unseen_classes = ['Edema', 'Pneumonia', 'Emphysema', 'Fibrosis']\n",
        "\n",
        "        self.seen_classes = [ 'Atelectasis', 'Effusion', 'Infiltration', 'Mass', 'Nodule',\n",
        "                'Pneumothorax', 'Consolidation', 'Cardiomegaly', 'Pleural_Thickening', 'Hernia']\n",
        "\n",
        "        self._class_ids = {v: i for i, v in enumerate(self.CLASSES) if v != 'No Finding'}\n",
        "\n",
        "        self.seen_class_ids = [self._class_ids[label] for label in self.seen_classes]\n",
        "        self.unseen_class_ids = [self._class_ids[label] for label in self.unseen_classes]\n",
        "\n",
        "        self.classes_to_load = classes_to_load\n",
        "        self.exclude_all = exclude_all\n",
        "        self._construct_index()\n",
        "\n",
        "    def _construct_index(self):\n",
        "        # Compile the split data path\n",
        "        max_labels = 0\n",
        "        paths = glob.glob(f'{self._data_path}/images/*.png')\n",
        "        self.names_to_path = {path.split('/')[-1]: path for path in paths}\n",
        "        data_entry_file = 'Data_Entry_2017_v2020.csv'\n",
        "\n",
        "        print(f'data partition path: {self.split_path}')\n",
        "        with open(self.split_path, 'r') as f: file_names = f.readlines()\n",
        "\n",
        "        split_file_names = np.array([file_name.strip().split(' ')[0].split('/')[-1] for file_name in file_names])\n",
        "        df = pd.read_csv(f'{self._data_path}/{data_entry_file}')\n",
        "        image_index = df.iloc[:, 0].values\n",
        "\n",
        "        _, split_index, _ = np.intersect1d(image_index, split_file_names, return_indices=True)\n",
        "\n",
        "        labels = df.iloc[:, 1].values\n",
        "        labels = np.array(labels)[split_index]\n",
        "\n",
        "        labels = [label.split('|') for label in labels]\n",
        "\n",
        "        image_index = image_index[split_index]\n",
        "\n",
        "        # Construct the image db\n",
        "        self._imdb = []\n",
        "        self.class_ids_loaded = []\n",
        "        for index in range(len(split_index)):\n",
        "            if len(labels[index]) == 1 and labels[index][0] == 'No Finding':\n",
        "                continue\n",
        "            if self._should_load_image(labels[index]) is False:\n",
        "                continue\n",
        "            class_ids = [self._class_ids[label] for label in labels[index]]\n",
        "            self.class_ids_loaded +=class_ids\n",
        "            self._imdb.append({\n",
        "                'im_path': self.names_to_path[image_index[index]],\n",
        "                'labels': class_ids,\n",
        "            })\n",
        "            max_labels = max(max_labels, len(class_ids))\n",
        "\n",
        "        self.class_ids_loaded = np.unique(np.array(self.class_ids_loaded))\n",
        "        print(f'Number of images: {len(self._imdb)}')\n",
        "        print(f'Number of max labels per image: {max_labels}')\n",
        "        print(f'Number of classes: {len(self.class_ids_loaded)}')\n",
        "\n",
        "    def _should_load_image(self, labels):\n",
        "        selected_class_labels = self.CLASSES\n",
        "        if self.classes_to_load == 'seen':\n",
        "            selected_class_labels = self.seen_classes\n",
        "        elif self.classes_to_load == 'unseen':\n",
        "            selected_class_labels = self.unseen_classes\n",
        "        elif self.classes_to_load == 'all':\n",
        "            return True\n",
        "\n",
        "        count = 0\n",
        "        for label in labels:\n",
        "            if label in selected_class_labels:\n",
        "                count+=1\n",
        "\n",
        "        if count == len(labels):\n",
        "            # all labels from selected sub set\n",
        "            return True\n",
        "        elif count == 0:\n",
        "            # none label in selected sub set\n",
        "            return False\n",
        "        else:\n",
        "            # some labels in selected sub set\n",
        "            if self.exclude_all is True:\n",
        "                return False\n",
        "            else:\n",
        "                return True\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        imagePath = self._imdb[index]['im_path']\n",
        "        imageData = Image.open(imagePath).convert('RGB')\n",
        "        labels = torch.tensor(self._imdb[index]['labels'])\n",
        "        labels = labels.unsqueeze(0)\n",
        "        imageLabel = torch.zeros(labels.size(0), self.num_classes).scatter_(1, labels, 1.).squeeze()\n",
        "        img = self.transform(imageData)\n",
        "\n",
        "        return img, imageLabel\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._imdb)"
      ],
      "metadata": {
        "id": "p4Nive-pjOel"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### plot_array"
      ],
      "metadata": {
        "id": "YdyCvcywoH06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Credit: https://github.com/nyuad-cai/CXR-ML-GZSL/blob/master/plots.py\n",
        "\n",
        "def plot_array(array, disc='loss'):\n",
        "    plt.plot(array)\n",
        "    plt.ylabel(disc)\n",
        "    plt.savefig(f'{disc}.pdf')\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "9XHV1q1xoH-9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ChexnetTrainer"
      ],
      "metadata": {
        "id": "5RvfqMkilbad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Credit: https://github.com/nyuad-cai/CXR-ML-GZSL/blob/master/ChexnetTrainer.py\n",
        "\n",
        "class ChexnetTrainer(object):\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.textual_embeddings = np.load(args.textual_embeddings)\n",
        "\n",
        "        self.model = ZSLNet(self.args, self.textual_embeddings, self.device).to(self.device)\n",
        "        self.optimizer = optim.Adam (self.model.parameters(), lr=self.args.lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n",
        "        self.scheduler = ReduceLROnPlateau(self.optimizer, factor=0.1, patience=5, mode='min')\n",
        "\n",
        "        self.loss = torch.nn.BCELoss(size_average=True)\n",
        "        self.auroc_min_loss = 0.0\n",
        "\n",
        "        self.start_epoch = 1\n",
        "        self.lossMIN = float('inf')\n",
        "        self.max_auroc_mean = float('-inf')\n",
        "        self.best_epoch = 1\n",
        "\n",
        "        self.val_losses = []\n",
        "\n",
        "        self.resume_from()\n",
        "        self.load_from()\n",
        "        self.init_dataset()\n",
        "\n",
        "        self.steps = [int(step) for step in self.args.steps.split(',')]\n",
        "        self.time_start = time.time()\n",
        "        self.time_end = time.time()\n",
        "        self.should_test = False\n",
        "        self.model.class_ids_loaded = self.train_dl.dataset.class_ids_loaded\n",
        "\n",
        "    def __call__(self):\n",
        "        self.train()\n",
        "\n",
        "    def load_from(self):\n",
        "        if self.args.load_from is not None:\n",
        "            checkpoint = torch.load(self.args.load_from, weights_only=False)\n",
        "            self.model.load_state_dict(checkpoint['state_dict'])\n",
        "            print(f'loaded checkpoint from {self.args.load_from}')\n",
        "\n",
        "    def resume_from(self):\n",
        "        if self.args.resume_from is not None:\n",
        "            checkpoint = torch.load(self.args.resume_from)\n",
        "            self.model.load_state_dict(checkpoint['state_dict'])\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            self.start_epoch = checkpoint['epoch'] + 1\n",
        "            self.lossMIN = checkpoint['lossMIN']\n",
        "            self.max_auroc_mean = checkpoint['max_auroc_mean']\n",
        "            print(f'resuming training from epoch {self.start_epoch}')\n",
        "\n",
        "    def save_checkpoint(self, prefix='best'):\n",
        "        path = f'{self.args.save_dir}/{prefix}_checkpoint.pth.tar'\n",
        "        torch.save(\n",
        "            {\n",
        "            'epoch': self.epoch,\n",
        "            'state_dict': self.model.state_dict(),\n",
        "            'max_auroc_mean': self.max_auroc_mean,\n",
        "            'optimizer' : self.optimizer.state_dict(),\n",
        "            'lossMIN' : self.lossMIN\n",
        "            }, path)\n",
        "        print(f\"saving {prefix} checkpoint\")\n",
        "\n",
        "    def init_dataset(self):\n",
        "        normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "\n",
        "        train_transforms = []\n",
        "        train_transforms.append(transforms.RandomResizedCrop(self.args.crop))\n",
        "        train_transforms.append(transforms.RandomHorizontalFlip())\n",
        "        train_transforms.append(transforms.ToTensor())\n",
        "        train_transforms.append(normalize)\n",
        "\n",
        "        datasetTrain = NIHChestXray(self.args, self.args.train_file, transform=transforms.Compose(train_transforms))\n",
        "\n",
        "        self.train_dl = DataLoader(dataset=datasetTrain, batch_size=self.args.batch_size, shuffle=True,  num_workers=4, pin_memory=True)\n",
        "\n",
        "        test_transforms = []\n",
        "        test_transforms.append(transforms.Resize(self.args.resize))\n",
        "        test_transforms.append(transforms.TenCrop(self.args.crop))\n",
        "        test_transforms.append(transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])))\n",
        "        test_transforms.append(transforms.Lambda(lambda crops: torch.stack([normalize(crop) for crop in crops])))\n",
        "\n",
        "        datasetVal = NIHChestXray(self.args, self.args.val_file, transform=transforms.Compose(test_transforms))\n",
        "        self.val_dl = DataLoader(dataset=datasetVal, batch_size=self.args.batch_size*10, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "        datasetTest = NIHChestXray(self.args, self.args.test_file, transform=transforms.Compose(test_transforms), classes_to_load='all')\n",
        "        self.test_dl = DataLoader(dataset=datasetTest, batch_size=self.args.batch_size*3, num_workers=8, shuffle=False, pin_memory=True)\n",
        "        print(datasetTest.CLASSES)\n",
        "\n",
        "    def train(self):\n",
        "        for self.epoch in range (self.start_epoch, self.args.epochs):\n",
        "            self.epochTrain()\n",
        "            lossVal, val_ind_auroc = self.epochVal()\n",
        "            val_ind_auroc = np.array(val_ind_auroc)\n",
        "\n",
        "            aurocMean = val_ind_auroc.mean()\n",
        "            self.save_checkpoint(prefix=f'last_epoch')\n",
        "            self.should_test = False\n",
        "\n",
        "            if aurocMean > self.max_auroc_mean:\n",
        "                self.max_auroc_mean = aurocMean\n",
        "                self.save_checkpoint(prefix='best_auroc')\n",
        "                self.best_epoch = self.epoch\n",
        "                self.should_test = True\n",
        "\n",
        "            if lossVal < self.lossMIN:\n",
        "                self.lossMIN = lossVal\n",
        "                self.auroc_min_loss = aurocMean\n",
        "                self.save_checkpoint(prefix='min_loss')\n",
        "                self.should_test = True\n",
        "\n",
        "            self.print_auroc(val_ind_auroc, self.val_dl.dataset.class_ids_loaded, prefix='val')\n",
        "            if self.should_test is True:\n",
        "                test_ind_auroc = self.test()\n",
        "                test_ind_auroc = np.array(test_ind_auroc)\n",
        "\n",
        "                self.write_results(val_ind_auroc, self.val_dl.dataset.class_ids_loaded, prefix=f'\\n\\nepoch {self.epoch}\\nval', mode='a')\n",
        "\n",
        "                self.write_results(test_ind_auroc[self.test_dl.dataset.seen_class_ids], self.test_dl.dataset.seen_class_ids, prefix='\\ntest_seen', mode='a')\n",
        "                self.write_results(test_ind_auroc[self.test_dl.dataset.unseen_class_ids], self.test_dl.dataset.unseen_class_ids, prefix='\\ntest_unseen', mode='a')\n",
        "\n",
        "                self.print_auroc(test_ind_auroc[self.test_dl.dataset.seen_class_ids], self.test_dl.dataset.seen_class_ids, prefix='\\ntest_seen')\n",
        "                self.print_auroc(test_ind_auroc[self.test_dl.dataset.unseen_class_ids], self.test_dl.dataset.unseen_class_ids, prefix='\\ntest_unseen')\n",
        "\n",
        "            plot_array(self.val_losses, f'{self.args.save_dir}/val_loss')\n",
        "            print(f'best epoch {self.best_epoch} best auroc {self.max_auroc_mean} loss {lossVal:.6f} auroc at min loss {self.auroc_min_loss:0.4f}')\n",
        "\n",
        "            self.scheduler.step(lossVal)\n",
        "\n",
        "    def get_eta(self, epoch, iter):\n",
        "        self.time_end = time.time()\n",
        "        delta = self.time_end - self.time_start\n",
        "        delta = delta * (len(self.train_dl) * (self.args.epochs - epoch) - iter)\n",
        "        sec = timedelta(seconds=int(delta))\n",
        "        d = (datetime(1,1,1) + sec)\n",
        "        eta = f\"{d.day-1} Days {d.hour}:{d.minute}:{d.second}\"\n",
        "        self.time_start = time.time()\n",
        "\n",
        "        return eta\n",
        "\n",
        "    def epochTrain(self):\n",
        "        self.model.train()\n",
        "        epoch_loss = 0\n",
        "        for batchID, (inputs, target) in enumerate (self.train_dl):\n",
        "\n",
        "            target = target.to(self.device)\n",
        "            inputs = inputs.to(self.device)\n",
        "            output, loss = self.model(inputs, target, self.epoch)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            eta = self.get_eta(self.epoch, batchID)\n",
        "            epoch_loss +=loss.item()\n",
        "            if batchID % 10 == 9:\n",
        "                print(f\" epoch [{self.epoch:04d} / {self.args.epochs:04d}] eta: {eta:<20} [{batchID:04}/{len(self.train_dl)}] lr: \\t{self.optimizer.param_groups[0]['lr']:0.4E} loss: \\t{epoch_loss/batchID:0.5f}\")\n",
        "\n",
        "    def epochVal(self):\n",
        "        self.model.eval()\n",
        "\n",
        "        lossVal = 0\n",
        "\n",
        "        outGT = torch.FloatTensor().to(self.device)\n",
        "        outPRED = torch.FloatTensor().to(self.device)\n",
        "        for i, (inputs, target) in enumerate (tqdm(self.val_dl)):\n",
        "            with torch.no_grad():\n",
        "                target = target.to(self.device)\n",
        "                inputs = inputs.to(self.device)\n",
        "                varTarget = torch.autograd.Variable(target)\n",
        "                bs, n_crops, c, h, w = inputs.size()\n",
        "\n",
        "                varInput = torch.autograd.Variable(inputs.view(-1, c, h, w).to(self.device))\n",
        "\n",
        "                varOutput, losstensor = self.model(varInput, varTarget, n_crops=n_crops, bs=bs)\n",
        "\n",
        "                outPRED = torch.cat((outPRED, varOutput), 0)\n",
        "                outGT = torch.cat((outGT, target), 0)\n",
        "\n",
        "                lossVal+=losstensor.item()\n",
        "                del varOutput, varTarget, varInput, target, inputs\n",
        "        lossVal = lossVal / len(self.val_dl)\n",
        "\n",
        "        aurocIndividual = self.computeAUROC(outGT, outPRED, self.val_dl.dataset.class_ids_loaded)\n",
        "        self.val_losses.append(lossVal)\n",
        "\n",
        "        return lossVal, aurocIndividual\n",
        "\n",
        "    def test(self):\n",
        "        cudnn.benchmark = True\n",
        "        outGT = torch.FloatTensor().cuda()\n",
        "        outPRED = torch.FloatTensor().cuda()\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        for i, (inputs, target) in enumerate(tqdm(self.test_dl)):\n",
        "            with torch.no_grad():\n",
        "                target = target.to(self.device)\n",
        "                outGT = torch.cat((outGT, target), 0)\n",
        "\n",
        "                bs, n_crops, c, h, w = inputs.size()\n",
        "\n",
        "                varInput = torch.autograd.Variable(inputs.view(-1, c, h, w).to(self.device))\n",
        "\n",
        "                out, _ = self.model(varInput, n_crops=n_crops, bs=bs)\n",
        "\n",
        "                outPRED = torch.cat((outPRED, out.data), 0)\n",
        "\n",
        "        aurocIndividual = self.computeAUROC(outGT, outPRED, self.test_dl.dataset.class_ids_loaded)\n",
        "\n",
        "        return aurocIndividual\n",
        "\n",
        "    def computeAUROC(self, dataGT, dataPRED, class_ids):\n",
        "        outAUROC = []\n",
        "        datanpGT = dataGT.cpu().numpy()\n",
        "        datanpPRED = dataPRED.cpu().numpy()\n",
        "\n",
        "        for i in class_ids:\n",
        "            outAUROC.append(roc_auc_score(datanpGT[:, i], datanpPRED[:, i]))\n",
        "        return outAUROC\n",
        "\n",
        "    def write_results(self, aurocIndividual, class_ids, prefix='val', mode='a'):\n",
        "        with open(f\"{self.args.save_dir}/results.txt\", mode) as results_file:\n",
        "            aurocMean = aurocIndividual.mean()\n",
        "\n",
        "            results_file.write(f'{prefix} AUROC mean {aurocMean:0.4f}\\n')\n",
        "            for i, class_id in enumerate(class_ids):\n",
        "                results_file.write(f'{self.val_dl.dataset.CLASSES[class_id]} {aurocIndividual[i]:0.4f}\\n')\n",
        "\n",
        "    def print_auroc(self, aurocIndividual, class_ids, prefix='val'):\n",
        "        aurocMean = aurocIndividual.mean()\n",
        "\n",
        "        print (f'{prefix} AUROC mean {aurocMean:0.4f}')\n",
        "\n",
        "        for i, class_id in enumerate(class_ids):\n",
        "            print (f'{self.val_dl.dataset.CLASSES[class_id]} {aurocIndividual[i]:0.4f}')"
      ],
      "metadata": {
        "id": "W5pimYcnlbgy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### argParser"
      ],
      "metadata": {
        "id": "8IbBrmujsoC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Credit: https://github.com/nyuad-cai/CXR-ML-GZSL/blob/master/arguments.py\n",
        "\n",
        "argParser = argparse.ArgumentParser(description='arguments')\n",
        "\n",
        "argParser.add_argument('--data-root', default=DATA_PATH, type=str, help='the path to dataset')\n",
        "argParser.add_argument('--save-dir', default=SAVE_PATH, type=str, help='the path to save the checkpoints')\n",
        "argParser.add_argument('--train-file', default=f'{SPLITS_PATH}/train.txt', type=str, help='the path to train list ')\n",
        "argParser.add_argument('--val-file', default=f'{SPLITS_PATH}/val.txt', type=str, help='the path to val list ')\n",
        "argParser.add_argument('--test-file', default=f'{SPLITS_PATH}/test.txt', type=str, help='the path to test list')\n",
        "\n",
        "argParser.add_argument('--pretrained', dest='pretrained', action='store_true',  help='load imagenet pretrained model')\n",
        "argParser.add_argument('--bce-only', dest='bce_only', help='train with only binary cross entropy loss', action='store_true')\n",
        "\n",
        "argParser.add_argument('--num-classes', default=14, type=int, help='number of classes')\n",
        "argParser.add_argument('--batch-size', default=16, type=int, help='training batch size')\n",
        "argParser.add_argument('--epochs', default=40, type=int, help='number of epochs to train')\n",
        "argParser.add_argument('--vision-backbone', default='densenet121', type=str, help='[densenet121, densenet169, densenet201]')\n",
        "argParser.add_argument('--resume-from', default=None, type=str, help='path to checkpoint to resume the training from')\n",
        "argParser.add_argument('--load-from', default=None, type=str, help='path to checkpoint to load the weights from')\n",
        "\n",
        "argParser.add_argument('--resize', default=256, type=int, help='number of epochs to train')\n",
        "argParser.add_argument('--crop', default=224, type=int, help='number of epochs to train')\n",
        "argParser.add_argument('--lr', default=0.0001, type=float, help='learning rate')\n",
        "argParser.add_argument('--steps', default='20, 40, 60, 80', type=str, help='learning rate decay steps comma separated')\n",
        "\n",
        "argParser.add_argument('--beta-map', default=0.1, type=float, help='learning rate')\n",
        "argParser.add_argument('--beta-con', default=0.1, type=float, help='learning rate')\n",
        "argParser.add_argument('--beta-rank', default=1, type=float, help='learning rate')\n",
        "argParser.add_argument('--neg-penalty', default=0.03, type=float, help='learning rate')\n",
        "\n",
        "argParser.add_argument('--wo-con', dest='wo_con', help='train with out semantic consistency regularizer loss', action='store_true')\n",
        "argParser.add_argument('--wo-map', dest='wo_map', help='train with out alignement loss', action='store_true')\n",
        "\n",
        "argParser.add_argument('--textual-embeddings', default=BIOBERT_PATH, type=str, help='the path to labels embeddings')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHONAIP5soMa",
        "outputId": "49e036e5-40e8-4c62-db56-0990154a17e2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--textual-embeddings'], dest='textual_embeddings', nargs=None, const=None, default='CXR-ML-GZSL/embeddings/nih_chest_xray_biobert.npy', type=<class 'str'>, choices=None, required=False, help='the path to labels embeddings', metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Testing\n",
        "\n"
      ],
      "metadata": {
        "id": "XrdwFdHCo4qd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test with Pretrained Weights"
      ],
      "metadata": {
        "id": "gYsubhrzt-Sq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Credit: https://github.com/nyuad-cai/CXR-ML-GZSL/blob/master/scripts/test_densenet121.sh\n",
        "# Credit: https://github.com/nyuad-cai/CXR-ML-GZSL/blob/master/test.py\n",
        "\n",
        "args = argParser.parse_args(['--load-from', WEIGHTS_PATH])\n",
        "\n",
        "trainer = ChexnetTrainer(args)\n",
        "\n",
        "test_ind_auroc = trainer.test()\n",
        "test_ind_auroc = np.array(test_ind_auroc)\n",
        "\n",
        "trainer.print_auroc(test_ind_auroc[trainer.test_dl.dataset.seen_class_ids], trainer.test_dl.dataset.seen_class_ids, prefix='\\ntest_seen')\n",
        "trainer.print_auroc(test_ind_auroc[trainer.test_dl.dataset.unseen_class_ids], trainer.test_dl.dataset.unseen_class_ids, prefix='\\ntest_unseen')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWw5DgZco40u",
        "outputId": "7cf5d2a7-2e3e-4e84-e11b-1a3cdec37364"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded checkpoint from CXR-ML-GZSL/checkpoints/best_auroc_checkpoint.pth.tar\n",
            "data partition path: CXR-ML-GZSL/dataset_splits/train.txt\n",
            "Number of images: 30758\n",
            "Number of max labels per image: 7\n",
            "Number of classes: 10\n",
            "data partition path: CXR-ML-GZSL/dataset_splits/val.txt\n",
            "Number of images: 4474\n",
            "Number of max labels per image: 6\n",
            "Number of classes: 10\n",
            "data partition path: CXR-ML-GZSL/dataset_splits/test.txt\n",
            "Number of images: 10510\n",
            "Number of max labels per image: 7\n",
            "Number of classes: 14\n",
            "['Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia', 'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 219/219 [03:56<00:00,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "test_seen AUROC mean 0.7891\n",
            "Atelectasis 0.7637\n",
            "Effusion 0.8275\n",
            "Infiltration 0.7021\n",
            "Mass 0.8041\n",
            "Nodule 0.7542\n",
            "Pneumothorax 0.8278\n",
            "Consolidation 0.6935\n",
            "Cardiomegaly 0.9036\n",
            "Pleural_Thickening 0.7174\n",
            "Hernia 0.8976\n",
            "\n",
            "test_unseen AUROC mean 0.6574\n",
            "Edema 0.6730\n",
            "Pneumonia 0.6184\n",
            "Emphysema 0.7390\n",
            "Fibrosis 0.5992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}