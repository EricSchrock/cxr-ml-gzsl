{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CXR-ML-GZSL Extension"
      ],
      "metadata": {
        "id": "8GpPIrWTmtva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "This notebook tests the model described by the paper, [\"Multi-Label Generalized Zero Shot Learning for the Classification of Disease in Chest Radiographs\"](https://arxiv.org/abs/2107.06563), but with a different visual encoder (EfficientNet-B0). The goal is to measure the impact on AUROC and training time. Due to the project deadline, I cut the dataset and the number of epochs in half.\n",
        "\n",
        "**Note**: The dataset is ~42 GB. Expect significant download times.\n",
        "\n",
        "**Note**: The notebook will request access to Google Drive to save training checkpoints. This is to avoid loosing training progress if Google Colab times out."
      ],
      "metadata": {
        "id": "w5eH6S0Cm0Zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "from datetime import datetime, timedelta\n",
        "import glob\n",
        "import multiprocessing\n",
        "import os\n",
        "import requests\n",
        "from statistics import mean\n",
        "import tarfile\n",
        "import time\n",
        "import urllib.request\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.plotting import table\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "joTxgkRZm0l-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DRIVE_PATH = '/content/drive'\n",
        "\n",
        "if not os.path.ismount(DRIVE_PATH):\n",
        "    drive.mount(DRIVE_PATH)\n",
        "\n",
        "DRIVE_PATH = f'{DRIVE_PATH}/MyDrive'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDFLItJqpQ3d",
        "outputId": "d6ba1529-070c-4d31-ab88-3ac96b86bb7d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 1002\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)"
      ],
      "metadata": {
        "id": "-38ZtQXdoMlG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment"
      ],
      "metadata": {
        "id": "-_skedoanth0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!if command -v nvidia-smi &> /dev/null; then nvidia-smi --query-gpu=name --format=csv,noheader; else echo 'No NVIDIA GPU detected'; fi\n",
        "!echo\n",
        "!python --version\n",
        "!echo\n",
        "!pip list | grep -E \"matplotlib|numpy|pandas|pillow|scikit-learn|tqdm|torch|torchvision\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdIZXA5YnqOR",
        "outputId": "56eba9ec-2230-423e-e9ce-b29291f66583"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA A100-SXM4-40GB\n",
            "\n",
            "Python 3.11.12\n",
            "\n",
            "geopandas                             1.0.1\n",
            "matplotlib                            3.10.0\n",
            "matplotlib-inline                     0.1.7\n",
            "matplotlib-venn                       1.1.2\n",
            "numpy                                 2.0.2\n",
            "pandas                                2.2.2\n",
            "pandas-datareader                     0.10.0\n",
            "pandas-gbq                            0.28.0\n",
            "pandas-stubs                          2.2.2.240909\n",
            "pillow                                11.1.0\n",
            "scikit-learn                          1.6.1\n",
            "sklearn-pandas                        2.2.0\n",
            "torch                                 2.6.0+cu124\n",
            "torchaudio                            2.6.0+cu124\n",
            "torchsummary                          1.5.1\n",
            "torchvision                           0.21.0+cu124\n",
            "tqdm                                  4.67.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download"
      ],
      "metadata": {
        "id": "9gHR5WMrn_IH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for filename in [\"train.txt\", \"val.txt\", \"test.txt\"]:\n",
        "    response = requests.get(f\"https://raw.githubusercontent.com/nyuad-cai/CXR-ML-GZSL/master/dataset_splits/{filename}\")\n",
        "\n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(response.text)\n",
        "\n",
        "    print(f\"Downloaded: {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuscYh5An_Og",
        "outputId": "0b5894a8-171d-4fec-a978-62f4979f42cd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: train.txt\n",
            "Downloaded: val.txt\n",
            "Downloaded: test.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"Data_Entry_2017_v2020.csv\"\n",
        "response = requests.get('https://drive.google.com/uc?export=download&id=1mkOZNfYt-Px52b8CJZJANNbM3ULUVO3f')\n",
        "\n",
        "with open(filename, \"wb\") as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "print(f\"Downloaded: {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1VV543RoUWs",
        "outputId": "258e46a0-5e66-495f-e9a3-967a3471b40c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: Data_Entry_2017_v2020.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CLASS_EMBEDDINGS = \"nih_chest_xray_biobert.npy\"\n",
        "\n",
        "response = requests.get(f\"https://raw.githubusercontent.com/nyuad-cai/CXR-ML-GZSL/master/embeddings/{CLASS_EMBEDDINGS}\")\n",
        "\n",
        "with open(CLASS_EMBEDDINGS, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "print(f\"Downloaded: {CLASS_EMBEDDINGS}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSXMRXMzoYiC",
        "outputId": "8d7dc6ad-d100-4d89-c9f9-8690de6962c4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: nih_chest_xray_biobert.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    {\"filename\": \"images_001.tar.gz\", \"url\": \"https://nihcc.box.com/shared/static/vfk49d74nhbxq3nqjg0900w5nvkorp5c.gz\"},\n",
        "    {\"filename\": \"images_002.tar.gz\", \"url\": \"https://nihcc.box.com/shared/static/i28rlmbvmfjbl8p2n3ril0pptcmcu9d1.gz\"},\n",
        "    {\"filename\": \"images_003.tar.gz\", \"url\": \"https://nihcc.box.com/shared/static/f1t00wrtdk94satdfb9olcolqx20z2jp.gz\"},\n",
        "    {\"filename\": \"images_004.tar.gz\", \"url\": \"https://nihcc.box.com/shared/static/0aowwzs5lhjrceb3qp67ahp0rd1l1etg.gz\"},\n",
        "    {\"filename\": \"images_005.tar.gz\", \"url\": \"https://nihcc.box.com/shared/static/v5e3goj22zr6h8tzualxfsqlqaygfbsn.gz\"},\n",
        "    {\"filename\": \"images_006.tar.gz\", \"url\": \"https://nihcc.box.com/shared/static/asi7ikud9jwnkrnkj99jnpfkjdes7l6l.gz\"},\n",
        "]\n",
        "\n",
        "for item in dataset:\n",
        "    filename = item[\"filename\"]\n",
        "    url = item[\"url\"]\n",
        "\n",
        "    urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "    with tarfile.open(filename, \"r:gz\") as tar:\n",
        "        tar.extractall()\n",
        "\n",
        "    os.remove(filename)\n",
        "\n",
        "    print(f\"Downloaded and extracted: {filename}\")\n",
        "\n",
        "IMAGE_PATH = \"images\"\n",
        "\n",
        "assert os.path.exists(IMAGE_PATH), \"Dataset is not in the expected directory!\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e2y32M6oZTT",
        "outputId": "557bd45c-25c7-4d9a-a64a-a52ed54aba09"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded and extracted: images_001.tar.gz\n",
            "Downloaded and extracted: images_002.tar.gz\n",
            "Downloaded and extracted: images_003.tar.gz\n",
            "Downloaded and extracted: images_004.tar.gz\n",
            "Downloaded and extracted: images_005.tar.gz\n",
            "Downloaded and extracted: images_006.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "2at94CtIoca_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NIHChestXray(Dataset):\n",
        "    def __init__ (self, args, pathDatasetFile, transform, classes_to_load='seen', exclude_all=True):\n",
        "        self.listImagePaths = []\n",
        "        self.listImageLabels = []\n",
        "        self.transform = transform\n",
        "        self.num_classes = args.num_classes\n",
        "\n",
        "        self._data_path = args.data_root\n",
        "        self.args = args\n",
        "\n",
        "        self.split_path = pathDatasetFile\n",
        "        self.CLASSES = [ 'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia',\n",
        "                'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia']\n",
        "\n",
        "        self.unseen_classes = ['Edema', 'Pneumonia', 'Emphysema', 'Fibrosis']\n",
        "\n",
        "        self.seen_classes = [ 'Atelectasis', 'Effusion', 'Infiltration', 'Mass', 'Nodule',\n",
        "                'Pneumothorax', 'Consolidation', 'Cardiomegaly', 'Pleural_Thickening', 'Hernia']\n",
        "\n",
        "        self._class_ids = {v: i for i, v in enumerate(self.CLASSES) if v != 'No Finding'}\n",
        "\n",
        "        self.seen_class_ids = [self._class_ids[label] for label in self.seen_classes]\n",
        "        self.unseen_class_ids = [self._class_ids[label] for label in self.unseen_classes]\n",
        "\n",
        "        self.classes_to_load = classes_to_load\n",
        "        self.exclude_all = exclude_all\n",
        "        self._construct_index()\n",
        "\n",
        "    def _construct_index(self):\n",
        "        # Compile the split data path\n",
        "        max_labels = 0\n",
        "        paths = glob.glob('images/*.png' if self._data_path == '' else f'{self._data_path}/images/*.png')\n",
        "        self.names_to_path = {path.split('/')[-1]: path for path in paths}\n",
        "        data_entry_file = 'Data_Entry_2017_v2020.csv'\n",
        "\n",
        "        print(f'data partition path: {self.split_path}')\n",
        "        with open(self.split_path, 'r') as f: file_names = f.readlines()\n",
        "\n",
        "        split_file_names = np.array([file_name.strip().split(' ')[0].split('/')[-1] for file_name in file_names])\n",
        "        df = pd.read_csv(f'{data_entry_file}' if self._data_path == '' else f'{self._data_path}/{data_entry_file}')\n",
        "        image_index = df.iloc[:, 0].values\n",
        "\n",
        "        _, split_index, _ = np.intersect1d(image_index, split_file_names, return_indices=True)\n",
        "\n",
        "        labels = df.iloc[:, 1].values\n",
        "        labels = np.array(labels)[split_index]\n",
        "\n",
        "        labels = [label.split('|') for label in labels]\n",
        "\n",
        "        image_index = image_index[split_index]\n",
        "\n",
        "        # Construct the image db\n",
        "        self._imdb = []\n",
        "        self.class_ids_loaded = []\n",
        "        for index in range(len(split_index)):\n",
        "            if len(labels[index]) == 1 and labels[index][0] == 'No Finding':\n",
        "                continue\n",
        "            if self._should_load_image(labels[index]) is False:\n",
        "                continue\n",
        "            if image_index[index] not in self.names_to_path.keys():\n",
        "                continue\n",
        "            class_ids = [self._class_ids[label] for label in labels[index]]\n",
        "            self.class_ids_loaded +=class_ids\n",
        "            self._imdb.append({\n",
        "                'im_path': self.names_to_path[image_index[index]],\n",
        "                'labels': class_ids,\n",
        "            })\n",
        "            max_labels = max(max_labels, len(class_ids))\n",
        "\n",
        "        self.class_ids_loaded = np.unique(np.array(self.class_ids_loaded))\n",
        "        print(f'Number of images: {len(self._imdb)}')\n",
        "        print(f'Number of max labels per image: {max_labels}')\n",
        "        print(f'Number of classes: {len(self.class_ids_loaded)}')\n",
        "\n",
        "    def _should_load_image(self, labels):\n",
        "        selected_class_labels = self.CLASSES\n",
        "        if self.classes_to_load == 'seen':\n",
        "            selected_class_labels = self.seen_classes\n",
        "        elif self.classes_to_load == 'unseen':\n",
        "            selected_class_labels = self.unseen_classes\n",
        "        elif self.classes_to_load == 'all':\n",
        "            return True\n",
        "\n",
        "        count = 0\n",
        "        for label in labels:\n",
        "            if label in selected_class_labels:\n",
        "                count+=1\n",
        "\n",
        "        if count == len(labels):\n",
        "            # all labels from selected sub set\n",
        "            return True\n",
        "        elif count == 0:\n",
        "            # none label in selected sub set\n",
        "            return False\n",
        "        else:\n",
        "            # some labels in selected sub set\n",
        "            if self.exclude_all is True:\n",
        "                return False\n",
        "            else:\n",
        "                return True\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        imagePath = self._imdb[index]['im_path']\n",
        "        imageData = Image.open(imagePath).convert('RGB')\n",
        "        labels = torch.tensor(self._imdb[index]['labels'])\n",
        "        labels = labels.unsqueeze(0)\n",
        "        imageLabel = torch.zeros(labels.size(0), self.num_classes).scatter_(1, labels, 1.).squeeze()\n",
        "        img = self.transform(imageData)\n",
        "\n",
        "        return img, imageLabel\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._imdb)"
      ],
      "metadata": {
        "id": "8_FNZd4oodnr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "FjkdiOOXoiHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RankingLoss(nn.Module):\n",
        "    def __init__(self, neg_penalty=0.03):\n",
        "        super(RankingLoss, self).__init__()\n",
        "\n",
        "        self.neg_penalty = neg_penalty\n",
        "\n",
        "    def forward(self, ranks, labels, class_ids_loaded, device):\n",
        "        '''\n",
        "        for each correct it should be higher then the absence\n",
        "        '''\n",
        "        labels = labels[:, class_ids_loaded]\n",
        "        ranks_loaded = ranks[:, class_ids_loaded]\n",
        "        neg_labels = 1+(labels*-1)\n",
        "        loss_rank = torch.zeros(1).to(device)\n",
        "        for i in range(len(labels)):\n",
        "            correct = ranks_loaded[i, labels[i]==1]\n",
        "            wrong = ranks_loaded[i, neg_labels[i]==1]\n",
        "            correct = correct.reshape((-1, 1)).repeat((1, len(wrong)))\n",
        "            wrong = wrong.repeat(len(correct)).reshape(len(correct), -1)\n",
        "            image_level_penalty = ((self.neg_penalty+wrong) - correct)\n",
        "            image_level_penalty[image_level_penalty<0]=0\n",
        "            loss_rank += image_level_penalty.sum()\n",
        "        loss_rank /=len(labels)\n",
        "\n",
        "        return loss_rank\n",
        "\n",
        "class CosineLoss(nn.Module):\n",
        "    def forward(self, t_emb, v_emb ):\n",
        "        a_norm = v_emb / v_emb.norm(dim=1)[:, None]\n",
        "        b_norm = t_emb / t_emb.norm(dim=1)[:, None]\n",
        "        loss = 1 - torch.mean(torch.diagonal(torch.mm(a_norm, b_norm.t()), 0))\n",
        "\n",
        "        return loss\n",
        "\n",
        "class ZSLNet(nn.Module):\n",
        "    def __init__(self, args, textual_embeddings=None, device='cpu'):\n",
        "        super(ZSLNet, self).__init__()\n",
        "        self.args = args\n",
        "        self.device = device\n",
        "        self.vision_backbone = getattr(torchvision.models, self.args.vision_backbone)(pretrained=self.args.pretrained)\n",
        "        # remove classification layer from visual encoder\n",
        "        classifiers = [ 'classifier', 'fc']\n",
        "        for classifier in classifiers:\n",
        "            cls_layer = getattr(self.vision_backbone, classifier, None)\n",
        "            if cls_layer is None:\n",
        "                continue\n",
        "            if isinstance(cls_layer, nn.Sequential):\n",
        "                last_layer = cls_layer[-1]\n",
        "                d_visual = last_layer.in_features\n",
        "            else:\n",
        "                d_visual = cls_layer.in_features\n",
        "            setattr(self.vision_backbone, classifier, nn.Identity(d_visual))\n",
        "            break\n",
        "\n",
        "        pretrained_encoder = False\n",
        "        if pretrained_encoder:\n",
        "            self.vision_backbone.classifier = nn.Identity(d_visual)\n",
        "\n",
        "            path = 'checkpoints/bce_only_imagenet/last_epoch_checkpoint.pth.tar'\n",
        "\n",
        "            self.classifier = nn.Sequential(nn.Linear(d_visual, self.args.num_classes), nn.Sigmoid())\n",
        "            checkpoint = torch.load(path, weights_only=False)\n",
        "            self.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "            for p in self.vision_backbone.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        if self.args.bce_only:\n",
        "            self.bce_loss = torch.nn.BCELoss(size_average=True)\n",
        "            self.classifier = nn.Sequential(nn.Linear(d_visual, self.args.num_classes), nn.Sigmoid())\n",
        "        else:\n",
        "            self.emb_loss = CosineLoss()\n",
        "            self.ranking_loss = RankingLoss(neg_penalty=self.args.neg_penalty)\n",
        "            self.textual_embeddings = textual_embeddings\n",
        "            d_textual = self.textual_embeddings.shape[-1]\n",
        "\n",
        "            self.textual_embeddings = torch.from_numpy(self.textual_embeddings).to(self.device)\n",
        "\n",
        "            self.fc_v = nn.Sequential(\n",
        "                nn.Linear(d_visual, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(512, 256),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(256, 128),\n",
        "            )\n",
        "\n",
        "            self.fc_t = nn.Sequential(\n",
        "                nn.Linear(d_textual, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(512, 256),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(256, 128)\n",
        "            )\n",
        "\n",
        "    def forward(self, x, labels=None, epoch=0, n_crops=0, bs=16):\n",
        "        if self.args.bce_only:\n",
        "            return self.forward_bce_only(x, labels=labels, n_crops=n_crops, bs=bs)\n",
        "        else:\n",
        "            return self.forward_ranking(x, labels=labels, epoch=epoch, n_crops=n_crops, bs=bs)\n",
        "\n",
        "    def forward_bce_only(self, x, labels=None, n_crops=0, bs=16):\n",
        "        lossvalue_bce = torch.zeros(1).to(self.device)\n",
        "\n",
        "        visual_feats = self.vision_backbone(x)\n",
        "        preds = self.classifier(visual_feats)\n",
        "\n",
        "        if labels is not None:\n",
        "            lossvalue_bce = self.bce_loss(preds, labels)\n",
        "\n",
        "        return preds, lossvalue_bce, f'bce:\\t {lossvalue_bce.item():0.4f}'\n",
        "\n",
        "    def forward_ranking(self, x, labels=None, epoch=0, n_crops=0, bs=16):\n",
        "        loss_rank = torch.zeros(1).to(self.device)\n",
        "        loss_allignment_cos = torch.zeros(1).to(self.device)\n",
        "        loss_mapping_consistency = torch.zeros(1).to(self.device)\n",
        "\n",
        "        visual_feats = self.vision_backbone(x)\n",
        "        visual_feats = self.fc_v(visual_feats)\n",
        "        text_feats = self.fc_t(self.textual_embeddings)\n",
        "\n",
        "        if not self.args.wo_con and epoch >= 0:\n",
        "            text_mapped_sim = self.sim_score(text_feats, text_feats.detach())\n",
        "            text_orig_sim = self.sim_score(self.textual_embeddings, self.textual_embeddings)\n",
        "            loss_mapping_consistency = torch.abs(text_orig_sim - text_mapped_sim).mean()\n",
        "\n",
        "        if labels is not None:\n",
        "            mapped_visual, mapped_text = self.map_visual_text(visual_feats, labels, text_feats)\n",
        "            if mapped_visual is not None and not self.args.wo_map and epoch >= 0:\n",
        "                loss_allignment_cos = self.emb_loss(mapped_text, mapped_visual)\n",
        "\n",
        "        ranks = self.sim_score(visual_feats, text_feats)\n",
        "        if n_crops > 0:\n",
        "            ranks = ranks.view(bs, n_crops, -1).mean(1)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_rank = self.ranking_loss(ranks, labels, self.class_ids_loaded, self.device)\n",
        "        loss_allignment_cos = (self.args.beta_map * loss_allignment_cos)\n",
        "        loss_rank = (self.args.beta_rank * loss_rank)\n",
        "        loss_mapping_consistency = (self.args.beta_con * loss_mapping_consistency)\n",
        "        losses = loss_rank + loss_mapping_consistency + loss_allignment_cos\n",
        "\n",
        "        return ranks, losses\n",
        "\n",
        "    def sim_score(self, a, b):\n",
        "        a_norm = a / a.norm(dim=1)[:, None]\n",
        "        b_norm = b / (1e-6+b.norm(dim=1))[:, None]\n",
        "        score = (torch.mm(a_norm, b_norm.t()))\n",
        "\n",
        "        return score\n",
        "\n",
        "    def map_visual_text(self, visual_feats, labels, labels_embd):\n",
        "        mapped_labels_embd = []\n",
        "        labels == 1\n",
        "        for i in range(0, labels.shape[0]):\n",
        "            class_embd = labels_embd[labels[i]==1].mean(dim=0)[None,:]\n",
        "            mapped_labels_embd.append(class_embd)\n",
        "        mapped_labels_embd = torch.cat(mapped_labels_embd)\n",
        "\n",
        "        return visual_feats.detach(), mapped_labels_embd.detach()"
      ],
      "metadata": {
        "id": "8NZ_2A3loifE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "La5HaDGdopV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_array(array, disc='loss'):\n",
        "    plt.plot(array)\n",
        "    plt.ylabel(disc)\n",
        "    plt.savefig(f'{disc}.pdf')\n",
        "    plt.close()\n",
        "\n",
        "class ChexnetTrainer(object):\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.textual_embeddings = np.load(args.textual_embeddings)\n",
        "\n",
        "        self.model = ZSLNet(self.args, self.textual_embeddings, self.device).to(self.device)\n",
        "        self.optimizer = optim.Adam (self.model.parameters(), lr=self.args.lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n",
        "        self.scheduler = ReduceLROnPlateau(self.optimizer, factor=0.01, patience=10, mode='min')\n",
        "\n",
        "        self.loss = torch.nn.BCELoss(size_average=True)\n",
        "        self.auroc_min_loss = 0.0\n",
        "\n",
        "        self.start_epoch = 1\n",
        "        self.lossMIN = float('inf')\n",
        "        self.max_auroc_mean = float('-inf')\n",
        "        self.best_epoch = 1\n",
        "\n",
        "        self.val_losses = []\n",
        "\n",
        "        self.resume_from()\n",
        "        self.load_from()\n",
        "        self.init_dataset()\n",
        "\n",
        "        self.steps = [int(step) for step in self.args.steps.split(',')]\n",
        "        self.time_start = time.time()\n",
        "        self.time_end = time.time()\n",
        "        self.should_test = False\n",
        "        self.model.class_ids_loaded = self.train_dl.dataset.class_ids_loaded\n",
        "\n",
        "    def __call__(self):\n",
        "        self.train()\n",
        "\n",
        "    def load_from(self):\n",
        "        if self.args.load_from is not None:\n",
        "            checkpoint = torch.load(self.args.load_from, weights_only=False)\n",
        "            self.model.load_state_dict(checkpoint['state_dict'])\n",
        "            print(f'loaded checkpoint from {self.args.load_from}')\n",
        "\n",
        "    def resume_from(self):\n",
        "        if self.args.resume_from is not None:\n",
        "            checkpoint = torch.load(self.args.resume_from, weights_only=False)\n",
        "            self.model.load_state_dict(checkpoint['state_dict'])\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            self.start_epoch = checkpoint['epoch'] + 1\n",
        "            self.lossMIN = checkpoint['lossMIN']\n",
        "            self.max_auroc_mean = checkpoint['max_auroc_mean']\n",
        "            print(f'resuming training from epoch {self.start_epoch}')\n",
        "\n",
        "    def save_checkpoint(self, prefix='best'):\n",
        "        path = f'{self.args.save_dir}/{prefix}_checkpoint.pth.tar'\n",
        "        torch.save(\n",
        "            {\n",
        "            'epoch': self.epoch,\n",
        "            'state_dict': self.model.state_dict(),\n",
        "            'max_auroc_mean': self.max_auroc_mean,\n",
        "            'optimizer' : self.optimizer.state_dict(),\n",
        "            'lossMIN' : self.lossMIN\n",
        "            }, path)\n",
        "        print(f\"saving {prefix} checkpoint\")\n",
        "\n",
        "    def init_dataset(self):\n",
        "        normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "\n",
        "        train_transforms = []\n",
        "        train_transforms.append(transforms.RandomResizedCrop(self.args.crop))\n",
        "        train_transforms.append(transforms.RandomHorizontalFlip())\n",
        "        train_transforms.append(transforms.ToTensor())\n",
        "        train_transforms.append(normalize)\n",
        "\n",
        "        datasetTrain = NIHChestXray(self.args, self.args.train_file, transform=transforms.Compose(train_transforms))\n",
        "\n",
        "        self.train_dl = DataLoader(dataset=datasetTrain, batch_size=self.args.batch_size, shuffle=True,  num_workers=4, pin_memory=True)\n",
        "\n",
        "        test_transforms = []\n",
        "        test_transforms.append(transforms.Resize(self.args.resize))\n",
        "        test_transforms.append(transforms.TenCrop(self.args.crop))\n",
        "        test_transforms.append(transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])))\n",
        "        test_transforms.append(transforms.Lambda(lambda crops: torch.stack([normalize(crop) for crop in crops])))\n",
        "\n",
        "        datasetVal = NIHChestXray(self.args, self.args.val_file, transform=transforms.Compose(test_transforms))\n",
        "        self.val_dl = DataLoader(dataset=datasetVal, batch_size=self.args.batch_size*10, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "        datasetTest = NIHChestXray(self.args, self.args.test_file, transform=transforms.Compose(test_transforms), classes_to_load='all')\n",
        "        self.test_dl = DataLoader(dataset=datasetTest, batch_size=self.args.batch_size*3, num_workers=8, shuffle=False, pin_memory=True)\n",
        "        print(datasetTest.CLASSES)\n",
        "\n",
        "    def train(self):\n",
        "        for self.epoch in range(self.start_epoch, self.args.epochs + 1):\n",
        "            self.epochTrain()\n",
        "            lossVal, val_ind_auroc = self.epochVal()\n",
        "            val_ind_auroc = np.array(val_ind_auroc)\n",
        "\n",
        "            aurocMean = val_ind_auroc.mean()\n",
        "            self.save_checkpoint(prefix=f'last_epoch')\n",
        "            self.should_test = False\n",
        "\n",
        "            if aurocMean > self.max_auroc_mean:\n",
        "                self.max_auroc_mean = aurocMean\n",
        "                self.save_checkpoint(prefix='best_auroc')\n",
        "                self.best_epoch = self.epoch\n",
        "                self.should_test = True\n",
        "\n",
        "            if lossVal < self.lossMIN:\n",
        "                self.lossMIN = lossVal\n",
        "                self.auroc_min_loss = aurocMean\n",
        "                self.save_checkpoint(prefix='min_loss')\n",
        "                self.should_test = True\n",
        "\n",
        "            self.print_auroc(val_ind_auroc, self.val_dl.dataset.class_ids_loaded, prefix='val')\n",
        "            if self.should_test is True:\n",
        "                test_ind_auroc = self.test()\n",
        "                test_ind_auroc = np.array(test_ind_auroc)\n",
        "\n",
        "                self.write_results(val_ind_auroc, self.val_dl.dataset.class_ids_loaded, prefix=f'\\n\\nepoch {self.epoch}\\nval', mode='a')\n",
        "\n",
        "                self.write_results(test_ind_auroc[self.test_dl.dataset.seen_class_ids], self.test_dl.dataset.seen_class_ids, prefix='\\ntest_seen', mode='a')\n",
        "                self.write_results(test_ind_auroc[self.test_dl.dataset.unseen_class_ids], self.test_dl.dataset.unseen_class_ids, prefix='\\ntest_unseen', mode='a')\n",
        "\n",
        "                self.print_auroc(test_ind_auroc[self.test_dl.dataset.seen_class_ids], self.test_dl.dataset.seen_class_ids, prefix='\\ntest_seen')\n",
        "                self.print_auroc(test_ind_auroc[self.test_dl.dataset.unseen_class_ids], self.test_dl.dataset.unseen_class_ids, prefix='\\ntest_unseen')\n",
        "\n",
        "            plot_array(self.val_losses, f'{self.args.save_dir}/val_loss')\n",
        "            print(f'best epoch {self.best_epoch} best auroc {self.max_auroc_mean} loss {lossVal:.6f} auroc at min loss {self.auroc_min_loss:0.4f}')\n",
        "\n",
        "            self.scheduler.step(lossVal)\n",
        "\n",
        "    def get_eta(self, epoch, iter):\n",
        "        self.time_end = time.time()\n",
        "        delta = self.time_end - self.time_start\n",
        "        delta = delta * (len(self.train_dl) * ((self.args.epochs + 1) - epoch) - iter)\n",
        "        sec = timedelta(seconds=int(delta))\n",
        "        d = (datetime(1,1,1) + sec)\n",
        "        eta = f\"{d.day-1} Days {d.hour}:{d.minute}:{d.second}\"\n",
        "        self.time_start = time.time()\n",
        "\n",
        "        return eta\n",
        "\n",
        "    def epochTrain(self):\n",
        "        self.model.train()\n",
        "        epoch_loss = 0\n",
        "        for batchID, (inputs, target) in enumerate (self.train_dl):\n",
        "\n",
        "            target = target.to(self.device)\n",
        "            inputs = inputs.to(self.device)\n",
        "            output, loss = self.model(inputs, target, self.epoch)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            eta = self.get_eta(self.epoch, batchID)\n",
        "            epoch_loss +=loss.item()\n",
        "            if batchID % 100 == 99:\n",
        "                print(f\" epoch [{self.epoch:04d} / {self.args.epochs:04d}] eta: {eta:<20} [{batchID:04}/{len(self.train_dl)}] lr: \\t{self.optimizer.param_groups[0]['lr']:0.4E} loss: \\t{epoch_loss/batchID:0.5f}\")\n",
        "\n",
        "    def epochVal(self):\n",
        "        self.model.eval()\n",
        "\n",
        "        lossVal = 0\n",
        "\n",
        "        outGT = torch.FloatTensor().to(self.device)\n",
        "        outPRED = torch.FloatTensor().to(self.device)\n",
        "        for i, (inputs, target) in enumerate (tqdm(self.val_dl)):\n",
        "            with torch.no_grad():\n",
        "                target = target.to(self.device)\n",
        "                inputs = inputs.to(self.device)\n",
        "                varTarget = torch.autograd.Variable(target)\n",
        "                bs, n_crops, c, h, w = inputs.size()\n",
        "\n",
        "                varInput = torch.autograd.Variable(inputs.view(-1, c, h, w).to(self.device))\n",
        "\n",
        "                varOutput, losstensor = self.model(varInput, varTarget, n_crops=n_crops, bs=bs)\n",
        "\n",
        "                outPRED = torch.cat((outPRED, varOutput), 0)\n",
        "                outGT = torch.cat((outGT, target), 0)\n",
        "\n",
        "                lossVal+=losstensor.item()\n",
        "                del varOutput, varTarget, varInput, target, inputs\n",
        "        lossVal = lossVal / len(self.val_dl)\n",
        "\n",
        "        aurocIndividual = self.computeAUROC(outGT, outPRED, self.val_dl.dataset.class_ids_loaded)\n",
        "        self.val_losses.append(lossVal)\n",
        "\n",
        "        return lossVal, aurocIndividual\n",
        "\n",
        "    def test(self):\n",
        "        cudnn.benchmark = True\n",
        "        outGT = torch.FloatTensor().cuda()\n",
        "        outPRED = torch.FloatTensor().cuda()\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        for i, (inputs, target) in enumerate(tqdm(self.test_dl)):\n",
        "            with torch.no_grad():\n",
        "                target = target.to(self.device)\n",
        "                outGT = torch.cat((outGT, target), 0)\n",
        "\n",
        "                bs, n_crops, c, h, w = inputs.size()\n",
        "\n",
        "                varInput = torch.autograd.Variable(inputs.view(-1, c, h, w).to(self.device))\n",
        "\n",
        "                out, _ = self.model(varInput, n_crops=n_crops, bs=bs)\n",
        "\n",
        "                outPRED = torch.cat((outPRED, out.data), 0)\n",
        "\n",
        "        aurocIndividual = self.computeAUROC(outGT, outPRED, self.test_dl.dataset.class_ids_loaded)\n",
        "\n",
        "        return aurocIndividual\n",
        "\n",
        "    def computeAUROC(self, dataGT, dataPRED, class_ids):\n",
        "        outAUROC = []\n",
        "        datanpGT = dataGT.cpu().numpy()\n",
        "        datanpPRED = dataPRED.cpu().numpy()\n",
        "\n",
        "        for i in class_ids:\n",
        "            outAUROC.append(roc_auc_score(datanpGT[:, i], datanpPRED[:, i]))\n",
        "        return outAUROC\n",
        "\n",
        "    def write_results(self, aurocIndividual, class_ids, prefix='val', mode='a'):\n",
        "        with open(f\"{self.args.save_dir}/results.txt\", mode) as results_file:\n",
        "            aurocMean = aurocIndividual.mean()\n",
        "\n",
        "            results_file.write(f'{prefix} AUROC mean {aurocMean:0.4f}\\n')\n",
        "            for i, class_id in enumerate(class_ids):\n",
        "                results_file.write(f'{self.val_dl.dataset.CLASSES[class_id]} {aurocIndividual[i]:0.4f}\\n')\n",
        "\n",
        "    def print_auroc(self, aurocIndividual, class_ids, prefix='val'):\n",
        "        aurocMean = aurocIndividual.mean()\n",
        "\n",
        "        print (f'{prefix} AUROC mean {aurocMean:0.4f}')\n",
        "\n",
        "        for i, class_id in enumerate(class_ids):\n",
        "            print (f'{self.val_dl.dataset.CLASSES[class_id]} {aurocIndividual[i]:0.4f}')"
      ],
      "metadata": {
        "id": "T24vragaopeZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "argParser = argparse.ArgumentParser(description='arguments')\n",
        "\n",
        "argParser.add_argument('--data-root', default='', type=str, help='the path to dataset')\n",
        "argParser.add_argument('--save-dir', default='', type=str, help='the path to save the checkpoints')\n",
        "argParser.add_argument('--train-file', default=f'train.txt', type=str, help='the path to train list ')\n",
        "argParser.add_argument('--val-file', default=f'val.txt', type=str, help='the path to val list ')\n",
        "argParser.add_argument('--test-file', default=f'test.txt', type=str, help='the path to test list')\n",
        "\n",
        "argParser.add_argument('--pretrained', dest='pretrained', action='store_true',  help='load imagenet pretrained model')\n",
        "argParser.add_argument('--bce-only', dest='bce_only', help='train with only binary cross entropy loss', action='store_true')\n",
        "\n",
        "argParser.add_argument('--num-classes', default=14, type=int, help='number of classes')\n",
        "argParser.add_argument('--batch-size', default=16, type=int, help='training batch size')\n",
        "argParser.add_argument('--epochs', default=40, type=int, help='number of epochs to train')\n",
        "argParser.add_argument('--vision-backbone', default='densenet121', type=str, help='[densenet121, densenet169, densenet201]')\n",
        "argParser.add_argument('--resume-from', default=None, type=str, help='path to checkpoint to resume the training from')\n",
        "argParser.add_argument('--load-from', default=None, type=str, help='path to checkpoint to load the weights from')\n",
        "\n",
        "argParser.add_argument('--resize', default=256, type=int, help='number of epochs to train')\n",
        "argParser.add_argument('--crop', default=224, type=int, help='number of epochs to train')\n",
        "argParser.add_argument('--lr', default=0.0001, type=float, help='learning rate')\n",
        "argParser.add_argument('--steps', default='20, 40, 60, 80', type=str, help='learning rate decay steps comma separated')\n",
        "\n",
        "argParser.add_argument('--beta-map', default=0.1, type=float, help='learning rate')\n",
        "argParser.add_argument('--beta-con', default=0.1, type=float, help='learning rate')\n",
        "argParser.add_argument('--beta-rank', default=1, type=float, help='learning rate')\n",
        "argParser.add_argument('--neg-penalty', default=0.03, type=float, help='learning rate')\n",
        "\n",
        "argParser.add_argument('--wo-con', dest='wo_con', help='train with out semantic consistency regularizer loss', action='store_true')\n",
        "argParser.add_argument('--wo-map', dest='wo_map', help='train with out alignement loss', action='store_true')\n",
        "\n",
        "argParser.add_argument('--textual-embeddings', default=CLASS_EMBEDDINGS, type=str, help='the path to labels embeddings')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtX9ha2Io3K1",
        "outputId": "67c96eab-f1ea-46ab-c306-da9a6de3b446"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--textual-embeddings'], dest='textual_embeddings', nargs=None, const=None, default='nih_chest_xray_biobert.npy', type=<class 'str'>, choices=None, required=False, help='the path to labels embeddings', metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "\n",
        "def harmonic_mean(a, b):\n",
        "    return (2 * a * b) / (a + b)"
      ],
      "metadata": {
        "id": "QsisAU1Lr9P1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DenseNet-121"
      ],
      "metadata": {
        "id": "9sutLLxUpf0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arg_list = ['--epochs', '1',\n",
        "            '--neg-penalty', '0.5',\n",
        "            '--save-dir', DRIVE_PATH,\n",
        "            '--vision-backbone', 'densenet121']\n",
        "\n",
        "args = argParser.parse_args(arg_list)\n",
        "\n",
        "trainer = ChexnetTrainer(args)\n",
        "\n",
        "start = time.time()\n",
        "trainer()\n",
        "end = time.time()\n",
        "\n",
        "checkpoint = torch.load(f'{DRIVE_PATH}/best_auroc_checkpoint.pth.tar', weights_only=False)\n",
        "trainer.model.load_state_dict(checkpoint['state_dict'])\n",
        "test_ind_auroc = trainer.test()\n",
        "test_ind_auroc = np.array(test_ind_auroc)\n",
        "\n",
        "results[\"DenseNet-121\"] = {\n",
        "        \"Seen Mean (AUROC)\":          test_ind_auroc[trainer.test_dl.dataset.seen_class_ids].mean(),\n",
        "        \"Unseen Mean (AUROC)\":        test_ind_auroc[trainer.test_dl.dataset.unseen_class_ids].mean(),\n",
        "        \"Harmonic Mean (AUROC)\":      harmonic_mean(test_ind_auroc[trainer.test_dl.dataset.seen_class_ids].mean(), test_ind_auroc[trainer.test_dl.dataset.unseen_class_ids].mean()),\n",
        "        \"Training Time (hours)\":      (end - start) / (60 * 60)\n",
        "}\n",
        "\n",
        "os.rename(f'{DRIVE_PATH}/best_auroc_checkpoint.pth.tar', f'{DRIVE_PATH}/best_auroc_checkpoint_densenet121.pth.tar')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXpsvZcno87t",
        "outputId": "cf19c850-77de-4f94-9d3c-49a8b4720c58"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data partition path: train.txt\n",
            "Number of images: 14160\n",
            "Number of max labels per image: 6\n",
            "Number of classes: 10\n",
            "data partition path: val.txt\n",
            "Number of images: 1979\n",
            "Number of max labels per image: 6\n",
            "Number of classes: 10\n",
            "data partition path: test.txt\n",
            "Number of images: 4737\n",
            "Number of max labels per image: 7\n",
            "Number of classes: 14\n",
            "['Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia', 'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia']\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:28        [0099/885] lr: \t1.0000E-04 loss: \t3.82649\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:3         [0199/885] lr: \t1.0000E-04 loss: \t3.78114\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:54        [0299/885] lr: \t1.0000E-04 loss: \t3.78176\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:44        [0399/885] lr: \t1.0000E-04 loss: \t3.76971\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:38        [0499/885] lr: \t1.0000E-04 loss: \t3.71883\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:26        [0599/885] lr: \t1.0000E-04 loss: \t3.67482\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:17        [0699/885] lr: \t1.0000E-04 loss: \t3.65035\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:8         [0799/885] lr: \t1.0000E-04 loss: \t3.62095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:22<00:00,  1.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving last_epoch checkpoint\n",
            "saving best_auroc checkpoint\n",
            "saving min_loss checkpoint\n",
            "val AUROC mean 0.6221\n",
            "Atelectasis 0.5740\n",
            "Cardiomegaly 0.6536\n",
            "Effusion 0.5748\n",
            "Infiltration 0.5883\n",
            "Mass 0.5601\n",
            "Nodule 0.5951\n",
            "Pneumothorax 0.6621\n",
            "Consolidation 0.6389\n",
            "Pleural_Thickening 0.6110\n",
            "Hernia 0.7626\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 99/99 [00:30<00:00,  3.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "test_seen AUROC mean 0.6004\n",
            "Atelectasis 0.5570\n",
            "Effusion 0.5808\n",
            "Infiltration 0.5811\n",
            "Mass 0.5614\n",
            "Nodule 0.6353\n",
            "Pneumothorax 0.5476\n",
            "Consolidation 0.6294\n",
            "Cardiomegaly 0.5858\n",
            "Pleural_Thickening 0.5912\n",
            "Hernia 0.7345\n",
            "\n",
            "test_unseen AUROC mean 0.4945\n",
            "Edema 0.4839\n",
            "Pneumonia 0.4973\n",
            "Emphysema 0.4807\n",
            "Fibrosis 0.5158\n",
            "best epoch 1 best auroc 0.6220506486214765 loss 3.481332 auroc at min loss 0.6221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 99/99 [00:29<00:00,  3.34it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EfficientNet-B0"
      ],
      "metadata": {
        "id": "VG_lk31Epi76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arg_list = ['--epochs', '1',\n",
        "            '--neg-penalty', '0.5',\n",
        "            '--save-dir', DRIVE_PATH,\n",
        "            '--vision-backbone', 'efficientnet_b0']\n",
        "\n",
        "args = argParser.parse_args(arg_list)\n",
        "\n",
        "trainer = ChexnetTrainer(args)\n",
        "\n",
        "start = time.time()\n",
        "trainer()\n",
        "end = time.time()\n",
        "\n",
        "checkpoint = torch.load(f'{DRIVE_PATH}/best_auroc_checkpoint.pth.tar', weights_only=False)\n",
        "trainer.model.load_state_dict(checkpoint['state_dict'])\n",
        "test_ind_auroc = trainer.test()\n",
        "test_ind_auroc = np.array(test_ind_auroc)\n",
        "\n",
        "results[\"EfficientNet-B0\"] = {\n",
        "        \"Seen Mean (AUROC)\":          test_ind_auroc[trainer.test_dl.dataset.seen_class_ids].mean(),\n",
        "        \"Unseen Mean (AUROC)\":        test_ind_auroc[trainer.test_dl.dataset.unseen_class_ids].mean(),\n",
        "        \"Harmonic Mean (AUROC)\":      harmonic_mean(test_ind_auroc[trainer.test_dl.dataset.seen_class_ids].mean(), test_ind_auroc[trainer.test_dl.dataset.unseen_class_ids].mean()),\n",
        "        \"Training Time (hours)\":      (end - start) / (60 * 60)\n",
        "}\n",
        "\n",
        "os.rename(f'{DRIVE_PATH}/best_auroc_checkpoint.pth.tar', f'{DRIVE_PATH}/best_auroc_checkpoint_efficientnet_b0.pth.tar')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqeSJxBHpjBj",
        "outputId": "a8dbfe07-6a46-4fc6-e374-b66650307b38"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data partition path: train.txt\n",
            "Number of images: 14160\n",
            "Number of max labels per image: 6\n",
            "Number of classes: 10\n",
            "data partition path: val.txt\n",
            "Number of images: 1979\n",
            "Number of max labels per image: 6\n",
            "Number of classes: 10\n",
            "data partition path: test.txt\n",
            "Number of images: 4737\n",
            "Number of max labels per image: 7\n",
            "Number of classes: 14\n",
            "['Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia', 'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia']\n",
            " epoch [0001 / 0001] eta: 0 Days 0:1:1         [0099/885] lr: \t1.0000E-04 loss: \t3.98477\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:41        [0199/885] lr: \t1.0000E-04 loss: \t3.82956\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:35        [0299/885] lr: \t1.0000E-04 loss: \t3.74493\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:28        [0399/885] lr: \t1.0000E-04 loss: \t3.69547\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:23        [0499/885] lr: \t1.0000E-04 loss: \t3.67134\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:17        [0599/885] lr: \t1.0000E-04 loss: \t3.65143\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:11        [0699/885] lr: \t1.0000E-04 loss: \t3.64640\n",
            " epoch [0001 / 0001] eta: 0 Days 0:0:5         [0799/885] lr: \t1.0000E-04 loss: \t3.65463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:21<00:00,  1.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving last_epoch checkpoint\n",
            "saving best_auroc checkpoint\n",
            "saving min_loss checkpoint\n",
            "val AUROC mean 0.5373\n",
            "Atelectasis 0.5267\n",
            "Cardiomegaly 0.5624\n",
            "Effusion 0.4954\n",
            "Infiltration 0.5686\n",
            "Mass 0.4190\n",
            "Nodule 0.5768\n",
            "Pneumothorax 0.5063\n",
            "Consolidation 0.6284\n",
            "Pleural_Thickening 0.5841\n",
            "Hernia 0.5057\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 99/99 [00:29<00:00,  3.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "test_seen AUROC mean 0.5347\n",
            "Atelectasis 0.5197\n",
            "Effusion 0.5147\n",
            "Infiltration 0.5695\n",
            "Mass 0.4526\n",
            "Nodule 0.6046\n",
            "Pneumothorax 0.4820\n",
            "Consolidation 0.6310\n",
            "Cardiomegaly 0.5153\n",
            "Pleural_Thickening 0.5801\n",
            "Hernia 0.4771\n",
            "\n",
            "test_unseen AUROC mean 0.5184\n",
            "Edema 0.7259\n",
            "Pneumonia 0.4922\n",
            "Emphysema 0.4615\n",
            "Fibrosis 0.3939\n",
            "best epoch 1 best auroc 0.537337248472386 loss 3.568677 auroc at min loss 0.5373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 99/99 [00:30<00:00,  3.27it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "wGla3zrJp2C-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(results).T.round(2)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.axis('off')\n",
        "\n",
        "tbl = table(ax, df, loc='center', colWidths=[ 0.02 * (len(col)+2) for col in df.columns.tolist() ])\n",
        "tbl.auto_set_font_size(False)\n",
        "tbl.set_fontsize(12)\n",
        "tbl.scale(1.2, 2)\n",
        "\n",
        "plt.savefig(\"extension.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "t8QN8AH8p3gC",
        "outputId": "87a23db2-9fcb-43a9-b0a3-7b6e390f4abd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}