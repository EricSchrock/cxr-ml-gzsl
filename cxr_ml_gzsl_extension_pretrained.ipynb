{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Trained CXR-ML-GZSL Extension Results"
      ],
      "metadata": {
        "id": "G-M0ISx4OdDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "This notebook uses pre-trained weights to reproduce an [extension](https://github.com/EricSchrock/cxr-ml-gzsl/blob/main/cxr_ml_gzsl_extension.ipynb) of the findings of the paper, [\"Multi-Label Generalized Zero Shot Learning for the Classification of Disease in Chest Radiographs\"](https://arxiv.org/abs/2107.06563).\n",
        "\n",
        "**Note**: The dataset is large. Expect significant download times.\n",
        "\n",
        "**Note**: If you get an error when loading the pretrained weights, it's possible they got corrupted during the download. You can manually download them at the following links.\n",
        "\n",
        "1. https://drive.google.com/file/d/1rzvBXBX8u3Yv22ACe2qGeEpMffF6uwNh/view?usp=drive_link\n",
        "2. https://drive.google.com/file/d/14flyOtpZGwBDP0pPHxHkqZWtbE-8JcNP/view?usp=drive_link"
      ],
      "metadata": {
        "id": "s0M81qnMOgWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "from datetime import datetime, timedelta\n",
        "import glob\n",
        "import multiprocessing\n",
        "import os\n",
        "import requests\n",
        "from statistics import mean\n",
        "import tarfile\n",
        "import time\n",
        "import urllib.request\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.plotting import table\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "UyYWX0wNPkRD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 1002\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)"
      ],
      "metadata": {
        "id": "EuPH2JgQPnop"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment"
      ],
      "metadata": {
        "id": "fkPmtv5vQE53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!if command -v nvidia-smi &> /dev/null; then nvidia-smi --query-gpu=name --format=csv,noheader; else echo 'No NVIDIA GPU detected'; fi\n",
        "!echo\n",
        "!python --version\n",
        "!echo\n",
        "!pip list | grep -E \"matplotlib|numpy|pandas|pillow|scikit-learn|tqdm|torch|torchvision\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXquekyVQFA1",
        "outputId": "ccb6e91c-0c89-4b03-d9fa-6884e9f89c10"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA A100-SXM4-40GB\n",
            "\n",
            "Python 3.11.12\n",
            "\n",
            "geopandas                             1.0.1\n",
            "matplotlib                            3.10.0\n",
            "matplotlib-inline                     0.1.7\n",
            "matplotlib-venn                       1.1.2\n",
            "numpy                                 2.0.2\n",
            "pandas                                2.2.2\n",
            "pandas-datareader                     0.10.0\n",
            "pandas-gbq                            0.28.0\n",
            "pandas-stubs                          2.2.2.240909\n",
            "pillow                                11.1.0\n",
            "scikit-learn                          1.6.1\n",
            "sklearn-pandas                        2.2.0\n",
            "torch                                 2.6.0+cu124\n",
            "torchaudio                            2.6.0+cu124\n",
            "torchsummary                          1.5.1\n",
            "torchvision                           0.21.0+cu124\n",
            "tqdm                                  4.67.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download"
      ],
      "metadata": {
        "id": "hTcieaB3QBec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = [\n",
        "    {\"filename\": \"best_auroc_checkpoint_densenet121.pth.tar\", \"url\": \"https://drive.google.com/uc?export=download&id=1rzvBXBX8u3Yv22ACe2qGeEpMffF6uwNh\"},\n",
        "    {\"filename\": \"best_auroc_checkpoint_efficientnet_b0.pth.tar\", \"url\": \"https://drive.google.com/uc?export=download&id=14flyOtpZGwBDP0pPHxHkqZWtbE-8JcNP\"},\n",
        "]\n",
        "\n",
        "for item in weights:\n",
        "    response = requests.get(item['url'])\n",
        "\n",
        "    with open(item['filename'], 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    print(f\"Downloaded: {item['filename']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEGHl_ywQDEc",
        "outputId": "0feaaeaa-2110-404c-9c56-a8f57950e131"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: best_auroc_checkpoint_densenet121.pth.tar\n",
            "Downloaded: best_auroc_checkpoint_efficientnet_b0.pth.tar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for filename in [\"train.txt\", \"val.txt\", \"test.txt\"]:\n",
        "    response = requests.get(f\"https://raw.githubusercontent.com/nyuad-cai/CXR-ML-GZSL/master/dataset_splits/{filename}\")\n",
        "\n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(response.text)\n",
        "\n",
        "    print(f\"Downloaded: {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIg7kjlXQDHA",
        "outputId": "6a918860-0c7e-4d28-aaf1-0c0f7552f1c1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: train.txt\n",
            "Downloaded: val.txt\n",
            "Downloaded: test.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"Data_Entry_2017_v2020.csv\"\n",
        "response = requests.get('https://drive.google.com/uc?export=download&id=1mkOZNfYt-Px52b8CJZJANNbM3ULUVO3f')\n",
        "\n",
        "with open(filename, \"wb\") as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "print(f\"Downloaded: {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWRmbO8aQDJU",
        "outputId": "d8f3d1f5-97ef-40e7-bd17-c13aedf68394"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: Data_Entry_2017_v2020.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CLASS_EMBEDDINGS = \"nih_chest_xray_biobert.npy\"\n",
        "\n",
        "response = requests.get(f\"https://raw.githubusercontent.com/nyuad-cai/CXR-ML-GZSL/master/embeddings/{CLASS_EMBEDDINGS}\")\n",
        "\n",
        "with open(CLASS_EMBEDDINGS, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "print(f\"Downloaded: {CLASS_EMBEDDINGS}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKLYZ-YvQDLv",
        "outputId": "5452fd6e-8649-442d-e9b9-c337f73a5be6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: nih_chest_xray_biobert.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    {\"filename\": \"images_001.tar.gz\", \"url\": \"https://nihcc.box.com/shared/static/vfk49d74nhbxq3nqjg0900w5nvkorp5c.gz\"},\n",
        "    {\"filename\": \"images_002.tar.gz\", \"url\": \"https://nihcc.box.com/shared/static/i28rlmbvmfjbl8p2n3ril0pptcmcu9d1.gz\"},\n",
        "    {\"filename\": \"images_003.tar.gz\", \"url\": \"https://nihcc.box.com/shared/static/f1t00wrtdk94satdfb9olcolqx20z2jp.gz\"},\n",
        "    {\"filename\": \"images_004.tar.gz\", \"url\": \"https://nihcc.box.com/shared/static/0aowwzs5lhjrceb3qp67ahp0rd1l1etg.gz\"},\n",
        "    {\"filename\": \"images_005.tar.gz\", \"url\": \"https://nihcc.box.com/shared/static/v5e3goj22zr6h8tzualxfsqlqaygfbsn.gz\"},\n",
        "    {\"filename\": \"images_006.tar.gz\", \"url\": \"https://nihcc.box.com/shared/static/asi7ikud9jwnkrnkj99jnpfkjdes7l6l.gz\"},\n",
        "]\n",
        "\n",
        "for item in dataset:\n",
        "    filename = item[\"filename\"]\n",
        "    url = item[\"url\"]\n",
        "\n",
        "    urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "    with tarfile.open(filename, \"r:gz\") as tar:\n",
        "        tar.extractall()\n",
        "\n",
        "    os.remove(filename)\n",
        "\n",
        "    print(f\"Downloaded and extracted: {filename}\")\n",
        "\n",
        "IMAGE_PATH = \"images\"\n",
        "\n",
        "assert os.path.exists(IMAGE_PATH), \"Dataset is not in the expected directory!\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exaZDn41QskV",
        "outputId": "7d39e8b1-5a93-4ac4-f71d-dc38a0274745"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded and extracted: images_001.tar.gz\n",
            "Downloaded and extracted: images_002.tar.gz\n",
            "Downloaded and extracted: images_003.tar.gz\n",
            "Downloaded and extracted: images_004.tar.gz\n",
            "Downloaded and extracted: images_005.tar.gz\n",
            "Downloaded and extracted: images_006.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "lzBSsGOWQxNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NIHChestXray(Dataset):\n",
        "    def __init__ (self, args, pathDatasetFile, transform, classes_to_load='seen', exclude_all=True):\n",
        "        self.listImagePaths = []\n",
        "        self.listImageLabels = []\n",
        "        self.transform = transform\n",
        "        self.num_classes = args.num_classes\n",
        "\n",
        "        self._data_path = args.data_root\n",
        "        self.args = args\n",
        "\n",
        "        self.split_path = pathDatasetFile\n",
        "        self.CLASSES = [ 'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia',\n",
        "                'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia']\n",
        "\n",
        "        self.unseen_classes = ['Edema', 'Pneumonia', 'Emphysema', 'Fibrosis']\n",
        "\n",
        "        self.seen_classes = [ 'Atelectasis', 'Effusion', 'Infiltration', 'Mass', 'Nodule',\n",
        "                'Pneumothorax', 'Consolidation', 'Cardiomegaly', 'Pleural_Thickening', 'Hernia']\n",
        "\n",
        "        self._class_ids = {v: i for i, v in enumerate(self.CLASSES) if v != 'No Finding'}\n",
        "\n",
        "        self.seen_class_ids = [self._class_ids[label] for label in self.seen_classes]\n",
        "        self.unseen_class_ids = [self._class_ids[label] for label in self.unseen_classes]\n",
        "\n",
        "        self.classes_to_load = classes_to_load\n",
        "        self.exclude_all = exclude_all\n",
        "        self._construct_index()\n",
        "\n",
        "    def _construct_index(self):\n",
        "        # Compile the split data path\n",
        "        max_labels = 0\n",
        "        paths = glob.glob('images/*.png' if self._data_path == '' else f'{self._data_path}/images/*.png')\n",
        "        self.names_to_path = {path.split('/')[-1]: path for path in paths}\n",
        "        data_entry_file = 'Data_Entry_2017_v2020.csv'\n",
        "\n",
        "        print(f'data partition path: {self.split_path}')\n",
        "        with open(self.split_path, 'r') as f: file_names = f.readlines()\n",
        "\n",
        "        split_file_names = np.array([file_name.strip().split(' ')[0].split('/')[-1] for file_name in file_names])\n",
        "        df = pd.read_csv(f'{data_entry_file}' if self._data_path == '' else f'{self._data_path}/{data_entry_file}')\n",
        "        image_index = df.iloc[:, 0].values\n",
        "\n",
        "        _, split_index, _ = np.intersect1d(image_index, split_file_names, return_indices=True)\n",
        "\n",
        "        labels = df.iloc[:, 1].values\n",
        "        labels = np.array(labels)[split_index]\n",
        "\n",
        "        labels = [label.split('|') for label in labels]\n",
        "\n",
        "        image_index = image_index[split_index]\n",
        "\n",
        "        # Construct the image db\n",
        "        self._imdb = []\n",
        "        self.class_ids_loaded = []\n",
        "        for index in range(len(split_index)):\n",
        "            if len(labels[index]) == 1 and labels[index][0] == 'No Finding':\n",
        "                continue\n",
        "            if self._should_load_image(labels[index]) is False:\n",
        "                continue\n",
        "            if image_index[index] not in self.names_to_path.keys():\n",
        "                continue\n",
        "            class_ids = [self._class_ids[label] for label in labels[index]]\n",
        "            self.class_ids_loaded +=class_ids\n",
        "            self._imdb.append({\n",
        "                'im_path': self.names_to_path[image_index[index]],\n",
        "                'labels': class_ids,\n",
        "            })\n",
        "            max_labels = max(max_labels, len(class_ids))\n",
        "\n",
        "        self.class_ids_loaded = np.unique(np.array(self.class_ids_loaded))\n",
        "        print(f'Number of images: {len(self._imdb)}')\n",
        "        print(f'Number of max labels per image: {max_labels}')\n",
        "        print(f'Number of classes: {len(self.class_ids_loaded)}')\n",
        "\n",
        "    def _should_load_image(self, labels):\n",
        "        selected_class_labels = self.CLASSES\n",
        "        if self.classes_to_load == 'seen':\n",
        "            selected_class_labels = self.seen_classes\n",
        "        elif self.classes_to_load == 'unseen':\n",
        "            selected_class_labels = self.unseen_classes\n",
        "        elif self.classes_to_load == 'all':\n",
        "            return True\n",
        "\n",
        "        count = 0\n",
        "        for label in labels:\n",
        "            if label in selected_class_labels:\n",
        "                count+=1\n",
        "\n",
        "        if count == len(labels):\n",
        "            # all labels from selected sub set\n",
        "            return True\n",
        "        elif count == 0:\n",
        "            # none label in selected sub set\n",
        "            return False\n",
        "        else:\n",
        "            # some labels in selected sub set\n",
        "            if self.exclude_all is True:\n",
        "                return False\n",
        "            else:\n",
        "                return True\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        imagePath = self._imdb[index]['im_path']\n",
        "        imageData = Image.open(imagePath).convert('RGB')\n",
        "        labels = torch.tensor(self._imdb[index]['labels'])\n",
        "        labels = labels.unsqueeze(0)\n",
        "        imageLabel = torch.zeros(labels.size(0), self.num_classes).scatter_(1, labels, 1.).squeeze()\n",
        "        img = self.transform(imageData)\n",
        "\n",
        "        return img, imageLabel\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._imdb)"
      ],
      "metadata": {
        "id": "mnsyJzWAQxU0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "F_vV03p1QutY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RankingLoss(nn.Module):\n",
        "    def __init__(self, neg_penalty=0.03):\n",
        "        super(RankingLoss, self).__init__()\n",
        "\n",
        "        self.neg_penalty = neg_penalty\n",
        "\n",
        "    def forward(self, ranks, labels, class_ids_loaded, device):\n",
        "        '''\n",
        "        for each correct it should be higher then the absence\n",
        "        '''\n",
        "        labels = labels[:, class_ids_loaded]\n",
        "        ranks_loaded = ranks[:, class_ids_loaded]\n",
        "        neg_labels = 1+(labels*-1)\n",
        "        loss_rank = torch.zeros(1).to(device)\n",
        "        for i in range(len(labels)):\n",
        "            correct = ranks_loaded[i, labels[i]==1]\n",
        "            wrong = ranks_loaded[i, neg_labels[i]==1]\n",
        "            correct = correct.reshape((-1, 1)).repeat((1, len(wrong)))\n",
        "            wrong = wrong.repeat(len(correct)).reshape(len(correct), -1)\n",
        "            image_level_penalty = ((self.neg_penalty+wrong) - correct)\n",
        "            image_level_penalty[image_level_penalty<0]=0\n",
        "            loss_rank += image_level_penalty.sum()\n",
        "        loss_rank /=len(labels)\n",
        "\n",
        "        return loss_rank\n",
        "\n",
        "class CosineLoss(nn.Module):\n",
        "    def forward(self, t_emb, v_emb ):\n",
        "        a_norm = v_emb / v_emb.norm(dim=1)[:, None]\n",
        "        b_norm = t_emb / t_emb.norm(dim=1)[:, None]\n",
        "        loss = 1 - torch.mean(torch.diagonal(torch.mm(a_norm, b_norm.t()), 0))\n",
        "\n",
        "        return loss\n",
        "\n",
        "class ZSLNet(nn.Module):\n",
        "    def __init__(self, args, textual_embeddings=None, device='cpu'):\n",
        "        super(ZSLNet, self).__init__()\n",
        "        self.args = args\n",
        "        self.device = device\n",
        "        self.vision_backbone = getattr(torchvision.models, self.args.vision_backbone)(pretrained=self.args.pretrained)\n",
        "        # remove classification layer from visual encoder\n",
        "        classifiers = [ 'classifier', 'fc']\n",
        "        for classifier in classifiers:\n",
        "            cls_layer = getattr(self.vision_backbone, classifier, None)\n",
        "            if cls_layer is None:\n",
        "                continue\n",
        "            if isinstance(cls_layer, nn.Sequential):\n",
        "                last_layer = cls_layer[-1]\n",
        "                d_visual = last_layer.in_features\n",
        "            else:\n",
        "                d_visual = cls_layer.in_features\n",
        "            setattr(self.vision_backbone, classifier, nn.Identity(d_visual))\n",
        "            break\n",
        "\n",
        "        pretrained_encoder = False\n",
        "        if pretrained_encoder:\n",
        "            self.vision_backbone.classifier = nn.Identity(d_visual)\n",
        "\n",
        "            path = 'checkpoints/bce_only_imagenet/last_epoch_checkpoint.pth.tar'\n",
        "\n",
        "            self.classifier = nn.Sequential(nn.Linear(d_visual, self.args.num_classes), nn.Sigmoid())\n",
        "            checkpoint = torch.load(path, weights_only=False)\n",
        "            self.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "            for p in self.vision_backbone.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        if self.args.bce_only:\n",
        "            self.bce_loss = torch.nn.BCELoss(size_average=True)\n",
        "            self.classifier = nn.Sequential(nn.Linear(d_visual, self.args.num_classes), nn.Sigmoid())\n",
        "        else:\n",
        "            self.emb_loss = CosineLoss()\n",
        "            self.ranking_loss = RankingLoss(neg_penalty=self.args.neg_penalty)\n",
        "            self.textual_embeddings = textual_embeddings\n",
        "            d_textual = self.textual_embeddings.shape[-1]\n",
        "\n",
        "            self.textual_embeddings = torch.from_numpy(self.textual_embeddings).to(self.device)\n",
        "\n",
        "            self.fc_v = nn.Sequential(\n",
        "                nn.Linear(d_visual, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(512, 256),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(256, 128),\n",
        "            )\n",
        "\n",
        "            self.fc_t = nn.Sequential(\n",
        "                nn.Linear(d_textual, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(512, 256),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(256, 128)\n",
        "            )\n",
        "\n",
        "    def forward(self, x, labels=None, epoch=0, n_crops=0, bs=16):\n",
        "        if self.args.bce_only:\n",
        "            return self.forward_bce_only(x, labels=labels, n_crops=n_crops, bs=bs)\n",
        "        else:\n",
        "            return self.forward_ranking(x, labels=labels, epoch=epoch, n_crops=n_crops, bs=bs)\n",
        "\n",
        "    def forward_bce_only(self, x, labels=None, n_crops=0, bs=16):\n",
        "        lossvalue_bce = torch.zeros(1).to(self.device)\n",
        "\n",
        "        visual_feats = self.vision_backbone(x)\n",
        "        preds = self.classifier(visual_feats)\n",
        "\n",
        "        if labels is not None:\n",
        "            lossvalue_bce = self.bce_loss(preds, labels)\n",
        "\n",
        "        return preds, lossvalue_bce, f'bce:\\t {lossvalue_bce.item():0.4f}'\n",
        "\n",
        "    def forward_ranking(self, x, labels=None, epoch=0, n_crops=0, bs=16):\n",
        "        loss_rank = torch.zeros(1).to(self.device)\n",
        "        loss_allignment_cos = torch.zeros(1).to(self.device)\n",
        "        loss_mapping_consistency = torch.zeros(1).to(self.device)\n",
        "\n",
        "        visual_feats = self.vision_backbone(x)\n",
        "        visual_feats = self.fc_v(visual_feats)\n",
        "        text_feats = self.fc_t(self.textual_embeddings)\n",
        "\n",
        "        if not self.args.wo_con and epoch >= 0:\n",
        "            text_mapped_sim = self.sim_score(text_feats, text_feats.detach())\n",
        "            text_orig_sim = self.sim_score(self.textual_embeddings, self.textual_embeddings)\n",
        "            loss_mapping_consistency = torch.abs(text_orig_sim - text_mapped_sim).mean()\n",
        "\n",
        "        if labels is not None:\n",
        "            mapped_visual, mapped_text = self.map_visual_text(visual_feats, labels, text_feats)\n",
        "            if mapped_visual is not None and not self.args.wo_map and epoch >= 0:\n",
        "                loss_allignment_cos = self.emb_loss(mapped_text, mapped_visual)\n",
        "\n",
        "        ranks = self.sim_score(visual_feats, text_feats)\n",
        "        if n_crops > 0:\n",
        "            ranks = ranks.view(bs, n_crops, -1).mean(1)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_rank = self.ranking_loss(ranks, labels, self.class_ids_loaded, self.device)\n",
        "        loss_allignment_cos = (self.args.beta_map * loss_allignment_cos)\n",
        "        loss_rank = (self.args.beta_rank * loss_rank)\n",
        "        loss_mapping_consistency = (self.args.beta_con * loss_mapping_consistency)\n",
        "        losses = loss_rank + loss_mapping_consistency + loss_allignment_cos\n",
        "\n",
        "        return ranks, losses\n",
        "\n",
        "    def sim_score(self, a, b):\n",
        "        a_norm = a / a.norm(dim=1)[:, None]\n",
        "        b_norm = b / (1e-6+b.norm(dim=1))[:, None]\n",
        "        score = (torch.mm(a_norm, b_norm.t()))\n",
        "\n",
        "        return score\n",
        "\n",
        "    def map_visual_text(self, visual_feats, labels, labels_embd):\n",
        "        mapped_labels_embd = []\n",
        "        labels == 1\n",
        "        for i in range(0, labels.shape[0]):\n",
        "            class_embd = labels_embd[labels[i]==1].mean(dim=0)[None,:]\n",
        "            mapped_labels_embd.append(class_embd)\n",
        "        mapped_labels_embd = torch.cat(mapped_labels_embd)\n",
        "\n",
        "        return visual_feats.detach(), mapped_labels_embd.detach()"
      ],
      "metadata": {
        "id": "GLeEGJLgQv8r"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helpers"
      ],
      "metadata": {
        "id": "3r6xZAeoRFv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_array(array, disc='loss'):\n",
        "    plt.plot(array)\n",
        "    plt.ylabel(disc)\n",
        "    plt.savefig(f'{disc}.pdf')\n",
        "    plt.close()\n",
        "\n",
        "class ChexnetTrainer(object):\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.textual_embeddings = np.load(args.textual_embeddings)\n",
        "\n",
        "        self.model = ZSLNet(self.args, self.textual_embeddings, self.device).to(self.device)\n",
        "        self.optimizer = optim.Adam (self.model.parameters(), lr=self.args.lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n",
        "        self.scheduler = ReduceLROnPlateau(self.optimizer, factor=0.01, patience=10, mode='min')\n",
        "\n",
        "        self.loss = torch.nn.BCELoss(size_average=True)\n",
        "        self.auroc_min_loss = 0.0\n",
        "\n",
        "        self.start_epoch = 1\n",
        "        self.lossMIN = float('inf')\n",
        "        self.max_auroc_mean = float('-inf')\n",
        "        self.best_epoch = 1\n",
        "\n",
        "        self.val_losses = []\n",
        "\n",
        "        self.resume_from()\n",
        "        self.load_from()\n",
        "        self.init_dataset()\n",
        "\n",
        "        self.steps = [int(step) for step in self.args.steps.split(',')]\n",
        "        self.time_start = time.time()\n",
        "        self.time_end = time.time()\n",
        "        self.should_test = False\n",
        "        self.model.class_ids_loaded = self.train_dl.dataset.class_ids_loaded\n",
        "\n",
        "    def __call__(self):\n",
        "        self.train()\n",
        "\n",
        "    def load_from(self):\n",
        "        if self.args.load_from is not None:\n",
        "            checkpoint = torch.load(self.args.load_from, weights_only=False)\n",
        "            self.model.load_state_dict(checkpoint['state_dict'])\n",
        "            print(f'loaded checkpoint from {self.args.load_from}')\n",
        "\n",
        "    def resume_from(self):\n",
        "        if self.args.resume_from is not None:\n",
        "            checkpoint = torch.load(self.args.resume_from, weights_only=False)\n",
        "            self.model.load_state_dict(checkpoint['state_dict'])\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            self.start_epoch = checkpoint['epoch'] + 1\n",
        "            self.lossMIN = checkpoint['lossMIN']\n",
        "            self.max_auroc_mean = checkpoint['max_auroc_mean']\n",
        "            print(f'resuming training from epoch {self.start_epoch}')\n",
        "\n",
        "    def save_checkpoint(self, prefix='best'):\n",
        "        path = f'{self.args.save_dir}/{prefix}_checkpoint.pth.tar'\n",
        "        torch.save(\n",
        "            {\n",
        "            'epoch': self.epoch,\n",
        "            'state_dict': self.model.state_dict(),\n",
        "            'max_auroc_mean': self.max_auroc_mean,\n",
        "            'optimizer' : self.optimizer.state_dict(),\n",
        "            'lossMIN' : self.lossMIN\n",
        "            }, path)\n",
        "        print(f\"saving {prefix} checkpoint\")\n",
        "\n",
        "    def init_dataset(self):\n",
        "        normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "\n",
        "        train_transforms = []\n",
        "        train_transforms.append(transforms.RandomResizedCrop(self.args.crop))\n",
        "        train_transforms.append(transforms.RandomHorizontalFlip())\n",
        "        train_transforms.append(transforms.ToTensor())\n",
        "        train_transforms.append(normalize)\n",
        "\n",
        "        datasetTrain = NIHChestXray(self.args, self.args.train_file, transform=transforms.Compose(train_transforms))\n",
        "\n",
        "        self.train_dl = DataLoader(dataset=datasetTrain, batch_size=self.args.batch_size, shuffle=True,  num_workers=4, pin_memory=True)\n",
        "\n",
        "        test_transforms = []\n",
        "        test_transforms.append(transforms.Resize(self.args.resize))\n",
        "        test_transforms.append(transforms.TenCrop(self.args.crop))\n",
        "        test_transforms.append(transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])))\n",
        "        test_transforms.append(transforms.Lambda(lambda crops: torch.stack([normalize(crop) for crop in crops])))\n",
        "\n",
        "        datasetVal = NIHChestXray(self.args, self.args.val_file, transform=transforms.Compose(test_transforms))\n",
        "        self.val_dl = DataLoader(dataset=datasetVal, batch_size=self.args.batch_size*10, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "        datasetTest = NIHChestXray(self.args, self.args.test_file, transform=transforms.Compose(test_transforms), classes_to_load='all')\n",
        "        self.test_dl = DataLoader(dataset=datasetTest, batch_size=self.args.batch_size*3, num_workers=8, shuffle=False, pin_memory=True)\n",
        "        print(datasetTest.CLASSES)\n",
        "\n",
        "    def train(self):\n",
        "        for self.epoch in range(self.start_epoch, self.args.epochs + 1):\n",
        "            self.epochTrain()\n",
        "            lossVal, val_ind_auroc = self.epochVal()\n",
        "            val_ind_auroc = np.array(val_ind_auroc)\n",
        "\n",
        "            aurocMean = val_ind_auroc.mean()\n",
        "            self.save_checkpoint(prefix=f'last_epoch')\n",
        "            self.should_test = False\n",
        "\n",
        "            if aurocMean > self.max_auroc_mean:\n",
        "                self.max_auroc_mean = aurocMean\n",
        "                self.save_checkpoint(prefix='best_auroc')\n",
        "                self.best_epoch = self.epoch\n",
        "                self.should_test = True\n",
        "\n",
        "            if lossVal < self.lossMIN:\n",
        "                self.lossMIN = lossVal\n",
        "                self.auroc_min_loss = aurocMean\n",
        "                self.save_checkpoint(prefix='min_loss')\n",
        "                self.should_test = True\n",
        "\n",
        "            self.print_auroc(val_ind_auroc, self.val_dl.dataset.class_ids_loaded, prefix='val')\n",
        "            if self.should_test is True:\n",
        "                test_ind_auroc = self.test()\n",
        "                test_ind_auroc = np.array(test_ind_auroc)\n",
        "\n",
        "                self.write_results(val_ind_auroc, self.val_dl.dataset.class_ids_loaded, prefix=f'\\n\\nepoch {self.epoch}\\nval', mode='a')\n",
        "\n",
        "                self.write_results(test_ind_auroc[self.test_dl.dataset.seen_class_ids], self.test_dl.dataset.seen_class_ids, prefix='\\ntest_seen', mode='a')\n",
        "                self.write_results(test_ind_auroc[self.test_dl.dataset.unseen_class_ids], self.test_dl.dataset.unseen_class_ids, prefix='\\ntest_unseen', mode='a')\n",
        "\n",
        "                self.print_auroc(test_ind_auroc[self.test_dl.dataset.seen_class_ids], self.test_dl.dataset.seen_class_ids, prefix='\\ntest_seen')\n",
        "                self.print_auroc(test_ind_auroc[self.test_dl.dataset.unseen_class_ids], self.test_dl.dataset.unseen_class_ids, prefix='\\ntest_unseen')\n",
        "\n",
        "            plot_array(self.val_losses, f'{self.args.save_dir}/val_loss')\n",
        "            print(f'best epoch {self.best_epoch} best auroc {self.max_auroc_mean} loss {lossVal:.6f} auroc at min loss {self.auroc_min_loss:0.4f}')\n",
        "\n",
        "            self.scheduler.step(lossVal)\n",
        "\n",
        "    def get_eta(self, epoch, iter):\n",
        "        self.time_end = time.time()\n",
        "        delta = self.time_end - self.time_start\n",
        "        delta = delta * (len(self.train_dl) * ((self.args.epochs + 1) - epoch) - iter)\n",
        "        sec = timedelta(seconds=int(delta))\n",
        "        d = (datetime(1,1,1) + sec)\n",
        "        eta = f\"{d.day-1} Days {d.hour}:{d.minute}:{d.second}\"\n",
        "        self.time_start = time.time()\n",
        "\n",
        "        return eta\n",
        "\n",
        "    def epochTrain(self):\n",
        "        self.model.train()\n",
        "        epoch_loss = 0\n",
        "        for batchID, (inputs, target) in enumerate (self.train_dl):\n",
        "\n",
        "            target = target.to(self.device)\n",
        "            inputs = inputs.to(self.device)\n",
        "            output, loss = self.model(inputs, target, self.epoch)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            eta = self.get_eta(self.epoch, batchID)\n",
        "            epoch_loss +=loss.item()\n",
        "            if batchID % 100 == 99:\n",
        "                print(f\" epoch [{self.epoch:04d} / {self.args.epochs:04d}] eta: {eta:<20} [{batchID:04}/{len(self.train_dl)}] lr: \\t{self.optimizer.param_groups[0]['lr']:0.4E} loss: \\t{epoch_loss/batchID:0.5f}\")\n",
        "\n",
        "    def epochVal(self):\n",
        "        self.model.eval()\n",
        "\n",
        "        lossVal = 0\n",
        "\n",
        "        outGT = torch.FloatTensor().to(self.device)\n",
        "        outPRED = torch.FloatTensor().to(self.device)\n",
        "        for i, (inputs, target) in enumerate (tqdm(self.val_dl)):\n",
        "            with torch.no_grad():\n",
        "                target = target.to(self.device)\n",
        "                inputs = inputs.to(self.device)\n",
        "                varTarget = torch.autograd.Variable(target)\n",
        "                bs, n_crops, c, h, w = inputs.size()\n",
        "\n",
        "                varInput = torch.autograd.Variable(inputs.view(-1, c, h, w).to(self.device))\n",
        "\n",
        "                varOutput, losstensor = self.model(varInput, varTarget, n_crops=n_crops, bs=bs)\n",
        "\n",
        "                outPRED = torch.cat((outPRED, varOutput), 0)\n",
        "                outGT = torch.cat((outGT, target), 0)\n",
        "\n",
        "                lossVal+=losstensor.item()\n",
        "                del varOutput, varTarget, varInput, target, inputs\n",
        "        lossVal = lossVal / len(self.val_dl)\n",
        "\n",
        "        aurocIndividual = self.computeAUROC(outGT, outPRED, self.val_dl.dataset.class_ids_loaded)\n",
        "        self.val_losses.append(lossVal)\n",
        "\n",
        "        return lossVal, aurocIndividual\n",
        "\n",
        "    def test(self):\n",
        "        cudnn.benchmark = True\n",
        "        outGT = torch.FloatTensor().cuda()\n",
        "        outPRED = torch.FloatTensor().cuda()\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        for i, (inputs, target) in enumerate(tqdm(self.test_dl)):\n",
        "            with torch.no_grad():\n",
        "                target = target.to(self.device)\n",
        "                outGT = torch.cat((outGT, target), 0)\n",
        "\n",
        "                bs, n_crops, c, h, w = inputs.size()\n",
        "\n",
        "                varInput = torch.autograd.Variable(inputs.view(-1, c, h, w).to(self.device))\n",
        "\n",
        "                out, _ = self.model(varInput, n_crops=n_crops, bs=bs)\n",
        "\n",
        "                outPRED = torch.cat((outPRED, out.data), 0)\n",
        "\n",
        "        aurocIndividual = self.computeAUROC(outGT, outPRED, self.test_dl.dataset.class_ids_loaded)\n",
        "\n",
        "        return aurocIndividual\n",
        "\n",
        "    def computeAUROC(self, dataGT, dataPRED, class_ids):\n",
        "        outAUROC = []\n",
        "        datanpGT = dataGT.cpu().numpy()\n",
        "        datanpPRED = dataPRED.cpu().numpy()\n",
        "\n",
        "        for i in class_ids:\n",
        "            outAUROC.append(roc_auc_score(datanpGT[:, i], datanpPRED[:, i]))\n",
        "        return outAUROC\n",
        "\n",
        "    def write_results(self, aurocIndividual, class_ids, prefix='val', mode='a'):\n",
        "        with open(f\"{self.args.save_dir}/results.txt\", mode) as results_file:\n",
        "            aurocMean = aurocIndividual.mean()\n",
        "\n",
        "            results_file.write(f'{prefix} AUROC mean {aurocMean:0.4f}\\n')\n",
        "            for i, class_id in enumerate(class_ids):\n",
        "                results_file.write(f'{self.val_dl.dataset.CLASSES[class_id]} {aurocIndividual[i]:0.4f}\\n')\n",
        "\n",
        "    def print_auroc(self, aurocIndividual, class_ids, prefix='val'):\n",
        "        aurocMean = aurocIndividual.mean()\n",
        "\n",
        "        print (f'{prefix} AUROC mean {aurocMean:0.4f}')\n",
        "\n",
        "        for i, class_id in enumerate(class_ids):\n",
        "            print (f'{self.val_dl.dataset.CLASSES[class_id]} {aurocIndividual[i]:0.4f}')"
      ],
      "metadata": {
        "id": "OjyuOw8-RF2_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "argParser = argparse.ArgumentParser(description='arguments')\n",
        "\n",
        "argParser.add_argument('--data-root', default='', type=str, help='the path to dataset')\n",
        "argParser.add_argument('--save-dir', default='', type=str, help='the path to save the checkpoints')\n",
        "argParser.add_argument('--train-file', default=f'train.txt', type=str, help='the path to train list ')\n",
        "argParser.add_argument('--val-file', default=f'val.txt', type=str, help='the path to val list ')\n",
        "argParser.add_argument('--test-file', default=f'test.txt', type=str, help='the path to test list')\n",
        "\n",
        "argParser.add_argument('--pretrained', dest='pretrained', action='store_true',  help='load imagenet pretrained model')\n",
        "argParser.add_argument('--bce-only', dest='bce_only', help='train with only binary cross entropy loss', action='store_true')\n",
        "\n",
        "argParser.add_argument('--num-classes', default=14, type=int, help='number of classes')\n",
        "argParser.add_argument('--batch-size', default=16, type=int, help='training batch size')\n",
        "argParser.add_argument('--epochs', default=40, type=int, help='number of epochs to train')\n",
        "argParser.add_argument('--vision-backbone', default='densenet121', type=str, help='[densenet121, densenet169, densenet201]')\n",
        "argParser.add_argument('--resume-from', default=None, type=str, help='path to checkpoint to resume the training from')\n",
        "argParser.add_argument('--load-from', default=None, type=str, help='path to checkpoint to load the weights from')\n",
        "\n",
        "argParser.add_argument('--resize', default=256, type=int, help='number of epochs to train')\n",
        "argParser.add_argument('--crop', default=224, type=int, help='number of epochs to train')\n",
        "argParser.add_argument('--lr', default=0.0001, type=float, help='learning rate')\n",
        "argParser.add_argument('--steps', default='20, 40, 60, 80', type=str, help='learning rate decay steps comma separated')\n",
        "\n",
        "argParser.add_argument('--beta-map', default=0.1, type=float, help='learning rate')\n",
        "argParser.add_argument('--beta-con', default=0.1, type=float, help='learning rate')\n",
        "argParser.add_argument('--beta-rank', default=1, type=float, help='learning rate')\n",
        "argParser.add_argument('--neg-penalty', default=0.03, type=float, help='learning rate')\n",
        "\n",
        "argParser.add_argument('--wo-con', dest='wo_con', help='train with out semantic consistency regularizer loss', action='store_true')\n",
        "argParser.add_argument('--wo-map', dest='wo_map', help='train with out alignement loss', action='store_true')\n",
        "\n",
        "argParser.add_argument('--textual-embeddings', default=CLASS_EMBEDDINGS, type=str, help='the path to labels embeddings')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HXByTwwRLT3",
        "outputId": "2713bf2a-a3ec-49c3-c237-f7f933cec763"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--textual-embeddings'], dest='textual_embeddings', nargs=None, const=None, default='nih_chest_xray_biobert.npy', type=<class 'str'>, choices=None, required=False, help='the path to labels embeddings', metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "\n",
        "def harmonic_mean(a, b):\n",
        "    return (2 * a * b) / (a + b)"
      ],
      "metadata": {
        "id": "OzJacGMERNpj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "EdOoW3M3RQNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DenseNet-121"
      ],
      "metadata": {
        "id": "-7ez1nX8RWH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = argParser.parse_args(['--load-from', 'best_auroc_checkpoint_densenet121.pth.tar',\n",
        "                             '--textual-embeddings', 'nih_chest_xray_biobert.npy',\n",
        "                             '--data-root', '',\n",
        "                             '--vision-backbone', 'densenet121'])\n",
        "\n",
        "trainer = ChexnetTrainer(args)\n",
        "\n",
        "test_ind_auroc = trainer.test()\n",
        "test_ind_auroc = np.array(test_ind_auroc)\n",
        "\n",
        "results[\"DenseNet-121\"] = {\n",
        "        \"Seen Mean (AUROC)\":          test_ind_auroc[trainer.test_dl.dataset.seen_class_ids].mean(),\n",
        "        \"Unseen Mean (AUROC)\":        test_ind_auroc[trainer.test_dl.dataset.unseen_class_ids].mean(),\n",
        "        \"Harmonic Mean (AUROC)\":      harmonic_mean(test_ind_auroc[trainer.test_dl.dataset.seen_class_ids].mean(), test_ind_auroc[trainer.test_dl.dataset.unseen_class_ids].mean())\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLAV2JzhRQUg",
        "outputId": "76bdbbb4-11c6-4c6b-d8df-8d8b5d48a956"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded checkpoint from best_auroc_checkpoint_densenet121.pth.tar\n",
            "data partition path: train.txt\n",
            "Number of images: 14160\n",
            "Number of max labels per image: 6\n",
            "Number of classes: 10\n",
            "data partition path: val.txt\n",
            "Number of images: 1979\n",
            "Number of max labels per image: 6\n",
            "Number of classes: 10\n",
            "data partition path: test.txt\n",
            "Number of images: 4737\n",
            "Number of max labels per image: 7\n",
            "Number of classes: 14\n",
            "['Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia', 'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 99/99 [00:35<00:00,  2.76it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EfficientNet-B0"
      ],
      "metadata": {
        "id": "LBhe4m3CRXUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = argParser.parse_args(['--load-from', 'best_auroc_checkpoint_efficientnet_b0.pth.tar',\n",
        "                             '--textual-embeddings', 'nih_chest_xray_biobert.npy',\n",
        "                             '--data-root', '',\n",
        "                             '--vision-backbone', 'efficientnet_b0'])\n",
        "\n",
        "trainer = ChexnetTrainer(args)\n",
        "\n",
        "test_ind_auroc = trainer.test()\n",
        "test_ind_auroc = np.array(test_ind_auroc)\n",
        "\n",
        "results[\"EfficientNet-B0\"] = {\n",
        "        \"Seen Mean (AUROC)\":          test_ind_auroc[trainer.test_dl.dataset.seen_class_ids].mean(),\n",
        "        \"Unseen Mean (AUROC)\":        test_ind_auroc[trainer.test_dl.dataset.unseen_class_ids].mean(),\n",
        "        \"Harmonic Mean (AUROC)\":      harmonic_mean(test_ind_auroc[trainer.test_dl.dataset.seen_class_ids].mean(), test_ind_auroc[trainer.test_dl.dataset.unseen_class_ids].mean())\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btpOP-eORXbQ",
        "outputId": "835522d3-6d46-4a4d-f1a2-71654dacc9e3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded checkpoint from best_auroc_checkpoint_efficientnet_b0.pth.tar\n",
            "data partition path: train.txt\n",
            "Number of images: 14160\n",
            "Number of max labels per image: 6\n",
            "Number of classes: 10\n",
            "data partition path: val.txt\n",
            "Number of images: 1979\n",
            "Number of max labels per image: 6\n",
            "Number of classes: 10\n",
            "data partition path: test.txt\n",
            "Number of images: 4737\n",
            "Number of max labels per image: 7\n",
            "Number of classes: 14\n",
            "['Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia', 'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 99/99 [00:29<00:00,  3.35it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "uwCYcRHYR2AL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(results).T.round(2)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.axis('off')\n",
        "\n",
        "tbl = table(ax, df, loc='center', colWidths=[ 0.02 * (len(col)+2) for col in df.columns.tolist() ])\n",
        "tbl.auto_set_font_size(False)\n",
        "tbl.set_fontsize(12)\n",
        "tbl.scale(1.2, 2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "F2pfSyqdR2Is",
        "outputId": "8e00f2cc-a155-4175-b5db-e0d6b41cbb31"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}